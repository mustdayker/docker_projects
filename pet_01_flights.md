### USER
 –£ –º–µ–Ω—è –µ—Å—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π —Å—Ç–µ–Ω–¥ —Å–æ–±—Ä–∞–Ω–Ω—ã–π –Ω–∞ Docker. 

–¢–∞–º –µ—Å—Ç—å —Ç–∞–∫–æ–π –Ω–∞–±–æ—Ä —Å–µ—Ä–≤–∏—Å–æ–≤:

- `PostgreSQL`
- `Airflow`
- `Superset`
- `Prometheus` 
- `Grafana`
- `Jupyterlab`
- `MinIO`
- `Spark`

–Ø —É—á—É—Å—å –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ Data Engineer. 

–ü—Ä–∏–¥—É–º–∞–π –¥–ª—è –º–µ–Ω—è –∑–∞–¥–∞–Ω–∏–µ –Ω–∞ —ç—Ç–æ–º —Å—Ç–µ–Ω–¥–µ —á—Ç–æ–±—ã –º–Ω–µ –ø–æ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–∏ –Ω–∞–≤—ã–∫–∏. 
–í–æ–∑—å–º–∏ –∫–∞–∫–æ–π –Ω–∏–±—É–¥—å –±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, —á—Ç–æ–±—ã —è –æ—â—É—Ç–∏–ª –≤—Å—é –ø—Ä–µ–ª–µ—Å—Ç—å Spark –∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏

### ASSISTANT
–û—Ç–ª–∏—á–Ω–æ! –£—á–∏—Ç—ã–≤–∞—è —Ç–≤–æ–π —Å—Ç–µ–∫ (Spark, MinIO –∫–∞–∫ S3-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ, PostgreSQL, Airflow, JupyterLab –∏ —Ç.–¥.), —è –ø—Ä–µ–¥–ª–æ–∂—É —Ç–µ–±–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∑–∞–¥–∞–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –∑–∞–¥–µ–π—Å—Ç–≤—É–µ—Ç –ø–æ—á—Ç–∏ –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å—Ç–µ–Ω–¥–∞ –∏ –¥–∞—Å—Ç —Ç–µ–±–µ –ø–æ—á—É–≤—Å—Ç–≤–æ–≤–∞—Ç—å —Å–∏–ª—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π Spark –Ω–∞ –±–æ–ª—å—à–æ–º –æ–±—ä—ë–º–µ –¥–∞–Ω–Ω—ã—Ö.

---

### üéØ **–ó–∞–¥–∞–Ω–∏–µ: ETL-–ø–∞–π–ø–ª–∞–π–Ω –∞–Ω–∞–ª–∏–∑–∞ –∞–≤–∏–∞–ø–µ—Ä–µ–ª—ë—Ç–æ–≤ –≤ –°–®–ê**

#### üì¶ **–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ**
–ò—Å–ø–æ–ª—å–∑—É–π –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–∞—Ç–∞—Å–µ—Ç **"US Domestic Airline Flights Performance"** –æ—Ç [Bureau of Transportation Statistics (BTS)](https://www.transtats.bts.gov/).  
–û—Å–æ–±–µ–Ω–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–∞—Ç–∞—Å–µ—Ç **"On-Time Performance"** ‚Äî –æ–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç –º–∏–ª–ª–∏–æ–Ω—ã –∑–∞–ø–∏—Å–µ–π –æ —Ä–µ–π—Å–∞—Ö —Å 1987 –ø–æ 2024 –≥–æ–¥.

üîó –ü—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ –∞—Ä—Ö–∏–≤—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞ 2023 –≥–æ–¥):  
https://www.transtats.bts.gov/DL_SelectFields.aspx?gnoyr_VQ=FGK&QO_fu146_anil=3&gnoyr_VQ=None&airline=All&Airport=All&dtId=975&geo=All&crp=All&geo=All&crp=All&flg=All&flg=All&Submit=Submit

https://transtats.bts.gov/PREZIP/

- `On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_1`

https://catalog.data.gov/dataset/on-time-performance

–î–ª—è —Å—Ç–∞—Ä—Ç–∞ –≤–æ–∑—å–º–∏ **–¥–∞–Ω–Ω—ã–µ –∑–∞ 2020‚Äì2023 –≥–æ–¥—ã** (–ø–æ ~7‚Äì10 –º–ª–Ω —Å—Ç—Ä–æ–∫ –≤ –≥–æ–¥). –≠—Ç–æ –¥–∞—Å—Ç —Ç–µ–±–µ **~30‚Äì40 –º–ª–Ω —Å—Ç—Ä–æ–∫**, —á—Ç–æ –æ—Ç–ª–∏—á–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è Spark.

–§–æ—Ä–º–∞—Ç: CSV (–º–æ–∂–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ ORC/Parquet –ø–æ–∑–∂–µ).

---

### üß© **–≠—Ç–∞–ø—ã –∑–∞–¥–∞–Ω–∏—è**

#### 1. **–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ MinIO**
- –°–∫–∞—á–∞–π –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ –≥–æ–¥–∞–º) –ª–æ–∫–∞–ª—å–Ω–æ.
- –ó–∞–≥—Ä—É–∑–∏ –∏—Ö –≤ –±–∞–∫–µ—Ç MinIO (–Ω–∞–ø—Ä–∏–º–µ—Ä, `raw-flights-csv`) —á–µ—Ä–µ–∑ `mc` (MinIO Client) –∏–ª–∏ Python-—Å–∫—Ä–∏–ø—Ç.
- –£–±–µ–¥–∏—Å—å, —á—Ç–æ Spark –º–æ–∂–µ—Ç —á–∏—Ç–∞—Ç—å –∏–∑ MinIO —á–µ—Ä–µ–∑ S3A-–∫–æ–Ω–Ω–µ–∫—Ç–æ—Ä.

#### 2. **Spark-–æ–±—Ä–∞–±–æ—Ç–∫–∞: ETL**
–í JupyterLab (–ª–æ–∫–∞–ª—å–Ω–æ –∏–ª–∏ –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ) –Ω–∞–ø–∏—à–∏ PySpark-—Å–∫—Ä–∏–ø—Ç, –∫–æ—Ç–æ—Ä—ã–π:
- –ß–∏—Ç–∞–µ—Ç –≤—Å–µ CSV –∏–∑ MinIO (`s3a://raw-flights-csv/`).
- –í—ã–ø–æ–ª–Ω—è–µ—Ç –æ—á–∏—Å—Ç–∫—É:
  - –£–¥–∞–ª—è–µ—Ç —Å—Ç—Ä–æ–∫–∏ —Å –ø—É—Å—Ç—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –≤ –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–ª—è—Ö (`ORIGIN`, `DEST`, `DEP_DELAY`, `ARR_DELAY` –∏ —Ç.–¥.).
  - –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–∏–ø—ã (–¥–∞—Ç—ã, —á–∏—Å–ª–∞).
- –î–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –ø–æ–ª—è:
  - `IS_DELAYED` (–µ—Å–ª–∏ `ARR_DELAY > 15`)
  - `ROUTE` = `ORIGIN + '-' + DEST`
  - `HOUR_OF_DAY` –∏–∑ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ª–µ—Ç–∞
- –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ **ORC –∏–ª–∏ Parquet** –≤ –Ω–æ–≤—ã–π –±–∞–∫–µ—Ç MinIO: `processed-flights-parquet`.

üí° **–¶–µ–ª—å**: –ø–æ—á—É–≤—Å—Ç–≤—É–π —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É –æ–±—Ä–∞–±–æ—Ç–∫–æ–π 1 –º–ª–Ω –∏ 30 –º–ª–Ω —Å—Ç—Ä–æ–∫. –ü–æ–ø—Ä–æ–±—É–π –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å Spark –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ 2 –≤–æ—Ä–∫–µ—Ä–æ–≤ —Å –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π CPU –∏ –ø–∞–º—è—Ç–∏ (–∫–∞–∫ —Ç—ã —Ö–æ—Ç–µ–ª).

#### 3. **–ê–≥—Ä–µ–≥–∞—Ü–∏—è –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞**
–°–æ–∑–¥–∞–π –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã:
- –¢–æ–ø-20 —Å–∞–º—ã—Ö –∑–∞–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –º–∞—Ä—à—Ä—É—Ç–æ–≤.
- –°—Ä–µ–¥–Ω—è—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø–æ –∞—ç—Ä–æ–ø–æ—Ä—Ç–∞–º –≤—ã–ª–µ—Ç–∞.
- –î–∏–Ω–∞–º–∏–∫–∞ –∑–∞–¥–µ—Ä–∂–µ–∫ –ø–æ –º–µ—Å—è—Ü–∞–º/—á–∞—Å–∞–º.

–°–æ—Ö—Ä–∞–Ω–∏ –∏—Ö –∫–∞–∫ —Ç–∞–±–ª–∏—Ü—ã –≤ **PostgreSQL** (—á–µ—Ä–µ–∑ JDBC –∏–∑ Spark –∏–ª–∏ —á–µ—Ä–µ–∑ pandas + SQLAlchemy –∏–∑ Jupyter).

#### 4. **–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤ Superset**
- –ü–æ–¥–∫–ª—é—á–∏ Superset –∫ PostgreSQL.
- –°–æ–∑–¥–∞–π –¥–∞—à–±–æ—Ä–¥:
  - –ö–∞—Ä—Ç–∞ –°–®–ê —Å –∞—ç—Ä–æ–ø–æ—Ä—Ç–∞–º–∏ –∏ –∑–∞–¥–µ—Ä–∂–∫–∞–º–∏.
  - –ì—Ä–∞—Ñ–∏–∫ –∑–∞–¥–µ—Ä–∂–µ–∫ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å—É—Ç–æ–∫.
  - –¢–∞–±–ª–∏—Ü–∞ —Ç–æ–ø-–º–∞—Ä—à—Ä—É—Ç–æ–≤.

#### 5. **–û—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ Airflow**
- –°–æ–∑–¥–∞–π DAG –≤ Airflow, –∫–æ—Ç–æ—Ä—ã–π:
  1. –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ MinIO (–∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –∑–∞–ø—É—Å–∫–∞–µ—Ç ETL).
  2. –ó–∞–ø—É—Å–∫–∞–µ—Ç Spark-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ (—á–µ—Ä–µ–∑ `SparkSubmitOperator` –∏–ª–∏ —á–µ—Ä–µ–∑ `BashOperator`, –µ—Å–ª–∏ Spark –∑–∞–ø—É—â–µ–Ω –≤ standalone-—Ä–µ–∂–∏–º–µ).
  3. –ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è ‚Äî –æ–±–Ω–æ–≤–ª—è–µ—Ç Superset (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: —á–µ—Ä–µ–∑ REST API –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ—Ç —É—Å–ø–µ—Ö).

#### 6. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**
- –ù–∞—Å—Ç—Ä–æ–π —Å–±–æ—Ä –º–µ—Ç—Ä–∏–∫ Spark (—á–µ—Ä–µ–∑ Prometheus JMX Exporter –∏–ª–∏ Spark‚Äôs built-in metrics) ‚Üí Prometheus ‚Üí Grafana.
- –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–π:
  - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CPU/–ø–∞–º—è—Ç–∏ –≤–æ—Ä–∫–µ—Ä–æ–≤.
  - –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ç–∞–¥–∏–π Spark.
  - –ü—Ä–æ–ø—É—Å–∫–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å MinIO (–µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ).

---

### üß† –ß—Ç–æ —Ç—ã –ø—Ä–æ–∫–∞—á–∞–µ—à—å:
- –†–∞–±–æ—Ç—É —Å **–±–æ–ª—å—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏** –≤ Spark (–ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è).
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é **Spark + MinIO (S3A)**.
- **ETL-–ø—Ä–æ—Ü–µ—Å—Å—ã** –æ—Ç —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–æ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–∞–±–ª–∏—Ü.
- **–û—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—é** —á–µ—Ä–µ–∑ Airflow.
- **–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é** —á–µ—Ä–µ–∑ Superset.
- **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥** –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã.
- –ü—Ä–∞–∫—Ç–∏–∫—É **data lakehouse** (MinIO –∫–∞–∫ lake, PostgreSQL –∫–∞–∫ warehouse).

---

–ï—Å–ª–∏ —Ö–æ—á–µ—à—å ‚Äî –º–æ–≥—É –¥–∞—Ç—å —à–∞–±–ª–æ–Ω PySpark-–∫–æ–¥–∞ –¥–ª—è —á—Ç–µ–Ω–∏—è –∏–∑ MinIO –∏–ª–∏ –ø—Ä–∏–º–µ—Ä DAG –≤ Airflow. –ì–æ—Ç–æ–≤ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å!

### USER
–î–∞. –ü–æ–¥–≥–æ—Ç–æ–≤—å –¥–ª—è –º–µ–Ω—è –∫–∞–∫–∏–µ —Ç–æ –∑–∞–≥–æ—Ç–æ–≤–∫–∏, —á—Ç–æ–±—ã –º–Ω–µ –±—ã–ª–æ –ø—Ä–æ—â–µ –Ω–∞—á–∞—Ç—å. –Ø —Å–æ–≤—Å–µ–º –Ω–æ–≤–∏—á–æ–∫ 

### ASSISTANT
–û—Ç–ª–∏—á–Ω–æ! –†–∞–∑ —Ç—ã –Ω–æ–≤–∏—á–æ–∫ ‚Äî —è –ø–æ–¥–≥–æ—Ç–æ–≤–ª—é –¥–ª—è —Ç–µ–±—è **–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ, –Ω–æ —Ä–∞–±–æ—á–∏–µ –∑–∞–≥–æ—Ç–æ–≤–∫–∏**, —á—Ç–æ–±—ã —Ç—ã –º–æ–≥ –±—ã—Å—Ç—Ä–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å –ø–µ—Ä–≤—ã–π —ç—Ç–∞–ø: **—á—Ç–µ–Ω–∏–µ CSV –∏–∑ MinIO –≤ Spark —á–µ—Ä–µ–∑ JupyterLab** –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ Parquet/ORC –æ–±—Ä–∞—Ç–Ω–æ –≤ MinIO.

–í—Å—ë –±—É–¥–µ—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–æ –ø–æ–¥ —Ç–≤–æ–π Docker-—Å—Ç–µ–Ω–¥.

---

## üìÅ 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ MinIO

### –°–æ–∑–¥–∞–π –±–∞–∫–µ—Ç—ã (—á–µ—Ä–µ–∑ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å MinIO –∏–ª–∏ `mc`):
- `raw-flights-csv` ‚Äî –¥–ª—è –∏—Å—Ö–æ–¥–Ω—ã—Ö CSV
- `processed-flights` ‚Äî –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (Parquet/ORC)

> –õ–æ–≥–∏–Ω/–ø–∞—Ä–æ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–µ—Å–ª–∏ –Ω–µ –º–µ–Ω—è–ª): `minioadmin` / `minioadmin`

---

## üì• 2. –°–∫–∞—á–∞–π –∏ –∑–∞–≥—Ä—É–∑–∏ –¥–∞–Ω–Ω—ã–µ

### –ü—Ä–∏–º–µ—Ä: –¥–∞–Ω–Ω—ã–µ –∑–∞ —è–Ω–≤–∞—Ä—å 2023 –≥–æ–¥–∞
–°–∫–∞—á–∞–π –æ–¥–∏–Ω —Ñ–∞–π–ª –¥–ª—è —Å—Ç–∞—Ä—Ç–∞ (—ç—Ç–æ ~600k —Å—Ç—Ä–æ–∫ ‚Äî –±—ã—Å—Ç—Ä–æ –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç—Å—è):

üîó https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2023_1.zip

–†–∞—Å–ø–∞–∫—É–π ‚Üí –ø–æ–ª—É—á–∏—à—å CSV ‚Üí –∑–∞–ª–µ–π –≤ MinIO –≤ –±–∞–∫–µ—Ç `raw-flights-csv` –∫–∞–∫ `2023-01.csv`.

> üí° –°–æ–≤–µ—Ç: –ø–µ—Ä–µ–∏–º–µ–Ω—É–π –∫–æ–ª–æ–Ω–∫–∏ –≤ CSV, –µ—Å–ª–∏ –æ–Ω–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç –ø—Ä–æ–±–µ–ª—ã –∏–ª–∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã ‚Äî Spark –Ω–µ –ª—é–±–∏—Ç —Ç–∞–∫–∏–µ –∏–º–µ–Ω–∞. –ú–æ–∂–Ω–æ –≤—Ä—É—á–Ω—É—é –∑–∞–º–µ–Ω–∏—Ç—å –≤ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–µ:
> ```
> FL_DATE,OP_CARRIER,OP_CARRIER_FL_NUM,ORIGIN,DEST,CRS_DEP_TIME,DEP_TIME,DEP_DELAY,CRS_ARR_TIME,ARR_TIME,ARR_DELAY
> ```

---

## üêç 3. JupyterLab: PySpark-–∑–∞–≥–æ—Ç–æ–≤–∫–∞

–°–æ–∑–¥–∞–π –Ω–æ—É—Ç–±—É–∫ –≤ JupyterLab –∏ –≤—Å—Ç–∞–≤—å —Å–ª–µ–¥—É—é—â–∏–π –∫–æ–¥. –û–Ω –Ω–∞—Å—Ç—Ä–æ–µ–Ω –Ω–∞ —Ä–∞–±–æ—Ç—É —Å MinIO —á–µ—Ä–µ–∑ S3A.

> ‚ö†Ô∏è –£–±–µ–¥–∏—Å—å, —á—Ç–æ —Ç–≤–æ–π Spark-–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä **–¥–æ—Å—Ç—É–ø–µ–Ω –∏–∑ Jupyter** (–ª–∏–±–æ Jupyter –≤–Ω—É—Ç—Ä–∏ —Ç–æ–≥–æ –∂–µ Docker Compose, –ª–∏–±–æ Spark –∑–∞–ø—É—â–µ–Ω –≤ standalone-—Ä–µ–∂–∏–º–µ –∏ Jupyter –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫ –Ω–µ–º—É).

### üîß –ó–∞–≥–æ—Ç–æ–≤–∫–∞: `flights_etl.ipynb`

```python
# 1. –ò–º–ø–æ—Ä—Ç—ã
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, concat, lit, hour, to_timestamp
from pyspark.sql.types import IntegerType

# 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ MinIO (–∏–∑–º–µ–Ω–∏, –µ—Å–ª–∏ —É —Ç–µ–±—è –¥—Ä—É–≥–∏–µ!)
minio_endpoint = "minio:9000"  # –∏–º—è —Å–µ—Ä–≤–∏—Å–∞ –≤ docker-compose
access_key = "minioadmin"
secret_key = "minioadmin"

# 3. –°–æ–∑–¥–∞—ë–º SparkSession —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π S3A
spark = SparkSession.builder \
    .appName("Flights ETL") \
    .config("spark.hadoop.fs.s3a.endpoint", f"http://{minio_endpoint}") \
    .config("spark.hadoop.fs.s3a.access.key", access_key) \
    .config("spark.hadoop.fs.s3a.secret.key", secret_key) \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262") \
    .getOrCreate()

# 4. –ß–∏—Ç–∞–µ–º CSV –∏–∑ MinIO
input_path = "s3a://raw-flights-csv/2023-01.csv"
df = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .csv(input_path)

print("–°—Ö–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö:")
df.printSchema()
print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫: {df.count()}")

# 5. –û—á–∏—Å—Ç–∫–∞ –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ
df_clean = df \
    .filter(col("ORIGIN").isNotNull() & col("DEST").isNotNull()) \
    .filter(col("DEP_DELAY").isNotNull() & col("ARR_DELAY").isNotNull()) \
    .withColumn("IS_DELAYED", when(col("ARR_DELAY") > 15, 1).otherwise(0)) \
    .withColumn("ROUTE", concat(col("ORIGIN"), lit("-"), col("DEST"))) \
    .withColumn("DEP_HOUR", (col("DEP_TIME") / 100).cast(IntegerType()))

# –ü–æ–∫–∞–∂–µ–º –ø—Ä–∏–º–µ—Ä
df_clean.select("ORIGIN", "DEST", "ARR_DELAY", "IS_DELAYED", "ROUTE", "DEP_HOUR").show(5)

# 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Parquet –≤ MinIO
output_path = "s3a://processed-flights/flights_2023_01.parquet"
df_clean.write \
    .mode("overwrite") \
    .parquet(output_path)

print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_path}")

# 7. (–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ ORC
# df_clean.write.mode("overwrite").orc("s3a://processed-flights/flights_2023_01.orc")
```

---

## üê≥ 4. –ü—Ä–æ–≤–µ—Ä—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

–£–±–µ–¥–∏—Å—å, —á—Ç–æ –≤ —Ç–≤–æ—ë–º Spark-–æ–±—Ä–∞–∑–µ (–∏–ª–∏ –≤ Jupyter, –µ—Å–ª–∏ Spark –ª–æ–∫–∞–ª—å–Ω—ã–π) –µ—Å—Ç—å:

- `hadoop-aws`
- `aws-java-sdk-bundle`

–ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—à—å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –æ–±—Ä–∞–∑ Spark ‚Äî –¥–æ–±–∞–≤—å –≤ `docker-compose.yml` –¥–ª—è Spark:

```yaml
environment:
  - SPARK_SUBMIT_OPTS=-Dspark.jars.ivy=/tmp/.ivy
```

–ò–ª–∏ —Å–æ–±–µ—Ä–∏ –∫–∞—Å—Ç–æ–º–Ω—ã–π –æ–±—Ä–∞–∑ —Å –Ω—É–∂–Ω—ã–º–∏ JAR-—Ñ–∞–π–ª–∞–º–∏.

> üí° –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: —Å–∫–∞—á–∞–π JAR-—Ñ–∞–π–ª—ã –≤—Ä—É—á–Ω—É—é –∏ —É–∫–∞–∂–∏ –∏—Ö —á–µ—Ä–µ–∑ `.config("spark.jars", "/opt/spark/jars/hadoop-aws-3.3.4.jar,...")`.

---

## ‚ñ∂Ô∏è 5. –ö–∞–∫ –∑–∞–ø—É—Å–∫–∞—Ç—å

1. –ó–∞–ø—É—Å—Ç–∏ –≤–µ—Å—å —Å—Ç–µ–Ω–¥: `docker-compose up -d`
2. –ó–∞–π–¥–∏ –≤ JupyterLab (–æ–±—ã—á–Ω–æ `http://localhost:8888`)
3. –°–æ–∑–¥–∞–π –Ω–æ–≤—ã–π Python 3 notebook
4. –í—Å—Ç–∞–≤—å –∫–æ–¥ –≤—ã—à–µ
5. –ó–∞–ø—É—Å—Ç–∏ —è—á–µ–π–∫–∏ –ø–æ –æ—á–µ—Ä–µ–¥–∏

–ï—Å–ª–∏ –≤—Å—ë –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ ‚Äî —Ç—ã —É–≤–∏–¥–∏—à—å —Å—Ö–µ–º—É, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± —É—Å–ø–µ—à–Ω–æ–π –∑–∞–ø–∏—Å–∏ –≤ MinIO.

---

## üß© –ß—Ç–æ –¥–∞–ª—å—à–µ?

–ö–æ–≥–¥–∞ —ç—Ç–æ –∑–∞—Ä–∞–±–æ—Ç–∞–µ—Ç:
- –î–æ–±–∞–≤—å –±–æ–ª—å—à–µ —Ñ–∞–π–ª–æ–≤ (2023-02.csv –∏ —Ç.–¥.)
- –ò–∑–º–µ–Ω–∏ —á—Ç–µ–Ω–∏–µ –Ω–∞ `s3a://raw-flights-csv/*.csv`
- –ü–æ–ø—Ä–æ–±—É–π —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ **PostgreSQL** (–¥–∞–º –ø—Ä–∏–º–µ—Ä –æ—Ç–¥–µ–ª—å–Ω–æ)
- –°–æ–∑–¥–∞–π DAG –≤ Airflow, –∫–æ—Ç–æ—Ä—ã–π –∑–∞–ø—É—Å–∫–∞–µ—Ç —ç—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç

---

–•–æ—á–µ—à—å ‚Äî –º–æ–≥—É —Å—Ä–∞–∑—É –¥–∞—Ç—å:
- –ü—Ä–∏–º–µ—Ä –∑–∞–ø–∏—Å–∏ –≤ PostgreSQL –∏–∑ Spark
- –ü—Ä–∏–º–µ—Ä DAG –¥–ª—è Airflow
- –°–æ–≤–µ—Ç—ã –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ Spark –≤ standalone-—Ä–µ–∂–∏–º–µ —Å 2 –≤–æ—Ä–∫–µ—Ä–∞–º–∏

–°–∫–∞–∂–∏, —á—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ –¥–∞–ª—å—à–µ!

### USER
 - –ü—Ä–∏–º–µ—Ä –∑–∞–ø–∏—Å–∏ –≤ PostgreSQL –∏–∑ Spark
- –ü—Ä–∏–º–µ—Ä DAG –¥–ª—è Airflow
- –°–æ–≤–µ—Ç—ã –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ Spark –≤ standalone-—Ä–µ–∂–∏–º–µ —Å 2 –≤–æ—Ä–∫–µ—Ä–∞–º–∏

### ASSISTANT
–û—Ç–ª–∏—á–Ω–æ! –í–æ—Ç —Ç—Ä–∏ –ø–æ–ª–µ–∑–Ω—ã–µ –∑–∞–≥–æ—Ç–æ–≤–∫–∏ –ø–æ–¥ —Ç–≤–æ–π —Å—Ç–µ–∫:

---

## 1Ô∏è‚É£ **–ü—Ä–∏–º–µ—Ä –∑–∞–ø–∏—Å–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Spark –≤ PostgreSQL**

–ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —É —Ç–µ–±—è —É–∂–µ –µ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π `DataFrame` `df_clean` (–∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —à–∞–≥–∞).  
–¢—ã —Ö–æ—á–µ—à—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç–æ–ø-10 –º–∞—Ä—à—Ä—É—Ç–æ–≤) –≤ PostgreSQL.

### üîß –ö–æ–¥ (–¥–æ–±–∞–≤—å –≤ Jupyter –∏–ª–∏ Spark-—Å–∫—Ä–∏–ø—Ç):

```python
# –ê–≥—Ä–µ–≥–∞—Ü–∏—è: —Ç–æ–ø-10 —Å–∞–º—ã—Ö –∑–∞–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –º–∞—Ä—à—Ä—É—Ç–æ–≤
top_routes = df_clean.groupBy("ROUTE") \
    .agg(
        {"IS_DELAYED": "sum", "ARR_DELAY": "avg"}
    ) \
    .withColumnRenamed("sum(IS_DELAYED)", "total_delays") \
    .withColumnRenamed("avg(ARR_DELAY)", "avg_delay") \
    .orderBy(col("total_delays").desc()) \
    .limit(10)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ PostgreSQL
pg_url = "jdbc:postgresql://postgres-db:5432/learn_base"  # –∏–º—è —Å–µ—Ä–≤–∏—Å–∞ –∏–∑ docker-compose
pg_properties = {
    "user": "airflow",        # –∏–ª–∏ —Å–æ–∑–¥–∞–π –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    "password": "airflow",
    "driver": "org.postgresql.Driver"
}

# –ó–∞–ø–∏—Å—å –≤ —Ç–∞–±–ª–∏—Ü—É
top_routes.write \
    .mode("overwrite") \
    .option("truncate", "true") \
    .jdbc(url=pg_url, table="top_delayed_routes", properties=pg_properties)

print("‚úÖ –¢–∞–±–ª–∏—Ü–∞ top_delayed_routes –∑–∞–ø–∏—Å–∞–Ω–∞ –≤ PostgreSQL")
```

### ‚ö†Ô∏è –í–∞–∂–Ω–æ:
- –£–±–µ–¥–∏—Å—å, —á—Ç–æ –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ —Å Spark (–∏–ª–∏ Jupyter) —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω **PostgreSQL JDBC –¥—Ä–∞–π–≤–µ—Ä**.
- –°–∫–∞—á–∞–π –µ–≥–æ: [https://jdbc.postgresql.org/download/](https://jdbc.postgresql.org/download/postgresql-42.7.3.jar)
- –ü–æ–ª–æ–∂–∏ –≤ –ø–∞–ø–∫—É, –Ω–∞–ø—Ä–∏–º–µ—Ä: `./spark/jars/postgresql-42.7.3.jar`
- –ò –¥–æ–±–∞–≤—å –≤ `docker-compose.yml` –¥–ª—è Spark/Jupyter:

```yaml
volumes:
  - ./spark/jars:/opt/spark/jars
```

–¢–µ–ø–µ—Ä—å Spark —Å–º–æ–∂–µ—Ç –ø–∏—Å–∞—Ç—å –≤ PostgreSQL.

---

## 2Ô∏è‚É£ **–ü—Ä–∏–º–µ—Ä DAG –≤ Airflow –¥–ª—è –∑–∞–ø—É—Å–∫–∞ Spark-–∑–∞–¥–∞—á–∏**

–ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —Ç–≤–æ–π Spark —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ **standalone-—Ä–µ–∂–∏–º–µ**, –∏ —É —Ç–µ–±—è –µ—Å—Ç—å —Å–∫—Ä–∏–ø—Ç `/opt/airflow/dags/spark_flights_etl.py`.

### üìÅ –§–∞–π–ª: `dags/spark_flights_dag.py`

```python
from datetime import datetime
from airflow import DAG
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

default_args = {
    'owner': 'data-engineer',
    'depends_on_past': False,
    'start_date': datetime(2025, 10, 1),
    'retries': 1,
}

dag = DAG(
    'flights_etl_spark',
    default_args=default_args,
    description='ETL –∞–≤–∏–∞–ø–µ—Ä–µ–ª—ë—Ç–æ–≤ —á–µ—Ä–µ–∑ Spark',
    schedule_interval='@daily',
    catchup=False,
)

spark_task = SparkSubmitOperator(
    task_id='run_spark_etl',
    application='/opt/airflow/dags/spark_flights_etl.py',  # –ø—É—Ç—å –≤–Ω—É—Ç—Ä–∏ airflow-webserver/scheduler
    conn_id='spark_default',  # –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω –≤ Airflow UI!
    conf={
        'spark.executor.memory': '2g',
        'spark.executor.cores': '1',
        'spark.cores.max': '2',  # –º–∞–∫—Å–∏–º—É–º —è–¥–µ—Ä –¥–ª—è –≤—Å–µ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
        'spark.sql.adaptive.enabled': 'true'
    },
    dag=dag,
)
```

### üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –≤ Airflow UI:
1. –ó–∞–π–¥–∏ –≤ Airflow ‚Üí Admin ‚Üí Connections
2. –°–æ–∑–¥–∞–π –Ω–æ–≤–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ:
   - **Conn Id**: `spark_default`
   - **Conn Type**: `Spark`
   - **Host**: `spark://spark-master:7077` (–µ—Å–ª–∏ —É —Ç–µ–±—è master –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è `spark-master`)
   - –û—Å—Ç–∞–ª—å–Ω–æ–µ –æ—Å—Ç–∞–≤—å –ø—É—Å—Ç—ã–º

> üí° –£–±–µ–¥–∏—Å—å, —á—Ç–æ `spark-master` –∏ `spark-worker` –¥–æ—Å—Ç—É–ø–Ω—ã –∏–∑ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ Airflow –ø–æ –∏–º–µ–Ω–∏ —Å–µ—Ä–≤–∏—Å–∞.

---

## 3Ô∏è‚É£ **–°–æ–≤–µ—Ç—ã: Spark Standalone —Å 2 –≤–æ—Ä–∫–µ—Ä–∞–º–∏ –≤ Docker**

–î–æ–±–∞–≤—å –≤ —Å–≤–æ–π `docker-compose.yml` —Å–ª–µ–¥—É—é—â–∏–µ —Å–µ—Ä–≤–∏—Å—ã:

```yaml
  spark-master:
    image: bitnami/spark:3.5
    container_name: spark-master
    ports:
      - "7077:7077"
      - "8080:8080"
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    networks:
      - data-eng-net

  spark-worker-1:
    image: bitnami/spark:3.5
    container_name: spark-worker-1
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    networks:
      - data-eng-net

  spark-worker-2:
    image: bitnami/spark:3.5
    container_name: spark-worker-2
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    networks:
      - data-eng-net
```

### ‚úÖ –ß—Ç–æ —ç—Ç–æ –¥–∞—ë—Ç:
- Master –Ω–∞ `spark://spark-master:7077`
- 2 –≤–æ—Ä–∫–µ—Ä–∞ –ø–æ 1 —è–¥—Ä—É –∏ 2 –ì–ë RAM ‚Üí Spark —Å–º–æ–∂–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ
- –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å Spark: `http://localhost:8080` ‚Äî —Ç–∞–º –≤–∏–¥–Ω–æ –∞–∫—Ç–∏–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏

### üí° –°–æ–≤–µ—Ç—ã –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:
- –í PySpark-—Å–∫—Ä–∏–ø—Ç–µ –∏—Å–ø–æ–ª—å–∑—É–π `.repartition(4)` –∏–ª–∏ `.coalesce()` –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –ø–∞—Ä—Ç–∏—Ü–∏–π.
- –ß–∏—Ç–∞–π CSV —Å `option("multiline", "false")` ‚Äî –∏–Ω–∞—á–µ –±—É–¥–µ—Ç –º–µ–¥–ª–µ–Ω–Ω–æ.
- –ò—Å–ø–æ–ª—å–∑—É–π **Parquet/ORC** –¥–ª—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ‚Äî –æ–Ω–∏ –≤ 3‚Äì5 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ CSV.

---

## üß™ –ö–∞–∫ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –≤—Å—ë —Ä–∞–±–æ—Ç–∞–µ—Ç?

1. –ó–∞–ø—É—Å—Ç–∏ `docker-compose up -d`
2. –ó–∞–π–¥–∏ –≤ Airflow ‚Üí –∑–∞–ø—É—Å—Ç–∏ DAG –≤—Ä—É—á–Ω—É—é
3. –û—Ç–∫—Ä–æ–π Spark UI (`http://localhost:8080`) ‚Äî –¥–æ–ª–∂–µ–Ω –ø–æ—è–≤–∏—Ç—å—Å—è running application
4. –ü—Ä–æ–≤–µ—Ä—å MinIO ‚Äî –ø–æ—è–≤–∏–ª–∏—Å—å –ª–∏ Parquet-—Ñ–∞–π–ª—ã?
5. –ü–æ–¥–∫–ª—é—á–∏—Å—å –∫ PostgreSQL:  
   ```sql
   SELECT * FROM top_delayed_routes;
   ```

---

–ï—Å–ª–∏ —Ö–æ—á–µ—à—å ‚Äî –º–æ–≥—É:
- –î–∞—Ç—å –ø–æ–ª–Ω—ã–π `spark_flights_etl.py` (–∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π —Å–∫—Ä–∏–ø—Ç)
- –ü–æ–º–æ—á—å —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π JDBC –≤ Jupyter
- –ü–æ–¥—Å–∫–∞–∑–∞—Ç—å, –∫–∞–∫ –¥–æ–±–∞–≤–∏—Ç—å Spark History Server (—á—Ç–æ–±—ã –≤–∏–¥–µ—Ç—å –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏)

–ì–æ—Ç–æ–≤ –ø–æ–º–æ—á—å –Ω–∞ –ª—é–±–æ–º —ç—Ç–∞–ø–µ!