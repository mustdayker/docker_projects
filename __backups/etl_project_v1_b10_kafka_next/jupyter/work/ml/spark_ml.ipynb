{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7c60fe5-e73b-43ed-8784-4dacb3a15fcd",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection - SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb777805-606e-4c5c-a1ab-3f50cf878a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Spark пример для бинарной классификации на Credit Card Fraud Detection\n",
    "Запуск: spark-submit spark_example.py\n",
    "Или: python spark_example.py (если Spark настроен в PYTHONPATH)\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import (BinaryClassificationEvaluator, \n",
    "                                  MulticlassClassificationEvaluator)\n",
    "import time\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"SPARK ML ПАЙПЛАЙН - Credit Card Fraud Detection\") \n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # === ЭТАП 1: ИНИЦИАЛИЗАЦИЯ SPARK СЕССИИ ===\n",
    "    print(\"\\n1. ИНИЦИАЛИЗАЦИЯ SPARK СЕССИИ...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Создаем SparkSession - точку входа в Spark\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"CreditCardFraudDetection\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Настройки для лучшей производительности\n",
    "    spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "    \n",
    "    init_time = time.time() - start_time\n",
    "    print(f\"   ✓ Spark сессия создана за {init_time:.2f} секунд\")\n",
    "    print(f\"   ✓ Версия Spark: {spark.version}\")\n",
    "    \n",
    "    # === ЭТАП 2: ЗАГРУЗКА ДАННЫХ ===\n",
    "    print(\"\\n2. ЗАГРУЗКА ДАННЫХ В SPARK DATAFRAME...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Читаем CSV в распределенный Spark DataFrame\n",
    "        df = spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .csv(\"/work/dataset.csv\")\n",
    "        \n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"   ✓ Данные загружены за {load_time:.2f} секунд\")\n",
    "        \n",
    "        # Показываем информацию о данных\n",
    "        print(f\"   ✓ Размер данных: {df.count():,} строк, {len(df.columns)} столбцов\")\n",
    "        print(f\"   ✓ Partitions: {df.rdd.getNumPartitions()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ✗ Ошибка загрузки: {e}\")\n",
    "        print(\"   Скачайте датасет с Kaggle: https://www.kaggle.com/mlg-ulb/creditcardfraud\")\n",
    "        spark.stop()\n",
    "        return\n",
    "    \n",
    "    # === ЭТАП 3: АНАЛИЗ ДАННЫХ ===\n",
    "    print(\"\\n3. АНАЛИЗ ДАННЫХ В SPARK...\")\n",
    "    \n",
    "    # Схема данных (типы столбцов)\n",
    "    print(\"   Схема данных:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    # Распределение целевой переменной\n",
    "    class_dist = df.groupBy(\"Class\").agg(count(\"*\").alias(\"count\")).collect()\n",
    "    print(f\"   Распределение классов: { {row['Class']: row['count'] for row in class_dist} }\")\n",
    "    \n",
    "    # === ЭТАП 4: ПОДГОТОВКА ДАННЫХ ===\n",
    "    print(\"\\n4. ПОДГОТОВКА ДАННЫХ ДЛЯ SPARK ML...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # В Spark ML все признаки должны быть собраны в один векторный столбец\n",
    "    # Выбираем все столбцы кроме 'Class' как признаки\n",
    "    feature_columns = [col for col in df.columns if col != 'Class']\n",
    "    print(f\"   Признаки ({len(feature_columns)}): {feature_columns}\")\n",
    "    \n",
    "    # Создаем VectorAssembler - трансформатор для создания вектора признаков\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=feature_columns,   # Входные столбцы-признаки\n",
    "        outputCol=\"features\"         # Выходной столбец с вектором\n",
    "    )\n",
    "    \n",
    "    # Применяем трансформатор к данным\n",
    "    featured_df = assembler.transform(df)\n",
    "    \n",
    "    # Разделяем на обучающую и тестовую выборки\n",
    "    train_df, test_df = featured_df.randomSplit(\n",
    "        [0.7, 0.3],         # 70% train, 30% test\n",
    "        seed=42              # Для воспроизводимости\n",
    "    )\n",
    "    \n",
    "    prep_time = time.time() - start_time\n",
    "    print(f\"   ✓ Данные подготовлены за {prep_time:.2f} секунд\")\n",
    "    print(f\"   ✓ Обучающая выборка: {train_df.count():,} samples\")\n",
    "    print(f\"   ✓ Тестовая выборка: {test_df.count():,} samples\")\n",
    "    \n",
    "    # Показываем как выглядят данные после преобразования\n",
    "    print(\"\\n   Пример данных с векторными признаками:\")\n",
    "    train_df.select(\"features\", \"Class\").show(5, truncate=False)\n",
    "    \n",
    "    # === ЭТАП 5: ОБУЧЕНИЕ МОДЕЛИ ===\n",
    "    print(\"\\n5. ОБУЧЕНИЕ SPARK ML МОДЕЛИ (RandomForest)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Создаем модель RandomForest\n",
    "    rf = RandomForestClassifier(\n",
    "        featuresCol=\"features\",      # Столбец с вектором признаков\n",
    "        labelCol=\"Class\",           # Столбец с целевой переменной\n",
    "        numTrees=100,               # Количество деревьев\n",
    "        maxDepth=10,                # Максимальная глубина\n",
    "        seed=42,                    # Для воспроизводимости\n",
    "        subsamplingRate=0.7         # Доля данных для каждого дерева\n",
    "    )\n",
    "    \n",
    "    # Обучаем модель (распределенное обучение на кластере!)\n",
    "    model = rf.fit(train_df)\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"   ✓ Модель обучена за {train_time:.2f} секунд\")\n",
    "    print(f\"   ✓ Обучено деревьев: {model.getNumTrees}\")\n",
    "    \n",
    "    # === ЭТАП 6: ПРЕДСКАЗАНИЯ ===\n",
    "    print(\"\\n6. ПРЕДСКАЗАНИЯ НА ТЕСТОВЫХ ДАННЫХ...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Делаем предсказания - модель добавляет новые столбцы к DataFrame\n",
    "    predictions = model.transform(test_df)\n",
    "    \n",
    "    predict_time = time.time() - start_time\n",
    "    print(f\"   ✓ Предсказания сделаны за {predict_time:.2f} секунд\")\n",
    "    \n",
    "    # Показываем структуру предсказаний\n",
    "    print(\"\\n   Структура предсказаний:\")\n",
    "    predictions.select(\"Class\", \"probability\", \"prediction\").show(10, truncate=False)\n",
    "    \n",
    "    # === ЭТАП 7: ОЦЕНКА МОДЕЛИ ===\n",
    "    print(\"\\n7. ОЦЕНКА КАЧЕСТВА МОДЕЛИ:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Evaluator для AUC-ROC (бинарная классификация)\n",
    "    auc_evaluator = BinaryClassificationEvaluator(\n",
    "        labelCol=\"Class\",\n",
    "        rawPredictionCol=\"rawPrediction\",  # Столбец с \"сырыми\" предсказаниями\n",
    "        metricName=\"areaUnderROC\"\n",
    "    )\n",
    "    \n",
    "    # Evaluator для других метрик\n",
    "    multi_evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"Class\",\n",
    "        predictionCol=\"prediction\"\n",
    "    )\n",
    "    \n",
    "    # Вычисляем метрики\n",
    "    auc = auc_evaluator.evaluate(predictions)\n",
    "    accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
    "    precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
    "    f1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\"})\n",
    "    \n",
    "    # Выводим метрики\n",
    "    print(f\"   ТОЧНОСТЬ (Accuracy):  {accuracy:.4f}\")\n",
    "    print(f\"   ТОЧНОСТЬ (Precision): {precision:.4f}\")\n",
    "    print(f\"   ПОЛНОТА (Recall):     {recall:.4f}\")\n",
    "    print(f\"   F1-MEASURE:          {f1:.4f}\")\n",
    "    print(f\"   AUC-ROC:             {auc:.4f}\")\n",
    "    \n",
    "    # Детальная статистика по классам\n",
    "    print(\"\\n   СТАТИСТИКА ПРЕДСКАЗАНИЙ:\")\n",
    "    predictions.groupBy(\"Class\", \"prediction\").count().orderBy(\"Class\", \"prediction\").show()\n",
    "    \n",
    "    # === ЭТАП 8: ЗАВЕРШЕНИЕ РАБОТЫ ===\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ИТОГИ SPARK ПАЙПЛАЙНА:\")\n",
    "    print(f\"   • Общее время выполнения: {init_time + load_time + prep_time + train_time + predict_time:.2f} сек\")\n",
    "    print(f\"   • Время инициализации Spark: {init_time:.2f} сек\")\n",
    "    print(f\"   • Размер данных: {df.count():,} строк × {len(df.columns)} столбцов\")\n",
    "    print(f\"   • Partitions: {df.rdd.getNumPartitions()}\")\n",
    "    print(f\"   • Лучшая метрика (AUC-ROC): {auc:.4f}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Останавливаем Spark сессию\n",
    "    spark.stop()\n",
    "    print(\"   ✓ Spark сессия остановлена\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad08b9b-ec62-49a4-b4ae-24bbdb0fb96e",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection - SPARK - Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5d7c232-eddb-4101-adca-859958ee26a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Spark пример для бинарной классификации на Credit Card Fraud Detection\n",
    "Запуск: spark-submit spark_example.py\n",
    "Или: python spark_example.py (если Spark настроен в PYTHONPATH)\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import (BinaryClassificationEvaluator, \n",
    "                                  MulticlassClassificationEvaluator)\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44d529a4-dcf4-4b9a-a7d4-1cbb0144165f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SPARK ML ПАЙПЛАЙН - Credit Card Fraud Detection\n",
      "======================================================================\n",
      "\n",
      "1. ИНИЦИАЛИЗАЦИЯ SPARK СЕССИИ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/03 16:52:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ Spark сессия создана за 4.13 секунд\n",
      "   ✓ Версия Spark: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SPARK ML ПАЙПЛАЙН - Credit Card Fraud Detection\") \n",
    "print(\"=\" * 70)\n",
    "\n",
    "# === ЭТАП 1: ИНИЦИАЛИЗАЦИЯ SPARK СЕССИИ ===\n",
    "print(\"\\n1. ИНИЦИАЛИЗАЦИЯ SPARK СЕССИИ...\")\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "drivers = [\n",
    "    \"/home/jovyan/work/spark-jars/hadoop-aws-3.3.4.jar\",             # S3\n",
    "    \"/home/jovyan/work/spark-jars/aws-java-sdk-bundle-1.12.262.jar\", # S3\n",
    "    \"/home/jovyan/work/spark-jars/wildfly-openssl-1.0.7.Final.jar\",  # S3\n",
    "    \"/home/jovyan/work/spark-jars/postgresql-42.6.0.jar\",            # PostgreSQL\n",
    "]\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"mustdayker-Spark\")\n",
    "         .master(\"spark://spark-master:7077\") \n",
    "         .config(\"spark.jars\", \",\".join(drivers))\n",
    "         .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "         .getOrCreate()\n",
    "        )\n",
    "\n",
    "# Настройки для лучшей производительности\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "\n",
    "init_time = time.time() - start_time\n",
    "print(f\"   ✓ Spark сессия создана за {init_time:.2f} секунд\")\n",
    "print(f\"   ✓ Версия Spark: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45510252-9e23-4657-a662-507015712605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. ЗАГРУЗКА ДАННЫХ В SPARK DATAFRAME...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ Данные загружены за 6.65 секунд\n",
      "   ✓ Размер данных: 284,807 строк, 31 столбцов\n",
      "   ✓ Partitions: 10\n"
     ]
    }
   ],
   "source": [
    "# === ЭТАП 2: ЗАГРУЗКА ДАННЫХ ===\n",
    "print(\"\\n2. ЗАГРУЗКА ДАННЫХ В SPARK DATAFRAME...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Читаем CSV в распределенный Spark DataFrame\n",
    "    df = (spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .csv(\"/shared_data/creditcard.csv\"))\n",
    "    \n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"   ✓ Данные загружены за {load_time:.2f} секунд\")\n",
    "    \n",
    "    # Показываем информацию о данных\n",
    "    print(f\"   ✓ Размер данных: {df.count():,} строк, {len(df.columns)} столбцов\")\n",
    "    print(f\"   ✓ Partitions: {df.rdd.getNumPartitions()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ✗ Ошибка загрузки: {e}\")\n",
    "    print(\"   Скачайте датасет с Kaggle: https://www.kaggle.com/mlg-ulb/creditcardfraud\")\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22f2d481-c88d-494a-8b6f-6355b1bb4458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. АНАЛИЗ ДАННЫХ В SPARK...\n",
      "   Схема данных:\n",
      "root\n",
      " |-- Time: double (nullable = true)\n",
      " |-- V1: double (nullable = true)\n",
      " |-- V2: double (nullable = true)\n",
      " |-- V3: double (nullable = true)\n",
      " |-- V4: double (nullable = true)\n",
      " |-- V5: double (nullable = true)\n",
      " |-- V6: double (nullable = true)\n",
      " |-- V7: double (nullable = true)\n",
      " |-- V8: double (nullable = true)\n",
      " |-- V9: double (nullable = true)\n",
      " |-- V10: double (nullable = true)\n",
      " |-- V11: double (nullable = true)\n",
      " |-- V12: double (nullable = true)\n",
      " |-- V13: double (nullable = true)\n",
      " |-- V14: double (nullable = true)\n",
      " |-- V15: double (nullable = true)\n",
      " |-- V16: double (nullable = true)\n",
      " |-- V17: double (nullable = true)\n",
      " |-- V18: double (nullable = true)\n",
      " |-- V19: double (nullable = true)\n",
      " |-- V20: double (nullable = true)\n",
      " |-- V21: double (nullable = true)\n",
      " |-- V22: double (nullable = true)\n",
      " |-- V23: double (nullable = true)\n",
      " |-- V24: double (nullable = true)\n",
      " |-- V25: double (nullable = true)\n",
      " |-- V26: double (nullable = true)\n",
      " |-- V27: double (nullable = true)\n",
      " |-- V28: double (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=====>                                                    (1 + 9) / 10]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Распределение классов: {1: 492, 0: 284315}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# === ЭТАП 3: АНАЛИЗ ДАННЫХ ===\n",
    "print(\"\\n3. АНАЛИЗ ДАННЫХ В SPARK...\")\n",
    "\n",
    "# Схема данных (типы столбцов)\n",
    "print(\"   Схема данных:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Распределение целевой переменной\n",
    "class_dist = df.groupBy(\"Class\").agg(count(\"*\").alias(\"count\")).collect()\n",
    "print(f\"   Распределение классов: { {row['Class']: row['count'] for row in class_dist} }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62290886-1f8f-43aa-a01d-b404b84a5368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. ПОДГОТОВКА ДАННЫХ ДЛЯ SPARK ML...\n",
      "   Признаки (30): ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n",
      "   ✓ Данные подготовлены за 0.14 секунд\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ Обучающая выборка: 199,727 samples\n",
      "   ✓ Тестовая выборка: 85,080 samples\n",
      "\n",
      "   Пример данных с векторными признаками:\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |Class|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|[0.0,-1.3598071336738,-0.0727811733098497,2.53634673796914,1.37815522427443,-0.338320769942518,0.462387777762292,0.239598554061257,0.0986979012610507,0.363786969611213,0.0907941719789316,-0.551599533260813,-0.617800855762348,-0.991389847235408,-0.311169353699879,1.46817697209427,-0.470400525259478,0.207971241929242,0.0257905801985591,0.403992960255733,0.251412098239705,-0.018306777944153,0.277837575558899,-0.110473910188767,0.0669280749146731,0.128539358273528,-0.189114843888824,0.133558376740387,-0.0210530534538215,149.62]  |0    |\n",
      "|[0.0,1.19185711131486,0.26615071205963,0.16648011335321,0.448154078460911,0.0600176492822243,-0.0823608088155687,-0.0788029833323113,0.0851016549148104,-0.255425128109186,-0.166974414004614,1.61272666105479,1.06523531137287,0.48909501589608,-0.143772296441519,0.635558093258208,0.463917041022171,-0.114804663102346,-0.183361270123994,-0.145783041325259,-0.0690831352230203,-0.225775248033138,-0.638671952771851,0.101288021253234,-0.339846475529127,0.167170404418143,0.125894532368176,-0.00898309914322813,0.0147241691924927,2.69]  |0    |\n",
      "|[1.0,-0.966271711572087,-0.185226008082898,1.79299333957872,-0.863291275036453,-0.0103088796030823,1.24720316752486,0.23760893977178,0.377435874652262,-1.38702406270197,-0.0549519224713749,-0.226487263835401,0.178228225877303,0.507756869957169,-0.28792374549456,-0.631418117709045,-1.0596472454325,-0.684092786345479,1.96577500349538,-1.2326219700892,-0.208037781160366,-0.108300452035545,0.00527359678253453,-0.190320518742841,-1.17557533186321,0.647376034602038,-0.221928844458407,0.0627228487293033,0.0614576285006353,123.5]    |0    |\n",
      "|[2.0,-1.15823309349523,0.877736754848451,1.548717846511,0.403033933955121,-0.407193377311653,0.0959214624684256,0.592940745385545,-0.270532677192282,0.817739308235294,0.753074431976354,-0.822842877946363,0.53819555014995,1.3458515932154,-1.11966983471731,0.175121130008994,-0.451449182813529,-0.237033239362776,-0.0381947870352842,0.803486924960175,0.408542360392758,-0.00943069713232919,0.79827849458971,-0.137458079619063,0.141266983824769,-0.206009587619756,0.502292224181569,0.219422229513348,0.215153147499206,69.99]          |0    |\n",
      "|[2.0,-0.425965884412454,0.960523044882985,1.14110934232219,-0.168252079760302,0.42098688077219,-0.0297275516639742,0.476200948720027,0.260314333074874,-0.56867137571251,-0.371407196834471,1.34126198001957,0.359893837038039,-0.358090652573631,-0.137133700217612,0.517616806555742,0.401725895589603,-0.0581328233640131,0.0686531494425432,-0.0331937877876282,0.0849676720682049,-0.208253514656728,-0.559824796253248,-0.0263976679795373,-0.371426583174346,-0.232793816737034,0.105914779097957,0.253844224739337,0.0810802569229443,3.67]|0    |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === ЭТАП 4: ПОДГОТОВКА ДАННЫХ ===\n",
    "print(\"\\n4. ПОДГОТОВКА ДАННЫХ ДЛЯ SPARK ML...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# В Spark ML все признаки должны быть собраны в один векторный столбец\n",
    "# Выбираем все столбцы кроме 'Class' как признаки\n",
    "feature_columns = [col for col in df.columns if col != 'Class']\n",
    "print(f\"   Признаки ({len(feature_columns)}): {feature_columns}\")\n",
    "\n",
    "# Создаем VectorAssembler - трансформатор для создания вектора признаков\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_columns,   # Входные столбцы-признаки\n",
    "    outputCol=\"features\"         # Выходной столбец с вектором\n",
    ")\n",
    "\n",
    "# Применяем трансформатор к данным\n",
    "featured_df = assembler.transform(df)\n",
    "\n",
    "# Разделяем на обучающую и тестовую выборки\n",
    "train_df, test_df = featured_df.randomSplit(\n",
    "    [0.7, 0.3],         # 70% train, 30% test\n",
    "    seed=42              # Для воспроизводимости\n",
    ")\n",
    "\n",
    "prep_time = time.time() - start_time\n",
    "print(f\"   ✓ Данные подготовлены за {prep_time:.2f} секунд\")\n",
    "print(f\"   ✓ Обучающая выборка: {train_df.count():,} samples\")\n",
    "print(f\"   ✓ Тестовая выборка: {test_df.count():,} samples\")\n",
    "\n",
    "# Показываем как выглядят данные после преобразования\n",
    "print(\"\\n   Пример данных с векторными признаками:\")\n",
    "train_df.select(\"features\", \"Class\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "debb88db-0fca-4cb1-a2f5-3c5d5692b66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. ОБУЧЕНИЕ SPARK ML МОДЕЛИ (RandomForest)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/03 16:59:39 WARN DAGScheduler: Broadcasting large task binary with size 1326.9 KiB\n",
      "25/11/03 16:59:40 WARN DAGScheduler: Broadcasting large task binary with size 1842.7 KiB\n",
      "25/11/03 16:59:40 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/11/03 16:59:41 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ Модель обучена за 9.71 секунд\n",
      "   ✓ Обучено деревьев: 100\n"
     ]
    }
   ],
   "source": [
    "# === ЭТАП 5: ОБУЧЕНИЕ МОДЕЛИ ===\n",
    "print(\"\\n5. ОБУЧЕНИЕ SPARK ML МОДЕЛИ (RandomForest)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Создаем модель RandomForest\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",      # Столбец с вектором признаков\n",
    "    labelCol=\"Class\",           # Столбец с целевой переменной\n",
    "    numTrees=100,               # Количество деревьев\n",
    "    maxDepth=10,                # Максимальная глубина\n",
    "    seed=42,                    # Для воспроизводимости\n",
    "    subsamplingRate=0.7         # Доля данных для каждого дерева\n",
    ")\n",
    "\n",
    "# Обучаем модель (распределенное обучение на кластере!)\n",
    "model = rf.fit(train_df)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print(f\"   ✓ Модель обучена за {train_time:.2f} секунд\")\n",
    "print(f\"   ✓ Обучено деревьев: {model.getNumTrees}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e034a35a-4d87-4ce6-aa63-8529b721c168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. ПРЕДСКАЗАНИЯ НА ТЕСТОВЫХ ДАННЫХ...\n",
      "   ✓ Предсказания сделаны за 0.03 секунд\n",
      "\n",
      "   Структура предсказаний:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/03 17:00:03 WARN DAGScheduler: Broadcasting large task binary with size 1786.2 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------+----------+\n",
      "|Class|probability                               |prediction|\n",
      "+-----+------------------------------------------+----------+\n",
      "|0    |[0.9999447855271627,5.5214472837262884E-5]|0.0       |\n",
      "|0    |[0.999958526320884,4.147367911593034E-5]  |0.0       |\n",
      "|0    |[0.9999418196015177,5.818039848223632E-5] |0.0       |\n",
      "|0    |[0.9999508182498783,4.9181750121580874E-5]|0.0       |\n",
      "|0    |[0.9999452862175261,5.471378247389914E-5] |0.0       |\n",
      "|0    |[0.9996615070532238,3.384929467762534E-4] |0.0       |\n",
      "|0    |[0.9999151592755368,8.48407244632346E-5]  |0.0       |\n",
      "|0    |[0.9999579160687273,4.2083931272711693E-5]|0.0       |\n",
      "|0    |[0.9999306116679467,6.938833205323081E-5] |0.0       |\n",
      "|0    |[0.999928245465879,7.175453412106176E-5]  |0.0       |\n",
      "+-----+------------------------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === ЭТАП 6: ПРЕДСКАЗАНИЯ ===\n",
    "print(\"\\n6. ПРЕДСКАЗАНИЯ НА ТЕСТОВЫХ ДАННЫХ...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Делаем предсказания - модель добавляет новые столбцы к DataFrame\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "predict_time = time.time() - start_time\n",
    "print(f\"   ✓ Предсказания сделаны за {predict_time:.2f} секунд\")\n",
    "\n",
    "# Показываем структуру предсказаний\n",
    "print(\"\\n   Структура предсказаний:\")\n",
    "predictions.select(\"Class\", \"probability\", \"prediction\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8672966-884c-4714-920e-3f990770f523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. ОЦЕНКА КАЧЕСТВА МОДЕЛИ:\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/03 17:00:29 WARN DAGScheduler: Broadcasting large task binary with size 1785.5 KiB\n",
      "25/11/03 17:00:31 WARN DAGScheduler: Broadcasting large task binary with size 1797.9 KiB\n",
      "25/11/03 17:00:32 WARN DAGScheduler: Broadcasting large task binary with size 1797.9 KiB\n",
      "25/11/03 17:00:32 WARN DAGScheduler: Broadcasting large task binary with size 1797.9 KiB\n",
      "25/11/03 17:00:33 WARN DAGScheduler: Broadcasting large task binary with size 1797.9 KiB\n",
      "25/11/03 17:00:33 WARN DAGScheduler: Broadcasting large task binary with size 1793.1 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ТОЧНОСТЬ (Accuracy):  0.9995\n",
      "   ТОЧНОСТЬ (Precision): 0.9995\n",
      "   ПОЛНОТА (Recall):     0.9995\n",
      "   F1-MEASURE:          0.9995\n",
      "   AUC-ROC:             0.9762\n",
      "\n",
      "   СТАТИСТИКА ПРЕДСКАЗАНИЙ:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/03 17:00:34 WARN DAGScheduler: Broadcasting large task binary with size 1768.4 KiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|Class|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    0|       0.0|84915|\n",
      "|    0|       1.0|    4|\n",
      "|    1|       0.0|   39|\n",
      "|    1|       1.0|  122|\n",
      "+-----+----------+-----+\n",
      "\n",
      "\n",
      "======================================================================\n",
      "ИТОГИ SPARK ПАЙПЛАЙНА:\n",
      "   • Общее время выполнения: 20.67 сек\n",
      "   • Время инициализации Spark: 4.13 сек\n",
      "   • Размер данных: 284,807 строк × 31 столбцов\n",
      "   • Partitions: 10\n",
      "   • Лучшая метрика (AUC-ROC): 0.9762\n",
      "======================================================================\n",
      "   ✓ Spark сессия остановлена\n"
     ]
    }
   ],
   "source": [
    "# === ЭТАП 7: ОЦЕНКА МОДЕЛИ ===\n",
    "print(\"\\n7. ОЦЕНКА КАЧЕСТВА МОДЕЛИ:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Evaluator для AUC-ROC (бинарная классификация)\n",
    "auc_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"Class\",\n",
    "    rawPredictionCol=\"rawPrediction\",  # Столбец с \"сырыми\" предсказаниями\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# Evaluator для других метрик\n",
    "multi_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Class\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "# Вычисляем метрики\n",
    "auc = auc_evaluator.evaluate(predictions)\n",
    "accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
    "precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\"})\n",
    "\n",
    "# Выводим метрики\n",
    "print(f\"   ТОЧНОСТЬ (Accuracy):  {accuracy:.4f}\")\n",
    "print(f\"   ТОЧНОСТЬ (Precision): {precision:.4f}\")\n",
    "print(f\"   ПОЛНОТА (Recall):     {recall:.4f}\")\n",
    "print(f\"   F1-MEASURE:          {f1:.4f}\")\n",
    "print(f\"   AUC-ROC:             {auc:.4f}\")\n",
    "\n",
    "# Детальная статистика по классам\n",
    "print(\"\\n   СТАТИСТИКА ПРЕДСКАЗАНИЙ:\")\n",
    "predictions.groupBy(\"Class\", \"prediction\").count().orderBy(\"Class\", \"prediction\").show()\n",
    "\n",
    "# === ЭТАП 8: ЗАВЕРШЕНИЕ РАБОТЫ ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ИТОГИ SPARK ПАЙПЛАЙНА:\")\n",
    "print(f\"   • Общее время выполнения: {init_time + load_time + prep_time + train_time + predict_time:.2f} сек\")\n",
    "print(f\"   • Время инициализации Spark: {init_time:.2f} сек\")\n",
    "print(f\"   • Размер данных: {df.count():,} строк × {len(df.columns)} столбцов\")\n",
    "print(f\"   • Partitions: {df.rdd.getNumPartitions()}\")\n",
    "print(f\"   • Лучшая метрика (AUC-ROC): {auc:.4f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Останавливаем Spark сессию\n",
    "spark.stop()\n",
    "print(\"   ✓ Spark сессия остановлена\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8677e440-383b-4242-acf7-8ce5d1b13b60",
   "metadata": {},
   "source": [
    "# Housing - SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e0c5a2-e2de-43e5-b1bd-5b94f1d401fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Spark пример для РЕГРЕССИИ на California Housing Dataset\n",
    "Запуск: spark-submit spark_regression.py\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean, stddev, count, when, isnan\n",
    "from pyspark.ml.feature import VectorAssembler, Imputer\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import time\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"SPARK РЕГРЕССИЯ - California Housing Dataset\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # === ЭТАП 1: ИНИЦИАЛИЗАЦИЯ SPARK СЕССИИ ===\n",
    "    print(\"\\n1. ИНИЦИАЛИЗАЦИЯ SPARK СЕССИИ...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"CaliforniaHousingRegression\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    init_time = time.time() - start_time\n",
    "    print(f\"   ✓ Spark сессия создана за {init_time:.2f} секунд\")\n",
    "    \n",
    "    # === ЭТАП 2: ЗАГРУЗКА И ОЧИСТКА ДАННЫХ ===\n",
    "    print(\"\\n2. ЗАГРУЗКА И ОЧИСТКА ДАННЫХ...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        df = spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .csv(\"/work/california_housing.csv\")\n",
    "        \n",
    "        # Удаляем текстовый признак 'ocean_proximity'\n",
    "        if 'ocean_proximity' in df.columns:\n",
    "            df = df.drop('ocean_proximity')\n",
    "            print(\"   ✓ Текстовый признак 'ocean_proximity' удален\")\n",
    "        \n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"   ✓ Данные загружены за {load_time:.2f} секунд\")\n",
    "        print(f\"   ✓ Размер данных: {df.count():,} строк, {len(df.columns)} столбцов\")\n",
    "        print(f\"   ✓ Столбцы после очистки: {df.columns}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ✗ Ошибка загрузки: {e}\")\n",
    "        print(\"   Скачайте датасет: https://www.kaggle.com/datasets/camnugent/california-housing-prices\")\n",
    "        spark.stop()\n",
    "        return\n",
    "    \n",
    "    # === ЭТАП 3: АНАЛИЗ ПРОПУЩЕННЫХ ЗНАЧЕНИЙ ===\n",
    "    print(\"\\n3. АНАЛИЗ ПРОПУЩЕННЫХ ЗНАЧЕНИЙ...\")\n",
    "    \n",
    "    # Проверяем пропущенные значения для каждого столбца\n",
    "    print(\"   Пропущенные значения до обработки:\")\n",
    "    missing_stats = []\n",
    "    for column in df.columns:\n",
    "        missing_count = df.filter(col(column).isNull() | isnan(col(column))).count()\n",
    "        total_count = df.count()\n",
    "        if missing_count > 0:\n",
    "            missing_percent = (missing_count / total_count) * 100\n",
    "            print(f\"     {column}: {missing_count} пропусков ({missing_percent:.1f}%)\")\n",
    "            missing_stats.append((column, missing_count, missing_percent))\n",
    "    \n",
    "    if not missing_stats:\n",
    "        print(\"     ✓ Пропущенных значений не обнаружено\")\n",
    "    \n",
    "    # Статистика по целевой переменной\n",
    "    target_col = \"median_house_value\"\n",
    "    stats = df.select(\n",
    "        mean(target_col).alias(\"mean\"),\n",
    "        stddev(target_col).alias(\"stddev\"),\n",
    "        col(target_col).min().alias(\"min\"),\n",
    "        col(target_col).max().alias(\"max\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"   Целевая переменная '{target_col}':\")\n",
    "    print(f\"     Min: ${stats['min']:,.0f}, Max: ${stats['max']:,.0f}\")\n",
    "    print(f\"     Mean: ${stats['mean']:,.0f}, Std: ${stats['stddev']:,.0f}\")\n",
    "    \n",
    "    # === ЭТАП 4: ОБРАБОТКА ПРОПУЩЕННЫХ ЗНАЧЕНИЙ ===\n",
    "    print(\"\\n4. ОБРАБОТКА ПРОПУЩЕННЫХ ЗНАЧЕНИЙ...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # В Spark ML используем Imputer для заполнения пропусков медианой\n",
    "    # Выбираем только числовые столбцы для импьютации\n",
    "    numeric_columns = [col_name for col_name, data_type in df.dtypes \n",
    "                      if data_type in ['int', 'double', 'float']]\n",
    "    \n",
    "    print(f\"   Числовые столбцы для обработки: {numeric_columns}\")\n",
    "    \n",
    "    # Создаем Imputer для заполнения пропусков медианой\n",
    "    imputer = Imputer(\n",
    "        inputCols=numeric_columns,\n",
    "        outputCols=[f\"{col}_imputed\" for col in numeric_columns],\n",
    "        strategy=\"median\"\n",
    "    )\n",
    "    \n",
    "    # Обучаем импьютер и преобразуем данные\n",
    "    imputer_model = imputer.fit(df)\n",
    "    df_imputed = imputer_model.transform(df)\n",
    "    \n",
    "    # Удаляем оригинальные столбцы и переименовываем импутированные\n",
    "    for col_name in numeric_columns:\n",
    "        df_imputed = df_imputed.drop(col_name).withColumnRenamed(f\"{col_name}_imputed\", col_name)\n",
    "    \n",
    "    # Проверяем результат\n",
    "    missing_after = 0\n",
    "    for column in df_imputed.columns:\n",
    "        missing_count = df_imputed.filter(col(column).isNull() | isnan(col(column))).count()\n",
    "        missing_after += missing_count\n",
    "    \n",
    "    print(f\"   ✓ Пропуски заполнены медианой\")\n",
    "    print(f\"   ✓ Пропущенных значений после обработки: {missing_after}\")\n",
    "    \n",
    "    # Показываем какие столбцы были обработаны\n",
    "    for col_name, missing_count, missing_percent in missing_stats:\n",
    "        print(f\"     {col_name}: {missing_count} пропусков → заполнено медианой\")\n",
    "    \n",
    "    prep_time = time.time() - start_time\n",
    "    print(f\"   ✓ Обработка пропусков выполнена за {prep_time:.2f} секунд\")\n",
    "    \n",
    "    # === ЭТАП 5: ПОДГОТОВКА ДАННЫХ ДЛЯ SPARK ML ===\n",
    "    print(\"\\n5. ПОДГОТОВКА ДАННЫХ ДЛЯ SPARK ML...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # В Spark ML для регрессии нужен векторный столбец с признаками\n",
    "    feature_columns = [col for col in df_imputed.columns if col != target_col]\n",
    "    print(f\"   Числовые признаки ({len(feature_columns)}): {feature_columns}\")\n",
    "    \n",
    "    # Создаем VectorAssembler для объединения признаков в вектор\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=feature_columns,\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "    \n",
    "    # Применяем трансформатор\n",
    "    featured_df = assembler.transform(df_imputed)\n",
    "    \n",
    "    # Разделяем на обучающую и тестовую выборки\n",
    "    train_df, test_df = featured_df.randomSplit(\n",
    "        [0.7, 0.3],    # 70% train, 30% test\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    assemble_time = time.time() - start_time\n",
    "    print(f\"   ✓ Данные подготовлены за {assemble_time:.2f} секунд\")\n",
    "    print(f\"   ✓ Обучающая выборка: {train_df.count():,} samples\")\n",
    "    print(f\"   ✓ Тестовая выборка: {test_df.count():,} samples\")\n",
    "    \n",
    "    # Показываем пример данных с векторными признаками\n",
    "    print(\"\\n   Пример данных с векторными признаками:\")\n",
    "    train_df.select(\"features\", target_col).show(5, truncate=False)\n",
    "    \n",
    "    # === ЭТАП 6: ОБУЧЕНИЕ МОДЕЛИ ГРАДИЕНТНЫЙ БУСТИНГ ===\n",
    "    print(\"\\n6. ОБУЧЕНИЕ SPARK ML МОДЕЛИ (GBTRegressor)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Создаем модель Gradient Boosted Trees для регрессии\n",
    "    gbt = GBTRegressor(\n",
    "        featuresCol=\"features\",      # Столбец с вектором признаков\n",
    "        labelCol=target_col,         # Столбец с целевой переменной\n",
    "        maxIter=100,                 # Количество деревьев (итераций)\n",
    "        maxDepth=3,                  # Максимальная глубина деревьев\n",
    "        stepSize=0.1,               # Темп обучения (learning rate)\n",
    "        seed=42,                     # Для воспроизводимости\n",
    "        subsamplingRate=0.8         # Доля данных для каждого дерева\n",
    "    )\n",
    "    \n",
    "    # Обучаем модель\n",
    "    model = gbt.fit(train_df)\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"   ✓ Модель обучена за {train_time:.2f} секунд\")\n",
    "    print(f\"   ✓ Обучено деревьев: {model.getNumTrees}\")\n",
    "    \n",
    "    # === ЭТАП 7: ПРЕДСКАЗАНИЯ ===\n",
    "    print(\"\\n7. ПРЕДСКАЗАНИЯ НА ТЕСТОВЫХ ДАННЫХ...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Делаем предсказания\n",
    "    predictions = model.transform(test_df)\n",
    "    \n",
    "    predict_time = time.time() - start_time\n",
    "    print(f\"   ✓ Предсказания сделаны за {predict_time:.2f} секунд\")\n",
    "    \n",
    "    # Показываем структуру предсказаний\n",
    "    print(\"\\n   Пример предсказаний:\")\n",
    "    predictions.select(target_col, \"prediction\").show(10, truncate=False)\n",
    "    \n",
    "    # === ЭТАП 8: ОЦЕНКА МОДЕЛИ РЕГРЕССИИ ===\n",
    "    print(\"\\n8. ОЦЕНКА КАЧЕСТВА РЕГРЕССИИ:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Создаем evaluators для разных метрик регрессии\n",
    "    evaluator_mse = RegressionEvaluator(\n",
    "        labelCol=target_col,\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"mse\"\n",
    "    )\n",
    "    \n",
    "    evaluator_rmse = RegressionEvaluator(\n",
    "        labelCol=target_col,\n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"rmse\"\n",
    "    )\n",
    "    \n",
    "    evaluator_r2 = RegressionEvaluator(\n",
    "        labelCol=target_col,\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"r2\"\n",
    "    )\n",
    "    \n",
    "    evaluator_mae = RegressionEvaluator(\n",
    "        labelCol=target_col,\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"mae\"\n",
    "    )\n",
    "    \n",
    "    # Вычисляем все метрики\n",
    "    mse = evaluator_mse.evaluate(predictions)\n",
    "    rmse = evaluator_rmse.evaluate(predictions)\n",
    "    r2 = evaluator_r2.evaluate(predictions)\n",
    "    mae = evaluator_mae.evaluate(predictions)\n",
    "    \n",
    "    # Выводим метрики\n",
    "    print(f\"   СРЕДНЯЯ АБСОЛЮТНАЯ ОШИБКА (MAE):    ${mae:,.0f}\")\n",
    "    print(f\"   СРЕДНЯЯ КВАДРАТИЧНАЯ ОШИБКА (MSE):  ${mse:,.0f}\")\n",
    "    print(f\"   КОРЕНЬ ИЗ MSE (RMSE):               ${rmse:,.0f}\")\n",
    "    print(f\"   КОЭФФИЦИЕНТ ДЕТЕРМИНАЦИИ (R²):      {r2:.4f}\")\n",
    "    \n",
    "    # Интерпретация R²\n",
    "    print(f\"\\n   R² = {r2:.1%} - модель объясняет {r2:.1%} дисперсии целевой переменной\")\n",
    "    \n",
    "    # === ЭТАП 9: АНАЛИЗ ВАЖНОСТИ ПРИЗНАКОВ ===\n",
    "    print(\"\\n9. ВАЖНОСТЬ ПРИЗНАКОВ:\")\n",
    "    feature_importances = list(zip(feature_columns, model.featureImportances))\n",
    "    sorted_importances = sorted(feature_importances, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"   Признак -> Важность:\")\n",
    "    for feature, importance in sorted_importances:\n",
    "        print(f\"   {feature:20} {importance:.4f}\")\n",
    "    \n",
    "    # === ЭТАП 10: ЗАВЕРШЕНИЕ РАБОТЫ ===\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ИТОГИ SPARK РЕГРЕССИИ:\")\n",
    "    print(f\"   • Общее время выполнения: {init_time + load_time + prep_time + assemble_time + train_time + predict_time:.2f} сек\")\n",
    "    print(f\"   • Время инициализации Spark: {init_time:.2f} сек\")\n",
    "    print(f\"   • Размер данных: {df.count():,} строк × {len(df.columns)} столбцов\")\n",
    "    print(f\"   • Обработано пропусков: {sum(missing[1] for missing in missing_stats)} значений\")\n",
    "    print(f\"   • Все признаки числовые: {df_imputed.columns}\")\n",
    "    print(f\"   • Лучшая метрика (R²): {r2:.4f}\")\n",
    "    print(f\"   • Ошибка предсказания: ±${rmse:,.0f}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Останавливаем Spark сессию\n",
    "    spark.stop()\n",
    "    print(\"   ✓ Spark сессия остановлена\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0ac6f1-2084-4fa0-a907-5cba4fa774bf",
   "metadata": {},
   "source": [
    "# Housing - SKLEARN - Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09ba1663-fc98-400f-8ec8-d92adc4cc23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Spark пример для РЕГРЕССИИ на California Housing Dataset\n",
    "Запуск: spark-submit spark_regression.py\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean, stddev, count, when, isnan\n",
    "from pyspark.ml.feature import VectorAssembler, Imputer\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import time\n",
    "\n",
    "from pyspark.sql.functions import mean, stddev, min as spark_min, max as spark_max, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d4dba40-4049-489d-b4ee-1d4011ba0ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SPARK РЕГРЕССИЯ - California Housing Dataset\n",
      "======================================================================\n",
      "\n",
      "1. ИНИЦИАЛИЗАЦИЯ SPARK СЕССИИ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/03 20:31:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ Spark сессия создана за 3.43 секунд\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SPARK РЕГРЕССИЯ - California Housing Dataset\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# === ЭТАП 1: ИНИЦИАЛИЗАЦИЯ SPARK СЕССИИ ===\n",
    "print(\"\\n1. ИНИЦИАЛИЗАЦИЯ SPARK СЕССИИ...\")\n",
    "start_time = time.time()\n",
    "\n",
    "drivers = [\n",
    "    \"/home/jovyan/work/spark-jars/hadoop-aws-3.3.4.jar\",             # S3\n",
    "    \"/home/jovyan/work/spark-jars/aws-java-sdk-bundle-1.12.262.jar\", # S3\n",
    "    \"/home/jovyan/work/spark-jars/wildfly-openssl-1.0.7.Final.jar\",  # S3\n",
    "    \"/home/jovyan/work/spark-jars/postgresql-42.6.0.jar\",            # PostgreSQL\n",
    "]\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"mustdayker-Spark\")\n",
    "         .master(\"spark://spark-master:7077\") \n",
    "         .config(\"spark.jars\", \",\".join(drivers))\n",
    "         .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "         .getOrCreate()\n",
    "        )\n",
    "\n",
    "init_time = time.time() - start_time\n",
    "print(f\"   ✓ Spark сессия создана за {init_time:.2f} секунд\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c89a6f1a-5311-487a-92c4-4dc1f13d980b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. ЗАГРУЗКА И ОЧИСТКА ДАННЫХ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ Текстовый признак 'ocean_proximity' удален\n",
      "   ✓ Данные загружены за 6.72 секунд\n",
      "   ✓ Размер данных: 20,640 строк, 9 столбцов\n",
      "   ✓ Столбцы после очистки: ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value']\n"
     ]
    }
   ],
   "source": [
    "# === ЭТАП 2: ЗАГРУЗКА И ОЧИСТКА ДАННЫХ ===\n",
    "print(\"\\n2. ЗАГРУЗКА И ОЧИСТКА ДАННЫХ...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(\"/shared_data/housing.csv\")\n",
    "    \n",
    "    # Удаляем текстовый признак 'ocean_proximity'\n",
    "    if 'ocean_proximity' in df.columns:\n",
    "        df = df.drop('ocean_proximity')\n",
    "        print(\"   ✓ Текстовый признак 'ocean_proximity' удален\")\n",
    "    \n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"   ✓ Данные загружены за {load_time:.2f} секунд\")\n",
    "    print(f\"   ✓ Размер данных: {df.count():,} строк, {len(df.columns)} столбцов\")\n",
    "    print(f\"   ✓ Столбцы после очистки: {df.columns}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ✗ Ошибка загрузки: {e}\")\n",
    "    print(\"   Скачайте датасет: https://www.kaggle.com/datasets/camnugent/california-housing-prices\")\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67703cfd-b168-4a25-abac-17ebcc30e37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. АНАЛИЗ ПРОПУЩЕННЫХ ЗНАЧЕНИЙ...\n",
      "   Пропущенные значения до обработки:\n",
      "     total_bedrooms: 207 пропусков (1.0%)\n"
     ]
    }
   ],
   "source": [
    "# === ЭТАП 3: АНАЛИЗ ПРОПУЩЕННЫХ ЗНАЧЕНИЙ ===\n",
    "print(\"\\n3. АНАЛИЗ ПРОПУЩЕННЫХ ЗНАЧЕНИЙ...\")\n",
    "\n",
    "# Проверяем пропущенные значения для каждого столбца\n",
    "print(\"   Пропущенные значения до обработки:\")\n",
    "missing_stats = []\n",
    "for column in df.columns:\n",
    "    missing_count = df.filter(col(column).isNull() | isnan(col(column))).count()\n",
    "    total_count = df.count()\n",
    "    if missing_count > 0:\n",
    "        missing_percent = (missing_count / total_count) * 100\n",
    "        print(f\"     {column}: {missing_count} пропусков ({missing_percent:.1f}%)\")\n",
    "        missing_stats.append((column, missing_count, missing_percent))\n",
    "\n",
    "if not missing_stats:\n",
    "    print(\"     ✓ Пропущенных значений не обнаружено\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17e1a457-d174-487a-a57c-c4f07477a976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Целевая переменная 'median_house_value':\n",
      "     Min: $14,999, Max: $500,001\n",
      "     Mean: $206,856, Std: $115,396\n"
     ]
    }
   ],
   "source": [
    "# Статистика по целевой переменной\n",
    "target_col = \"median_house_value\"\n",
    "# stats = df.select(\n",
    "#     mean(target_col).alias(\"mean\"),\n",
    "#     stddev(target_col).alias(\"stddev\"),\n",
    "#     col(target_col).min().alias(\"min\"),\n",
    "#     col(target_col).max().alias(\"max\")\n",
    "# ).collect()[0]\n",
    "\n",
    "stats = df.select(\n",
    "    mean(col(target_col)).alias(\"mean\"),\n",
    "    stddev(col(target_col)).alias(\"stddev\"),\n",
    "    spark_min(col(target_col)).alias(\"min\"),\n",
    "    spark_max(col(target_col)).alias(\"max\")\n",
    ").collect()[0]\n",
    "\n",
    "\n",
    "print(f\"   Целевая переменная '{target_col}':\")\n",
    "print(f\"     Min: ${stats['min']:,.0f}, Max: ${stats['max']:,.0f}\")\n",
    "print(f\"     Mean: ${stats['mean']:,.0f}, Std: ${stats['stddev']:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7ae1f7c-b633-49ff-a106-859ce6164ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. ОБРАБОТКА ПРОПУЩЕННЫХ ЗНАЧЕНИЙ...\n",
      "   Числовые столбцы для обработки: ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ Пропуски заполнены медианой\n",
      "   ✓ Пропущенных значений после обработки: 0\n",
      "     total_bedrooms: 207 пропусков → заполнено медианой\n",
      "   ✓ Обработка пропусков выполнена за 2.50 секунд\n"
     ]
    }
   ],
   "source": [
    "# === ЭТАП 4: ОБРАБОТКА ПРОПУЩЕННЫХ ЗНАЧЕНИЙ ===\n",
    "print(\"\\n4. ОБРАБОТКА ПРОПУЩЕННЫХ ЗНАЧЕНИЙ...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# В Spark ML используем Imputer для заполнения пропусков медианой\n",
    "# Выбираем только числовые столбцы для импьютации\n",
    "numeric_columns = [col_name for col_name, data_type in df.dtypes \n",
    "                  if data_type in ['int', 'double', 'float']]\n",
    "\n",
    "print(f\"   Числовые столбцы для обработки: {numeric_columns}\")\n",
    "\n",
    "# Создаем Imputer для заполнения пропусков медианой\n",
    "imputer = Imputer(\n",
    "    inputCols=numeric_columns,\n",
    "    outputCols=[f\"{col}_imputed\" for col in numeric_columns],\n",
    "    strategy=\"median\"\n",
    ")\n",
    "\n",
    "# Обучаем импьютер и преобразуем данные\n",
    "imputer_model = imputer.fit(df)\n",
    "df_imputed = imputer_model.transform(df)\n",
    "\n",
    "# Удаляем оригинальные столбцы и переименовываем импутированные\n",
    "for col_name in numeric_columns:\n",
    "    df_imputed = df_imputed.drop(col_name).withColumnRenamed(f\"{col_name}_imputed\", col_name)\n",
    "\n",
    "# Проверяем результат\n",
    "missing_after = 0\n",
    "for column in df_imputed.columns:\n",
    "    missing_count = df_imputed.filter(col(column).isNull() | isnan(col(column))).count()\n",
    "    missing_after += missing_count\n",
    "\n",
    "print(f\"   ✓ Пропуски заполнены медианой\")\n",
    "print(f\"   ✓ Пропущенных значений после обработки: {missing_after}\")\n",
    "\n",
    "# Показываем какие столбцы были обработаны\n",
    "for col_name, missing_count, missing_percent in missing_stats:\n",
    "    print(f\"     {col_name}: {missing_count} пропусков → заполнено медианой\")\n",
    "\n",
    "prep_time = time.time() - start_time\n",
    "print(f\"   ✓ Обработка пропусков выполнена за {prep_time:.2f} секунд\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "062da0bf-1b5b-4188-88a8-bd4a2e869ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. ПОДГОТОВКА ДАННЫХ ДЛЯ SPARK ML...\n",
      "   Числовые признаки (8): ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']\n",
      "   ✓ Данные подготовлены за 0.10 секунд\n",
      "   ✓ Обучающая выборка: 14,509 samples\n",
      "   ✓ Тестовая выборка: 6,131 samples\n",
      "\n",
      "   Пример данных с векторными признаками:\n",
      "+-----------------------------------------------------+------------------+\n",
      "|features                                             |median_house_value|\n",
      "+-----------------------------------------------------+------------------+\n",
      "|[-124.35,40.54,52.0,1820.0,300.0,806.0,270.0,3.0147] |94600.0           |\n",
      "|[-124.3,41.8,19.0,2672.0,552.0,1298.0,478.0,1.9797]  |85800.0           |\n",
      "|[-124.27,40.69,36.0,2349.0,528.0,1194.0,465.0,2.5179]|79000.0           |\n",
      "|[-124.26,40.58,52.0,2217.0,394.0,907.0,369.0,2.3571] |111400.0          |\n",
      "|[-124.25,40.28,32.0,1430.0,419.0,434.0,187.0,1.9417] |76100.0           |\n",
      "+-----------------------------------------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === ЭТАП 5: ПОДГОТОВКА ДАННЫХ ДЛЯ SPARK ML ===\n",
    "print(\"\\n5. ПОДГОТОВКА ДАННЫХ ДЛЯ SPARK ML...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# В Spark ML для регрессии нужен векторный столбец с признаками\n",
    "feature_columns = [col for col in df_imputed.columns if col != target_col]\n",
    "print(f\"   Числовые признаки ({len(feature_columns)}): {feature_columns}\")\n",
    "\n",
    "# Создаем VectorAssembler для объединения признаков в вектор\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_columns,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Применяем трансформатор\n",
    "featured_df = assembler.transform(df_imputed)\n",
    "\n",
    "# Разделяем на обучающую и тестовую выборки\n",
    "train_df, test_df = featured_df.randomSplit(\n",
    "    [0.7, 0.3],    # 70% train, 30% test\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "assemble_time = time.time() - start_time\n",
    "print(f\"   ✓ Данные подготовлены за {assemble_time:.2f} секунд\")\n",
    "print(f\"   ✓ Обучающая выборка: {train_df.count():,} samples\")\n",
    "print(f\"   ✓ Тестовая выборка: {test_df.count():,} samples\")\n",
    "\n",
    "# Показываем пример данных с векторными признаками\n",
    "print(\"\\n   Пример данных с векторными признаками:\")\n",
    "train_df.select(\"features\", target_col).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5ed63ba-54e6-46b7-8e67-5f48e1b0f440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. ОБУЧЕНИЕ SPARK ML МОДЕЛИ (GBTRegressor)...\n",
      "   ✓ Модель обучена за 15.09 секунд\n",
      "   ✓ Обучено деревьев: 100\n"
     ]
    }
   ],
   "source": [
    "# === ЭТАП 6: ОБУЧЕНИЕ МОДЕЛИ ГРАДИЕНТНЫЙ БУСТИНГ ===\n",
    "print(\"\\n6. ОБУЧЕНИЕ SPARK ML МОДЕЛИ (GBTRegressor)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Создаем модель Gradient Boosted Trees для регрессии\n",
    "gbt = GBTRegressor(\n",
    "    featuresCol=\"features\",      # Столбец с вектором признаков\n",
    "    labelCol=target_col,         # Столбец с целевой переменной\n",
    "    maxIter=100,                 # Количество деревьев (итераций)\n",
    "    maxDepth=3,                  # Максимальная глубина деревьев\n",
    "    stepSize=0.1,               # Темп обучения (learning rate)\n",
    "    seed=42,                     # Для воспроизводимости\n",
    "    subsamplingRate=0.8         # Доля данных для каждого дерева\n",
    ")\n",
    "\n",
    "# Обучаем модель\n",
    "model = gbt.fit(train_df)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print(f\"   ✓ Модель обучена за {train_time:.2f} секунд\")\n",
    "print(f\"   ✓ Обучено деревьев: {model.getNumTrees}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a34cce77-19d7-43b2-9081-bc83c8f39962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. ПРЕДСКАЗАНИЯ НА ТЕСТОВЫХ ДАННЫХ...\n",
      "   ✓ Предсказания сделаны за 0.10 секунд\n",
      "\n",
      "   Пример предсказаний:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/opt/spark/jars/spark-core_2.12-3.5.0.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|median_house_value|prediction        |\n",
      "+------------------+------------------+\n",
      "|103600.0          |98551.11233609286 |\n",
      "|106700.0          |139656.5624114286 |\n",
      "|73200.0           |90367.54730552975 |\n",
      "|78300.0           |100622.5160580698 |\n",
      "|90100.0           |133051.29064697694|\n",
      "|69000.0           |95306.63816421998 |\n",
      "|70000.0           |90113.78742989336 |\n",
      "|67000.0           |71014.71124352298 |\n",
      "|70500.0           |61170.070879167695|\n",
      "|86400.0           |101184.34011455691|\n",
      "+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === ЭТАП 7: ПРЕДСКАЗАНИЯ ===\n",
    "print(\"\\n7. ПРЕДСКАЗАНИЯ НА ТЕСТОВЫХ ДАННЫХ...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Делаем предсказания\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "predict_time = time.time() - start_time\n",
    "print(f\"   ✓ Предсказания сделаны за {predict_time:.2f} секунд\")\n",
    "\n",
    "# Показываем структуру предсказаний\n",
    "print(\"\\n   Пример предсказаний:\")\n",
    "predictions.select(target_col, \"prediction\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4400acc-1cba-4e5f-a36a-7131d4a20fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. ОЦЕНКА КАЧЕСТВА РЕГРЕССИИ:\n",
      "--------------------------------------------------\n",
      "   СРЕДНЯЯ АБСОЛЮТНАЯ ОШИБКА (MAE):    $36,862\n",
      "   СРЕДНЯЯ КВАДРАТИЧНАЯ ОШИБКА (MSE):  $2,808,035,892\n",
      "   КОРЕНЬ ИЗ MSE (RMSE):               $52,991\n",
      "   КОЭФФИЦИЕНТ ДЕТЕРМИНАЦИИ (R²):      0.7921\n",
      "\n",
      "   R² = 79.2% - модель объясняет 79.2% дисперсии целевой переменной\n"
     ]
    }
   ],
   "source": [
    "# === ЭТАП 8: ОЦЕНКА МОДЕЛИ РЕГРЕССИИ ===\n",
    "print(\"\\n8. ОЦЕНКА КАЧЕСТВА РЕГРЕССИИ:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Создаем evaluators для разных метрик регрессии\n",
    "evaluator_mse = RegressionEvaluator(\n",
    "    labelCol=target_col,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"mse\"\n",
    ")\n",
    "\n",
    "evaluator_rmse = RegressionEvaluator(\n",
    "    labelCol=target_col,\n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "evaluator_r2 = RegressionEvaluator(\n",
    "    labelCol=target_col,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "\n",
    "evaluator_mae = RegressionEvaluator(\n",
    "    labelCol=target_col,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"mae\"\n",
    ")\n",
    "\n",
    "# Вычисляем все метрики\n",
    "mse = evaluator_mse.evaluate(predictions)\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "\n",
    "# Выводим метрики\n",
    "print(f\"   СРЕДНЯЯ АБСОЛЮТНАЯ ОШИБКА (MAE):    ${mae:,.0f}\")\n",
    "print(f\"   СРЕДНЯЯ КВАДРАТИЧНАЯ ОШИБКА (MSE):  ${mse:,.0f}\")\n",
    "print(f\"   КОРЕНЬ ИЗ MSE (RMSE):               ${rmse:,.0f}\")\n",
    "print(f\"   КОЭФФИЦИЕНТ ДЕТЕРМИНАЦИИ (R²):      {r2:.4f}\")\n",
    "\n",
    "# Интерпретация R²\n",
    "print(f\"\\n   R² = {r2:.1%} - модель объясняет {r2:.1%} дисперсии целевой переменной\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb647228-7f2d-44a1-bab1-c5b6cfd28bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9. ВАЖНОСТЬ ПРИЗНАКОВ:\n",
      "   Признак -> Важность:\n",
      "   longitude            0.2918\n",
      "   median_income        0.2479\n",
      "   latitude             0.2334\n",
      "   population           0.0719\n",
      "   total_bedrooms       0.0569\n",
      "   housing_median_age   0.0543\n",
      "   households           0.0276\n",
      "   total_rooms          0.0161\n"
     ]
    }
   ],
   "source": [
    "# === ЭТАП 9: АНАЛИЗ ВАЖНОСТИ ПРИЗНАКОВ ===\n",
    "print(\"\\n9. ВАЖНОСТЬ ПРИЗНАКОВ:\")\n",
    "feature_importances = list(zip(feature_columns, model.featureImportances))\n",
    "sorted_importances = sorted(feature_importances, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"   Признак -> Важность:\")\n",
    "for feature, importance in sorted_importances:\n",
    "    print(f\"   {feature:20} {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cff2330e-ba0f-45da-90f4-f22de41198f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ИТОГИ SPARK РЕГРЕССИИ:\n",
      "   • Общее время выполнения: 27.94 сек\n",
      "   • Время инициализации Spark: 3.43 сек\n",
      "   • Размер данных: 20,640 строк × 9 столбцов\n",
      "   • Обработано пропусков: 207 значений\n",
      "   • Все признаки числовые: ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value']\n",
      "   • Лучшая метрика (R²): 0.7921\n",
      "   • Ошибка предсказания: ±$52,991\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# === ЭТАП 10: ЗАВЕРШЕНИЕ РАБОТЫ ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ИТОГИ SPARK РЕГРЕССИИ:\")\n",
    "print(f\"   • Общее время выполнения: {init_time + load_time + prep_time + assemble_time + train_time + predict_time:.2f} сек\")\n",
    "print(f\"   • Время инициализации Spark: {init_time:.2f} сек\")\n",
    "print(f\"   • Размер данных: {df.count():,} строк × {len(df.columns)} столбцов\")\n",
    "print(f\"   • Обработано пропусков: {sum(missing[1] for missing in missing_stats)} значений\")\n",
    "print(f\"   • Все признаки числовые: {df_imputed.columns}\")\n",
    "print(f\"   • Лучшая метрика (R²): {r2:.4f}\")\n",
    "print(f\"   • Ошибка предсказания: ±${rmse:,.0f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6eb464b-303a-447d-b516-06c7afb3cb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ Spark сессия остановлена\n"
     ]
    }
   ],
   "source": [
    "# Останавливаем Spark сессию\n",
    "spark.stop()\n",
    "print(\"   ✓ Spark сессия остановлена\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d77936-b3a3-4959-b375-51e1fe352174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
