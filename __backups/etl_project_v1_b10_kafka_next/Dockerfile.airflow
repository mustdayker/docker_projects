# Базовый образ Airflow
FROM apache/airflow:2.7.1

# Переключаемся на root для установки системных зависимостей
USER root

# Устанавливаем системные зависимости
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        gcc \
        g++ \
        openjdk-11-jdk \
        wget \
        curl \
        netcat \
        && rm -rf /var/lib/apt/lists/*

# Устанавливаем Java
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
ENV PATH=$JAVA_HOME/bin:$PATH

# Копируем локальный Spark архив и распаковываем
COPY images/spark-3.5.0-bin-hadoop3.tgz /tmp/
RUN tar -xzf /tmp/spark-3.5.0-bin-hadoop3.tgz -C /opt/ && \
    mv /opt/spark-3.5.0-bin-hadoop3 /opt/spark && \
    rm /tmp/spark-3.5.0-bin-hadoop3.tgz

# Настраиваем переменные окружения Spark
ENV SPARK_HOME=/opt/spark
ENV PATH=$SPARK_HOME/bin:$PATH
ENV PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH

# Создаем симлинки для совместимости
RUN ln -s /opt/spark/bin/spark-submit /usr/local/bin/spark-submit && \
    ln -s /opt/spark/bin/spark-class /usr/local/bin/spark-class && \
    ln -s /opt/spark/bin/spark-shell /usr/local/bin/spark-shell && \
    ln -s /opt/spark/bin/pyspark /usr/local/bin/pyspark

# Создаем необходимые директории
RUN mkdir -p /opt/spark/logs /opt/spark/work

# Возвращаемся к пользователю airflow для установки Python пакетов
USER airflow

# Копируем ВСЮ папку requirements и устанавливаем зависимости
COPY --chown=airflow:airflow requirements /tmp/requirements

# Устанавливаем Python зависимости
RUN pip install --no-cache-dir -r /tmp/requirements/airflow.txt && \
    pip install --no-cache-dir -r /tmp/requirements/base.txt

# Проверяем установку
RUN java -version && \
    spark-submit --version && \
    python -c "import pyspark; print('PySpark version:', pyspark.__version__)"