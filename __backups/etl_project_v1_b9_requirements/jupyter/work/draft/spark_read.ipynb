{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e17de36-c8e0-485e-bd0c-bb2cb68de93e",
   "metadata": {},
   "source": [
    "# –ß—Ç–µ–Ω–∏–µ Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f65ea4a-e659-47a6-b346-dbe748543ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "drivers = [\n",
    "    \"/home/jovyan/work/spark-jars/hadoop-aws-3.3.4.jar\",             # S3\n",
    "    \"/home/jovyan/work/spark-jars/aws-java-sdk-bundle-1.12.262.jar\", # S3\n",
    "    \"/home/jovyan/work/spark-jars/wildfly-openssl-1.0.7.Final.jar\",  # S3\n",
    "    \"/home/jovyan/work/spark-jars/postgresql-42.6.0.jar\",            # PostgreSQL\n",
    "]\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"mustdayker-Spark\")\n",
    "         .master(\"spark://spark-master:7077\") \n",
    "         .config(\"spark.jars\", \",\".join(drivers))\n",
    "         .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb44935-f911-4ae2-ad5d-796fd9e4581f",
   "metadata": {},
   "source": [
    "## –ß—Ç–µ–Ω–∏–µ —Å –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –¥–∏—Å–∫–∞ `CSV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f20761-78be-41dc-b51a-bbdb60591acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/shared_data/bmw.csv\", \n",
    "                    header=True, \n",
    "                    inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e931d873-2afb-477e-a824-612f2c8f7a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ß—Ç–µ–Ω–∏–µ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ –ø–∞–ø–∫–∏\n",
    "# –í–ê–ñ–ù–û! –ú–æ–∂–Ω–æ —á–∏—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ —Ñ–∞–π–ª—ã\n",
    "\n",
    "df = spark.read.csv(\"/shared_data/*.csv\", \n",
    "                    header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20969f4e-8e40-4d49-aefc-8fcd2c46ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ß—Ç–µ–Ω–∏–µ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
    "df = (spark.read\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"delimiter\", \",\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .option(\"encoding\", \"utf-8\")\n",
    "      .option(\"nullValue\", \"NULL\")\n",
    "      .csv(\"/shared_data/bmw.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f77d0e4-8da7-42a7-8187-1bfba895644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞\n",
    "df = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .load(\"/shared_data/bmw.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca09457-f35f-454a-a994-f43f3a322162",
   "metadata": {},
   "source": [
    "## –ß—Ç–µ–Ω–∏–µ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —Å—Ö–µ–º—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fdd168-4325-4f88-b637-7a35076a6ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ß—Ç–µ–Ω–∏–µ CSV —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —Å—Ö–µ–º—ã\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ö–µ–º—É –¥–ª—è CSV —Ñ–∞–π–ª–∞\n",
    "schema = StructType([\n",
    "    StructField(\"Model\",                StringType(),  True),\n",
    "    StructField(\"Year\",                 IntegerType(), True),\n",
    "    StructField(\"Region\",               StringType(),  True),\n",
    "    StructField(\"Color\",                StringType(),  True),\n",
    "    StructField(\"Fuel_Type\",            StringType(),  True),\n",
    "    StructField(\"Transmission\",         StringType(),  True),\n",
    "    StructField(\"Engine_Size_L\",        DoubleType(),  True),\n",
    "    StructField(\"Mileage_KM\",           IntegerType(), True),\n",
    "    StructField(\"Price_USD\",            IntegerType(), True),\n",
    "    StructField(\"Sales_Volume\",         IntegerType(), True),\n",
    "    StructField(\"Sales_Classification\", StringType(),  True)\n",
    "])\n",
    "            \n",
    "df = spark.read.csv(\"/shared_data/bmw.csv\",\n",
    "                    schema=schema,\n",
    "                    header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f37bd7-bff9-445a-8845-9104bdd3c4a8",
   "metadata": {},
   "source": [
    "## –ß—Ç–µ–Ω–∏–µ –∏–∑ MinIO `CSV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08d117a-5fa0-4d65-82e6-69b8ab946bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "           .option(\"header\", \"false\")\n",
    "           .csv(\"s3a://learn-bucket/draft/bmw_csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2bdec4-441f-457d-b0aa-b0b54f18d3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f369274c-cb22-45bf-bfa5-d41cd87b4512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47bf5d64-165f-4683-b147-969cde62e98d",
   "metadata": {},
   "source": [
    "# Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0f8559-b4dc-45bb-9294-f9d854e6c029",
   "metadata": {},
   "source": [
    "### –° –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –¥–∏—Å–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9193ca-91f9-4c0e-a902-a63b6004f31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/shared_data/yellow_taxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f56f745-ef78-40bd-8111-6772211fcfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/shared_data/yellow_taxi/data_*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6843620b-a473-4c93-a43a-f77e3e22f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/shared_data/yellow_taxi/single_file.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec109fcb-929b-4689-81d4-0e71803a417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = \"silver\"\n",
    "\n",
    "df = (spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load([\n",
    "          f\"s3a://{layer}/nyc-taxi-data/yellow_tripdata_2023-*.parquet\",\n",
    "          f\"s3a://{layer}/nyc-taxi-data/yellow_tripdata_2024-*.parquet\",\n",
    "          f\"s3a://{layer}/nyc-taxi-data/yellow_tripdata_2025-*.parquet\",\n",
    "      ])\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16900153-e836-49da-9cac-7876ca647291",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫: {df.count()}\\n\")\n",
    "df.show(1)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f332e6b-25e3-4f0f-b726-1117ae7de9f0",
   "metadata": {},
   "source": [
    "–û—Ç–ª–∏—á–Ω–æ, –ø–æ–Ω—è–ª! –°–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏–º—Å—è –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ —Ç–µ—Ö–Ω–∏–∫–∞—Ö —á—Ç–µ–Ω–∏—è Parquet –≤ PySpark –ø–æ—Å–ª–µ `spark.read`. –í–æ—Ç —à–ø–∞—Ä–≥–∞–ª–∫–∞ —Å –≥–æ—Ç–æ–≤—ã–º–∏ –±–ª–æ–∫–∞–º–∏ –Ω–∞ —Ä–∞–∑–Ω—ã–µ —Å–ª—É—á–∞–∏.\n",
    "\n",
    "---\n",
    "\n",
    "# –ß—Ç–µ–Ω–∏–µ `Parquet` –∏–∑ `MinIO` –≤ `PySpark`\n",
    "\n",
    "### –ë–∞–∑–æ–≤—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å\n",
    "```python\n",
    "# –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç\n",
    "df = spark.read.parquet(\"s3a://my-bucket/path/to/data/\")\n",
    "\n",
    "# –° —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º —Ñ–æ—Ä–º–∞—Ç–∞\n",
    "df = spark.read.format(\"parquet\").load(\"s3a://my-bucket/path/to/data/\")\n",
    "```\n",
    "\n",
    "### 1. –ß—Ç–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏ –ø–∞–ø–æ–∫\n",
    "```python\n",
    "# –ß—Ç–µ–Ω–∏–µ –æ–¥–Ω–æ–π –ø–∞–ø–∫–∏\n",
    "df = spark.read.parquet(\"s3a://my-bucket/path/to/folder/\")\n",
    "\n",
    "# –ß—Ç–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—É—Ç–µ–π\n",
    "df = spark.read.parquet(\n",
    "    \"s3a://my-bucket/path1/\",\n",
    "    \"s3a://my-bucket/path2/\"\n",
    ")\n",
    "\n",
    "# –ß—Ç–µ–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ñ–∞–π–ª–∞\n",
    "df = spark.read.parquet(\"s3a://my-bucket/path/to/file.parquet\")\n",
    "```\n",
    "\n",
    "### 2. –ß—Ç–µ–Ω–∏–µ –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "```python\n",
    "# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä—Ç–∏—Ü–∏–π (Hive-style)\n",
    "# –°—Ç—Ä—É–∫—Ç—É—Ä–∞: s3a://.../date=2024-01-01/country=US/...\n",
    "df = spark.read.parquet(\"s3a://my-bucket/partitioned_data/\")\n",
    "\n",
    "# –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –ø–∞—Ä—Ç–∏—Ü–∏–π\n",
    "df = spark.read.parquet(\"s3a://my-bucket/partitioned_data/\") \\\n",
    "    .filter(\"date >= '2024-01-01' AND country = 'US'\")\n",
    "```\n",
    "\n",
    "### 3. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å—Ö–µ–º–æ–π –¥–∞–Ω–Ω—ã—Ö\n",
    "```python\n",
    "# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥ —Å—Ö–µ–º—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)\n",
    "df = spark.read.parquet(\"s3a://my-bucket/path/\")\n",
    "\n",
    "# –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ö–µ–º–∞ (–µ—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å —Ç–∏–ø—ã)\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "custom_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"value\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.schema(custom_schema).parquet(\"s3a://my-bucket/path/\")\n",
    "\n",
    "# –ü—Ä–æ—Å–º–æ—Ç—Ä —Å—Ö–µ–º—ã –ø–æ—Å–ª–µ —á—Ç–µ–Ω–∏—è\n",
    "df.printSchema()\n",
    "```\n",
    "\n",
    "### 4. –û–ø—Ü–∏–∏ —á—Ç–µ–Ω–∏—è –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "```python\n",
    "df = spark.read \\\n",
    "    .option(\"mergeSchema\", \"true\") \\  # –û–±—ä–µ–¥–∏–Ω—è—Ç—å —Å—Ö–µ–º—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤\n",
    "    .option(\"recursiveFileLookup\", \"true\") \\  # –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ä—Ç–∏—Ü–∏–∏ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Ñ–∞–π–ª–æ–≤\n",
    "    .parquet(\"s3a://my-bucket/path/\")\n",
    "```\n",
    "\n",
    "### 5. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ (–ø—Ä–µ–¥–∏–∫–∞—Ç–Ω—ã–π –ø—É—à–¥–∞—É–Ω)\n",
    "```python\n",
    "# –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è - –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç —á—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö\n",
    "df = spark.read.parquet(\"s3a://my-bucket/path/\") \\\n",
    "    .select(\"id\", \"name\", \"date\") \\\n",
    "    .filter(\"date >= '2024-01-01' AND status = 'active'\")\n",
    "```\n",
    "\n",
    "### 6. –ß—Ç–µ–Ω–∏–µ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∫–æ–ª–æ–Ω–æ–∫ (column pruning)\n",
    "```python\n",
    "# –ß—Ç–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ - —É—Å–∫–æ—Ä—è–µ—Ç —Ä–∞–±–æ—Ç—É\n",
    "df = spark.read.parquet(\"s3a://my-bucket/path/\") \\\n",
    "    .select(\"id\", \"name\", \"timestamp\")\n",
    "```\n",
    "\n",
    "### 7. –ü–∞–∫–µ—Ç–Ω–æ–µ —á—Ç–µ–Ω–∏–µ –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "```python\n",
    "# –ß—Ç–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã—Ö –ø–∞—Ä—Ç–∏—Ü–∏–π\n",
    "df_new = spark.read.parquet(\n",
    "    \"s3a://my-bucket/data/date=2024-01-15/\",\n",
    "    \"s3a://my-bucket/data/date=2024-01-16/\"\n",
    ")\n",
    "```\n",
    "\n",
    "### 8. –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è SQL\n",
    "```python\n",
    "# –ß—Ç–µ–Ω–∏–µ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ SQL –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è\n",
    "df = spark.read.parquet(\"s3a://my-bucket/path/\")\n",
    "df.createOrReplaceTempView(\"my_table\")\n",
    "\n",
    "# –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ SQL\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT * FROM my_table \n",
    "    WHERE date >= '2024-01-01'\n",
    "    ORDER BY timestamp DESC\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "### 9. –ß—Ç–µ–Ω–∏–µ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫\n",
    "```python\n",
    "df = spark.read \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\  # –∏–ª–∏ FAILFAST, DROPMALFORMED\n",
    "    .parquet(\"s3a://my-bucket/path/\")\n",
    "```\n",
    "\n",
    "### 10. –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è ETL –ø—Ä–æ—Ü–µ—Å—Å–∞\n",
    "```python\n",
    "def read_parquet_from_minio(path, selected_columns=None, filters=None):\n",
    "    \"\"\"–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —á—Ç–µ–Ω–∏—è Parquet\"\"\"\n",
    "    df = spark.read.parquet(path)\n",
    "    \n",
    "    if selected_columns:\n",
    "        df = df.select(*selected_columns)\n",
    "    \n",
    "    if filters:\n",
    "        df = df.filter(filters)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ\n",
    "df = read_parquet_from_minio(\n",
    "    path=\"s3a://my-bucket/data/\",\n",
    "    selected_columns=[\"id\", \"name\", \"value\"],\n",
    "    filters=\"value > 100\"\n",
    ")\n",
    "```\n",
    "\n",
    "### –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:\n",
    "\n",
    "**–î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞:**\n",
    "```python\n",
    "df = spark.read.parquet(\"s3a://my-bucket/path/\")\n",
    "df.show()\n",
    "df.printSchema()\n",
    "```\n",
    "\n",
    "**–î–ª—è ETL –ø—Ä–æ—Ü–µ—Å—Å–∞:**\n",
    "```python\n",
    "df = spark.read.parquet(\"s3a://my-bucket/source/\") \\\n",
    "    .select(\"col1\", \"col2\", \"date\") \\\n",
    "    .filter(\"date >= '2024-01-01'\") \\\n",
    "    .groupBy(\"col1\") \\\n",
    "    .count()\n",
    "```\n",
    "\n",
    "**–î–ª—è SQL –∞–Ω–∞–ª–∏–∑–∞:**\n",
    "```python\n",
    "spark.read.parquet(\"s3a://my-bucket/data/\").createOrReplaceTempView(\"source_table\")\n",
    "result = spark.sql(\"SELECT * FROM source_table WHERE value > 100\")\n",
    "```\n",
    "\n",
    "–í—Å–µ —ç—Ç–∏ –ø–æ–¥—Ö–æ–¥—ã –±—É–¥—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å –≤–∞—à–∏–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º –∫ MinIO —á–µ—Ä–µ–∑ `s3a://` –ø—É—Ç–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2d7395-84b7-4d9d-8601-d8be3e736ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a30340a-17ee-49ac-813d-b7f167ec05da",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996a1f34-0436-4c64-a2ef-b4160eff76ce",
   "metadata": {},
   "source": [
    "# Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcb18af-2665-4166-a2e8-65ec17b23caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_url = \"jdbc:postgresql://postgres-db:5432/learn_base\"\n",
    "\n",
    "postgres_con = {\n",
    "    \"user\": \"airflow\",\n",
    "    \"password\": \"airflow\", \n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "# –ü—Ä–∏–º–µ—Ä 1: –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "df = (spark.read.jdbc(url=jdbc_url, \n",
    "                      table=\"–∫–∞–∫–æ–π_–∫–∞–π—Ñ\", \n",
    "                      properties=postgres_con))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f727e8f9-6047-4f51-a0c8-e09dd69affeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db1b325-6758-4f86-95f7-0c36b5718615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7705303-1405-4915-a189-74480f195ece",
   "metadata": {},
   "source": [
    "# –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ `PostgreSQL` –≤ `Spark DataFrame`\n",
    "\n",
    "## 1. –ë–∞–∑–æ–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç\n",
    "```python\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\") \\\n",
    "    .option(\"dbtable\", \"table_name\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "## 2. –ö–æ—Ä–æ—Ç–∫–∏–π –≤–∞—Ä–∏–∞–Ω—Ç —á–µ—Ä–µ–∑ jdbc()\n",
    "```python\n",
    "df = spark.read.jdbc(\n",
    "    url=\"jdbc:postgresql://localhost:5432/mydb\",\n",
    "    table=\"table_name\",\n",
    "    properties={\n",
    "        \"user\": \"username\",\n",
    "        \"password\": \"password\", \n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "## 3. –ß—Ç–µ–Ω–∏–µ —Å SQL –∑–∞–ø—Ä–æ—Å–æ–º\n",
    "```python\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"query\", \"SELECT * FROM users WHERE age > 18 AND city = 'Moscow'\") \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "## 4. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ —Å –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º\n",
    "```python\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\") \\\n",
    "    .option(\"dbtable\", \"large_table\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .option(\"partitionColumn\", \"id\") \\     # –∫–æ–ª–æ–Ω–∫–∞ –¥–ª—è –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "    .option(\"lowerBound\", \"1\") \\           # –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "    .option(\"upperBound\", \"1000000\") \\     # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ  \n",
    "    .option(\"numPartitions\", \"10\") \\       # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä—Ç–∏—Ü–∏–π\n",
    "    .load()\n",
    "```\n",
    "\n",
    "## 5. –ß—Ç–µ–Ω–∏–µ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π —á–µ—Ä–µ–∑ predicates\n",
    "```python\n",
    "predicates = [\n",
    "    \"date >= '2024-01-01' AND date < '2024-02-01'\",  # –ø–∞—Ä—Ç–∏—Ü–∏—è 1\n",
    "    \"date >= '2024-02-01' AND date < '2024-03-01'\",  # –ø–∞—Ä—Ç–∏—Ü–∏—è 2\n",
    "    \"date >= '2024-03-01' AND date < '2024-04-01'\"   # –ø–∞—Ä—Ç–∏—Ü–∏—è 3\n",
    "]\n",
    "\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\") \\\n",
    "    .option(\"dbtable\", \"sales\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .option(\"predicates\", predicates) \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "## 6. –ß—Ç–µ–Ω–∏–µ —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏\n",
    "```python\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\") \\\n",
    "    .option(\"dbtable\", \"table_name\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"fetchsize\", \"10000\") \\          # —Ä–∞–∑–º–µ—Ä fetch –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    .option(\"sessionInitStatement\", \"SET TIME ZONE 'UTC'\") \\  # –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏\n",
    "    .option(\"customSchema\", \"id DECIMAL(38,0), name STRING\") \\  # –∫–∞—Å—Ç–æ–º–Ω—ã–µ —Ç–∏–ø—ã\n",
    "    .load()\n",
    "```\n",
    "\n",
    "## 7. –ß—Ç–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
    "```python\n",
    "# –ß–µ—Ä–µ–∑ SQL –∑–∞–ø—Ä–æ—Å\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .option(\"query\", \"SELECT id, name, email FROM users WHERE active = true\") \\\n",
    "    .load()\n",
    "\n",
    "# –ß–µ—Ä–µ–∑ dbtable —Å –ø–æ–¥–∑–∞–ø—Ä–æ—Å–æ–º\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .option(\"dbtable\", \"(SELECT id, name FROM users) AS users_subset\") \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "## 8. –ü–æ–ª–Ω—ã–π –ø—Ä–∏–º–µ—Ä —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫\n",
    "```python\n",
    "def read_from_postgres(table_name_or_query, **options):\n",
    "    try:\n",
    "        base_options = {\n",
    "            \"url\": \"jdbc:postgresql://localhost:5432/mydb\",\n",
    "            \"user\": \"username\",\n",
    "            \"password\": \"password\",\n",
    "            \"driver\": \"org.postgresql.Driver\"\n",
    "        }\n",
    "        base_options.update(options)\n",
    "        \n",
    "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —ç—Ç–æ —Ç–∞–±–ª–∏—Ü–∞ –∏–ª–∏ –∑–∞–ø—Ä–æ—Å\n",
    "        if \"SELECT\" in table_name_or_query.upper():\n",
    "            base_options[\"query\"] = table_name_or_query\n",
    "        else:\n",
    "            base_options[\"dbtable\"] = table_name_or_query\n",
    "            \n",
    "        df = spark.read.format(\"jdbc\").options(**base_options).load()\n",
    "        \n",
    "        print(f\"‚úÖ –î–∞–Ω–Ω—ã–µ –ø—Ä–æ—á–∏—Ç–∞–Ω—ã: {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫, {df.count()} —Å—Ç—Ä–æ–∫\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ\n",
    "df1 = read_from_postgres(\"users\")\n",
    "df2 = read_from_postgres(\"SELECT * FROM orders WHERE status = 'completed'\")\n",
    "df3 = read_from_postgres(\"large_table\", numPartitions=10, partitionColumn=\"id\")\n",
    "```\n",
    "\n",
    "## 9. –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —á—Ç–µ–Ω–∏—è\n",
    "```python\n",
    "def postgres_to_spark(source, **options):\n",
    "    \"\"\"\n",
    "    source: –∏–º—è —Ç–∞–±–ª–∏—Ü—ã –∏–ª–∏ SQL –∑–∞–ø—Ä–æ—Å\n",
    "    \"\"\"\n",
    "    base_config = {\n",
    "        \"url\": \"jdbc:postgresql://localhost:5432/mydb\",\n",
    "        \"user\": \"username\",\n",
    "        \"password\": \"password\",\n",
    "        \"driver\": \"org.postgresql.Driver\",\n",
    "        \"fetchsize\": 10000\n",
    "    }\n",
    "    base_config.update(options)\n",
    "    \n",
    "    reader = spark.read.format(\"jdbc\")\n",
    "    \n",
    "    if \"SELECT\" in source.upper():\n",
    "        reader.option(\"query\", source)\n",
    "    else:\n",
    "        reader.option(\"dbtable\", source)\n",
    "    \n",
    "    return reader.options(**base_config).load()\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "df_table = postgres_to_spark(\"products\")\n",
    "df_query = postgres_to_spark(\"SELECT * FROM sales WHERE amount > 1000\")\n",
    "df_partitioned = postgres_to_spark(\n",
    "    \"large_table\",\n",
    "    partitionColumn=\"id\",\n",
    "    lowerBound=1,\n",
    "    upperBound=1000000,\n",
    "    numPartitions=8\n",
    ")\n",
    "```\n",
    "\n",
    "–í—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –≥–æ—Ç–æ–≤—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
