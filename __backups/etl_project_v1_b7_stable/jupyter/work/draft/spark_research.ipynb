{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4ceb200-2974-4592-9abb-fd9337ce05d4",
   "metadata": {},
   "source": [
    "# План выполнения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "964d74d0-a380-4b51-a4c1-147dccd3b683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/29 18:14:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Read Session\")\n",
    "         .master(\"spark://spark-master:7077\") \n",
    "         .config(\"spark.jars\", \n",
    "                 \"/home/jovyan/work/spark-jars/hadoop-aws-3.3.4.jar,\"\n",
    "                 \"/home/jovyan/work/spark-jars/aws-java-sdk-bundle-1.12.262.jar,\"\n",
    "                 \"/home/jovyan/work/spark-jars/wildfly-openssl-1.0.7.Final.jar\")\n",
    "         .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904311a6-6540-408f-b1bf-8c5894ee1e4c",
   "metadata": {},
   "source": [
    "## Показать план выполнения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3edf9f8-1d90-417e-8721-5e640fe78e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Scan csv  (1)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [11]: [Model#56, Year#57, Region#58, Color#59, Fuel_Type#60, Transmission#61, Engine_Size_L#62, Mileage_KM#63, Price_USD#64, Sales_Volume#65, Sales_Classification#66]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/shared_data/bmw.csv]\n",
      "ReadSchema: struct<Model:string,Year:int,Region:string,Color:string,Fuel_Type:string,Transmission:string,Engine_Size_L:double,Mileage_KM:int,Price_USD:int,Sales_Volume:int,Sales_Classification:string>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/shared_data/bmw.csv\", header=True, inferSchema=True)\n",
    "df.explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f29548-9e79-4c22-9347-8dac9ae51bfb",
   "metadata": {},
   "source": [
    "## Все варианты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4716d88c-9df7-40b0-8fed-9a20a8af8a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "FileScan csv [Model#17,Year#18,Region#19,Color#20,Fuel_Type#21,Transmission#22,Engine_Size_L#23,Mileage_KM#24,Price_USD#25,Sales_Volume#26,Sales_Classification#27] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/shared_data/bmw.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Model:string,Year:int,Region:string,Color:string,Fuel_Type:string,Transmission:string,Engi...\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "FileScan csv [Model#17,Year#18,Region#19,Color#20,Fuel_Type#21,Transmission#22,Engine_Size_L#23,Mileage_KM#24,Price_USD#25,Sales_Volume#26,Sales_Classification#27] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/shared_data/bmw.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Model:string,Year:int,Region:string,Color:string,Fuel_Type:string,Transmission:string,Engi...\n",
      "\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "Relation [Model#17,Year#18,Region#19,Color#20,Fuel_Type#21,Transmission#22,Engine_Size_L#23,Mileage_KM#24,Price_USD#25,Sales_Volume#26,Sales_Classification#27] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Model: string, Year: int, Region: string, Color: string, Fuel_Type: string, Transmission: string, Engine_Size_L: double, Mileage_KM: int, Price_USD: int, Sales_Volume: int, Sales_Classification: string\n",
      "Relation [Model#17,Year#18,Region#19,Color#20,Fuel_Type#21,Transmission#22,Engine_Size_L#23,Mileage_KM#24,Price_USD#25,Sales_Volume#26,Sales_Classification#27] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Relation [Model#17,Year#18,Region#19,Color#20,Fuel_Type#21,Transmission#22,Engine_Size_L#23,Mileage_KM#24,Price_USD#25,Sales_Volume#26,Sales_Classification#27] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "FileScan csv [Model#17,Year#18,Region#19,Color#20,Fuel_Type#21,Transmission#22,Engine_Size_L#23,Mileage_KM#24,Price_USD#25,Sales_Volume#26,Sales_Classification#27] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/shared_data/bmw.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Model:string,Year:int,Region:string,Color:string,Fuel_Type:string,Transmission:string,Engi...\n",
      "\n",
      "Found 0 WholeStageCodegen subtrees.\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Relation [Model#17,Year#18,Region#19,Color#20,Fuel_Type#21,Transmission#22,Engine_Size_L#23,Mileage_KM#24,Price_USD#25,Sales_Volume#26,Sales_Classification#27] csv, Statistics(sizeInBytes=3.2 MiB)\n",
      "\n",
      "== Physical Plan ==\n",
      "FileScan csv [Model#17,Year#18,Region#19,Color#20,Fuel_Type#21,Transmission#22,Engine_Size_L#23,Mileage_KM#24,Price_USD#25,Sales_Volume#26,Sales_Classification#27] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/shared_data/bmw.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Model:string,Year:int,Region:string,Color:string,Fuel_Type:string,Transmission:string,Engi...\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "Scan csv  (1)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [11]: [Model#17, Year#18, Region#19, Color#20, Fuel_Type#21, Transmission#22, Engine_Size_L#23, Mileage_KM#24, Price_USD#25, Sales_Volume#26, Sales_Classification#27]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/shared_data/bmw.csv]\n",
      "ReadSchema: struct<Model:string,Year:int,Region:string,Color:string,Fuel_Type:string,Transmission:string,Engine_Size_L:double,Mileage_KM:int,Price_USD:int,Sales_Volume:int,Sales_Classification:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Или с указанием режима\n",
    "df.explain()           # по умолчанию - простой режим\n",
    "df.explain(\"simple\")   # только физический план\n",
    "df.explain(\"extended\") # логический + физический план\n",
    "df.explain(\"codegen\")  # сгенерированный код\n",
    "df.explain(\"cost\")     # с стоимостным анализом (если доступно)\n",
    "df.explain(\"formatted\") # форматированный вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f2d9d9-cf22-41bf-9a28-f701c0295ba8",
   "metadata": {},
   "source": [
    "# Описание"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e300c-8e72-4cd4-a732-5432e5562038",
   "metadata": {},
   "source": [
    "В плане выполнения обычно смотрят **критические точки производительности** и **потенциальные проблемы**. Вот основные аспекты:\n",
    "\n",
    "## 1. **Операции Shuffle (Exchange)**\n",
    "\n",
    "```python\n",
    "df.groupBy(\"region\").agg(F.avg(\"price\").alias(\"avg_price\"))\n",
    "```\n",
    "В плане ищите:\n",
    "```\n",
    "*(2) HashAggregate(keys=[region#25], functions=[avg(price#27)])\n",
    "+- Exchange hashpartitioning(region#25, 200)\n",
    "   +- *(1) HashAggregate(keys=[region#25], functions=[partial_avg(price#27)])\n",
    "```\n",
    "\n",
    "**Что смотреть:** `Exchange` означает перераспределение данных между узлами - это **дорогая операция**!\n",
    "\n",
    "## 2. **Pushed Filters vs Post-Read Filters**\n",
    "\n",
    "```python\n",
    "df.filter(F.col(\"year\") > 2020).filter(F.col(\"price\") > 50000)\n",
    "```\n",
    "\n",
    "В плане:\n",
    "```\n",
    "PushedFilters: [GreaterThan(year,2020), GreaterThan(price,50000)], \n",
    "```\n",
    "**Что смотреть:** \n",
    "- `PushedFilters` - фильтры применены при чтении (хорошо ✅)\n",
    "- Если фильтры в `Filter()` после `FileScan` - данные фильтруются после чтения (плохо ❌)\n",
    "\n",
    "## 3. **Типы Join**\n",
    "\n",
    "```python\n",
    "df1.join(df2, \"id\", \"inner\")\n",
    "```\n",
    "\n",
    "В плане ищите:\n",
    "```\n",
    "*(5) SortMergeJoin [id#10], [id#20], Inner\n",
    "```\n",
    "**Что смотреть:**\n",
    "- `BroadcastHashJoin` - оптимально для маленьких таблиц ✅\n",
    "- `SortMergeJoin` - для больших таблиц\n",
    "- `ShuffledHashJoin` - может быть дорогим\n",
    "\n",
    "## 4. **Data Skew (Перекос данных)**\n",
    "\n",
    "```python\n",
    "df.groupBy(\"category\").count()\n",
    "```\n",
    "В плане смотрите размеры партиций:\n",
    "```\n",
    "+- Exchange hashpartitioning(category#15, 200)\n",
    "```\n",
    "**Что смотреть:** если одна партиция обрабатывается значительно дольше - есть перекос данных\n",
    "\n",
    "## 5. **Predicate Pushdown**\n",
    "\n",
    "```python\n",
    "df.filter(F.col(\"year\") > 2020).select(\"model\", \"price\")\n",
    "```\n",
    "\n",
    "В плане:\n",
    "```\n",
    "FileScan csv [model#17,price#25] \n",
    "PushedFilters: [GreaterThan(year,2020)]\n",
    "```\n",
    "**Что смотреть:** фильтры и проекции применены на уровне чтения ✅\n",
    "\n",
    "## 6. **Количество партиций**\n",
    "\n",
    "```python\n",
    "df.rdd.getNumPartitions()  # можно проверить отдельно\n",
    "```\n",
    "\n",
    "В плане смотрите:\n",
    "```\n",
    "Exchange hashpartitioning(region#19, 200)  # 200 партиций\n",
    "```\n",
    "\n",
    "## 7. **Полное сканирование vs Прунинг партиций**\n",
    "\n",
    "Для партиционированных данных:\n",
    "```\n",
    "PartitionFilters: [isnotnull(region#19), (region#19 = Europe)]\n",
    "PushedFilters: [GreaterThan(price,50000)]\n",
    "```\n",
    "**Что смотреть:** \n",
    "- `PartitionFilters` - читаются только нужные партиции ✅\n",
    "- Если `PartitionFilters: []` - читаются все партиции ❌\n",
    "\n",
    "## 8. **Вложенные операции vs Стадиинг**\n",
    "\n",
    "```python\n",
    "# Плохо - два отдельных shuffle\n",
    "df1.groupBy(\"a\").agg(F.sum(\"b\"))\n",
    "df2.groupBy(\"a\").agg(F.avg(\"c\"))\n",
    "result = df1.join(df2, \"a\")\n",
    "\n",
    "# Лучше - один shuffle\n",
    "df1_with_agg = df1.groupBy(\"a\").agg(F.sum(\"b\").alias(\"sum_b\"))\n",
    "df2_with_agg = df2.groupBy(\"a\").agg(F.avg(\"c\").alias(\"avg_c\"))\n",
    "result = df1_with_agg.join(df2_with_agg, \"a\")\n",
    "```\n",
    "\n",
    "## Практический пример анализа:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PlanAnalysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Читаем данные\n",
    "df = spark.read.csv(\"/shared_data/bmw.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Сложный запрос для анализа\n",
    "complex_query = df \\\n",
    "    .filter(F.col(\"Year\") > 2018) \\\n",
    "    .filter(F.col(\"Price_USD\") > 30000) \\\n",
    "    .groupBy(\"Region\", \"Fuel_Type\") \\\n",
    "    .agg(\n",
    "        F.avg(\"Price_USD\").alias(\"AvgPrice\"),\n",
    "        F.count(\"*\").alias(\"Count\"),\n",
    "        F.max(\"Mileage_KM\").alias(\"MaxMileage\")\n",
    "    ) \\\n",
    "    .join(\n",
    "        df.groupBy(\"Region\").agg(F.avg(\"Price_USD\").alias(\"RegionAvgPrice\")),\n",
    "        \"Region\"\n",
    "    ) \\\n",
    "    .filter(F.col(\"AvgPrice\") > F.col(\"RegionAvgPrice\")) \\\n",
    "    .orderBy(\"AvgPrice\", ascending=False)\n",
    "\n",
    "print(\"=== ANALYZING CRITICAL POINTS ===\")\n",
    "complex_query.explain(\"formatted\")\n",
    "\n",
    "# Анализируем ключевые аспекты:\n",
    "print(\"\\n=== KEY ANALYSIS POINTS ===\")\n",
    "print(\"1. Check for Exchange operations (shuffle)\")\n",
    "print(\"2. Check PushedFilters vs runtime Filters\") \n",
    "print(\"3. Check Join type\")\n",
    "print(\"4. Check if aggregations are combined\")\n",
    "print(\"5. Check partition pruning\")\n",
    "\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "## Чеклист для анализа плана:\n",
    "\n",
    "✅ **Хорошие признаки:**\n",
    "- `PushedFilters` содержат ваши условия WHERE\n",
    "- `BroadcastJoin` для маленьких таблиц\n",
    "- Минимальное количество `Exchange` операций\n",
    "- `PartitionFilters` используются для партиционированных данных\n",
    "- Агрегации объединены где возможно\n",
    "\n",
    "❌ **Проблемные признаки:**\n",
    "- Много операций `Exchange` (shuffle)\n",
    "- `Filter` операции после `FileScan` (не pushed)\n",
    "- `CartesianProduct` для больших таблиц\n",
    "- Отсутствие `PartitionFilters` для партиционированных данных\n",
    "- Очень большое или маленькое количество партиций\n",
    "\n",
    "## Как использовать эту информацию:\n",
    "\n",
    "1. **Увидели много shuffle** → попробуйте переписать запрос\n",
    "2. **Фильтры не pushed** → проверьте условия, измените порядок операций  \n",
    "3. **Неоптимальный join** → используйте broadcast для маленьких таблиц\n",
    "4. **Перекос данных** → добавьте salt или измените ключ группировки\n",
    "\n",
    "Анализ плана выполнения - это ключ к оптимизации Spark приложений!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c4b97a-41be-46eb-9f1b-559773e4f6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ff9a62-3bfc-4383-8c43-d14e3998f35c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
