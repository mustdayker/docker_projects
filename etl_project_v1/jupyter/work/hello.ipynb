{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0f13c21-87fe-4112-bf57-8c7bf047ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0021b7b1-31ae-46dc-8102-7383a38d7e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a436ff02-c4ef-40aa-85c5-15fedd4565b5",
   "metadata": {},
   "source": [
    "### Spark —Å–µ—Å—Å–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f8ada13-0939-4069-899c-608687a95b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/04 15:42:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "drivers = [\n",
    "    \"/home/jovyan/work/spark-jars/hadoop-aws-3.3.4.jar\",             # S3\n",
    "    \"/home/jovyan/work/spark-jars/aws-java-sdk-bundle-1.12.262.jar\", # S3\n",
    "    \"/home/jovyan/work/spark-jars/wildfly-openssl-1.0.7.Final.jar\",  # S3\n",
    "    \"/home/jovyan/work/spark-jars/postgresql-42.6.0.jar\",            # PostgreSQL\n",
    "]\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"mustdayker-Spark2\")\n",
    "         .master(\"spark://spark-master:7077\") \n",
    "         .config(\"spark.jars\", \",\".join(drivers))\n",
    "         .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4170a035-4b55-4574-9929-18de614ba2f4",
   "metadata": {},
   "source": [
    "### –ß–∏—Ç–∞–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ac204c9-a6ca-4ff3-8dbd-1ca2c7fb17ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/04 15:42:42 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_2023_01 = spark.read.parquet(\"s3a://silver/nyc-taxi-data/yellow_tripdata_2023-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8768d1-8ded-4b3d-893e-51c26b410166",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2024_01 = spark.read.parquet(\"s3a://bronze/nyc-taxi-data/yellow_tripdata_2024-01.parquet\")\n",
    "df_2025_01 = spark.read.parquet(\"s3a://bronze/nyc-taxi-data/yellow_tripdata_2025-01.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36653660-b6e1-438e-8c4b-5203f601609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = \"silver\"\n",
    "\n",
    "df = (spark.read\n",
    "      .format(\"parquet\")\n",
    "      .option(\"mergeSchema\", \"true\")\n",
    "      .load([\n",
    "          f\"s3a://{layer}/nyc-taxi-data/yellow_tripdata_2023-*\",\n",
    "          f\"s3a://{layer}/nyc-taxi-data/yellow_tripdata_2024-*\",\n",
    "          f\"s3a://{layer}/nyc-taxi-data/yellow_tripdata_2025-*\",\n",
    "      ])\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8762ecf-0e8a-4c46-a43c-75d0e1a0dec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:==================================>                       (6 + 4) / 10]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ö–æ–ª–∏—á–µ—Ç—Å–≤–æ —Å—Ç—Ä–æ–∫: 3066766\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(f\"–ö–æ–ª–∏—á–µ—Ç—Å–≤–æ —Å—Ç—Ä–æ–∫: {df_2023_01.count()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621f0e8c-1cfd-48d4-9d19-c4f820dbf350",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf76c46-f813-4267-8615-40bd52a117ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"–ö–æ–ª–∏—á–µ—Ç—Å–≤–æ —Å—Ç—Ä–æ–∫: {df.count()}\\n\")\n",
    "\n",
    "(df.withColumn(\"year\",  F.year(\"tpep_pickup_datetime\"))\n",
    "   .withColumn(\"month\", F.month(\"tpep_pickup_datetime\"))\n",
    "   .select(\"year\", \n",
    "           \"month\", \n",
    "           \"VendorID\",\n",
    "           \"tpep_pickup_datetime\", \n",
    "           \"passenger_count\", \n",
    "           \"trip_distance\",\n",
    "           \"airport_fee\",\n",
    "          )\n",
    "   .groupBy(\"year\", \"month\")\n",
    "   .agg(\n",
    "       F.count(\"*\").alias(\"total_records\"),\n",
    "       F.sum(\"airport_fee\").alias(\"–û–±–¥–∏—Ä–∞–ª–æ–≤–∫–∞\"),\n",
    "       )\n",
    "   .orderBy(\"year\", \"month\")\n",
    "   .show(50)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3de6e-633c-40e1-b1f9-5084a8f6c509",
   "metadata": {},
   "source": [
    "### –ò—Å—Å–ª–µ–¥—É–µ–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf043c8a-8a10-407b-bcaf-d275ec43f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "def standardize_nyc_taxi_data(input_path, output_path):\n",
    "    \"\"\"\n",
    "    –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ NYC Taxi:\n",
    "    - –ü—Ä–∏–≤–æ–¥–∏—Ç –≤—Å–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "    - –ü—Ä–∏–≤–æ–¥–∏—Ç —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –∫ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–º\n",
    "    - –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø—É—Ç—å\n",
    "    \"\"\"\n",
    "\n",
    "    output_path = output_path.replace('.parquet', '')\n",
    "    \n",
    "    # –ß–∏—Ç–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "    df = spark.read.parquet(input_path)\n",
    "    \n",
    "    # 1. –ü—Ä–∏–≤–æ–¥–∏–º –≤—Å–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "    for col_name in df.columns:\n",
    "        df = df.withColumnRenamed(col_name, col_name.lower())\n",
    "    \n",
    "    # 2. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥ —Ç–∏–ø–æ–≤ –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "    type_mapping = {\n",
    "        # –ß–∏—Å–ª–æ–≤—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã - –≤ Integer\n",
    "        \"vendorid\": IntegerType(),\n",
    "        \"pulocationid\": IntegerType(), \n",
    "        \"dolocationid\": IntegerType(),\n",
    "        \"payment_type\": IntegerType(),\n",
    "        \"ratecodeid\": IntegerType(),\n",
    "        \n",
    "        # –ü–∞—Å—Å–∞–∂–∏—Ä—ã - –≤ Integer (–±–æ–ª–µ–µ –ª–æ–≥–∏—á–Ω–æ)\n",
    "        \"passenger_count\": IntegerType(),\n",
    "        \n",
    "        # –î–µ–Ω–µ–∂–Ω—ã–µ —Å—É–º–º—ã - –≤ Double (–¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏)\n",
    "        \"fare_amount\": DoubleType(),\n",
    "        \"extra\": DoubleType(),\n",
    "        \"mta_tax\": DoubleType(),\n",
    "        \"tip_amount\": DoubleType(),\n",
    "        \"tolls_amount\": DoubleType(),\n",
    "        \"improvement_surcharge\": DoubleType(),\n",
    "        \"total_amount\": DoubleType(),\n",
    "        \"congestion_surcharge\": DoubleType(),\n",
    "        \"airport_fee\": DoubleType(),\n",
    "        \"cbd_congestion_fee\": DoubleType(),\n",
    "        \n",
    "        # –î–∏—Å—Ç–∞–Ω—Ü–∏—è - –≤ Double\n",
    "        \"trip_distance\": DoubleType()\n",
    "    }\n",
    "    \n",
    "    # 3. –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫\n",
    "    for col_name, target_type in type_mapping.items():\n",
    "        if col_name in df.columns:\n",
    "            df = df.withColumn(\n",
    "                col_name, \n",
    "                F.coalesce(\n",
    "                    F.col(col_name).cast(target_type), \n",
    "                    F.lit(0 if target_type == IntegerType() else 0.0)\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    # 4. –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –ø–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è consistency\n",
    "    expected_columns = [\n",
    "        \"vendorid\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\",\n",
    "        \"passenger_count\", \"trip_distance\", \"ratecodeid\", \"store_and_fwd_flag\",\n",
    "        \"pulocationid\", \"dolocationid\", \"payment_type\", \"fare_amount\", \"extra\",\n",
    "        \"mta_tax\", \"tip_amount\", \"tolls_amount\", \"improvement_surcharge\",\n",
    "        \"total_amount\", \"congestion_surcharge\", \"airport_fee\", \"cbd_congestion_fee\"\n",
    "    ]\n",
    "    \n",
    "    # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ\n",
    "    final_columns = [col for col in expected_columns if col in df.columns]\n",
    "    df_standardized = df.select(final_columns)\n",
    "    \n",
    "    # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏\n",
    "    (df_standardized\n",
    "     .coalesce(1)\n",
    "     .write\n",
    "     .mode(\"overwrite\")\n",
    "     .option(\"compression\", \"snappy\")  # —Ö–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç—å/—Å–∂–∞—Ç–∏–µ\n",
    "     .parquet(output_path)\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {input_path} -> {output_path}\")\n",
    "    # print(f\"üìä –°—Ö–µ–º–∞ –ø–æ—Å–ª–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏:\")\n",
    "    # df_standardized.printSchema()\n",
    "    \n",
    "    return df_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd36c7e4-33e0-42e4-8bb3-0c8b620e18c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞\n",
    "input_path = \"s3a://bronze/nyc-taxi-data/yellow_tripdata_2023-01.parquet\"\n",
    "output_path = \"s3a://silver/nyc-taxi-data/yellow_tripdata_2023-01.parquet\"\n",
    "\n",
    "standardized_df = standardize_nyc_taxi_data(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c1f8ed-0c6d-4ff8-a73c-66884d269d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_nyc_taxi_files():\n",
    "    \"\"\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã NYC Taxi –∏–∑ Bronze –≤ Silver\"\"\"\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö parquet —Ñ–∞–π–ª–æ–≤ –≤ bronze\n",
    "    bronze_files = spark.sql(f\"\"\"\n",
    "        SELECT path \n",
    "        FROM (\n",
    "            SELECT input_file_name() as path \n",
    "            FROM parquet.`s3a://bronze/nyc-taxi-data/`\n",
    "        ) \n",
    "        GROUP BY path\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    print(f\"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(bronze_files)}\")\n",
    "    \n",
    "    for i, row in enumerate(bronze_files, 1):\n",
    "        input_path = row['path']\n",
    "        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ –ø—É—Ç–∏\n",
    "        file_name = input_path.split('/')[-1]\n",
    "        output_path = f\"s3a://silver/nyc-taxi-data/{file_name}\"\n",
    "        \n",
    "        print(f\"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é ({i}/{len(bronze_files)}): {file_name}\")\n",
    "        \n",
    "        try:\n",
    "            standardize_nyc_taxi_data(input_path, output_path)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_name}: {e}\")\n",
    "    \n",
    "    print(\"üéâ –í—Å–µ —Ñ–∞–π–ª—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13223e7-6875-46fd-89aa-163df47d1617",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤\n",
    "process_all_nyc_taxi_files()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {execution_time:.4f} —Å–µ–∫—É–Ω–¥\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
