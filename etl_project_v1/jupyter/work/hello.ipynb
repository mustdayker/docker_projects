{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0f13c21-87fe-4112-bf57-8c7bf047ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0021b7b1-31ae-46dc-8102-7383a38d7e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a436ff02-c4ef-40aa-85c5-15fedd4565b5",
   "metadata": {},
   "source": [
    "### Spark —Å–µ—Å—Å–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f8ada13-0939-4069-899c-608687a95b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/04 15:42:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "drivers = [\n",
    "    \"/home/jovyan/work/spark-jars/hadoop-aws-3.3.4.jar\",             # S3\n",
    "    \"/home/jovyan/work/spark-jars/aws-java-sdk-bundle-1.12.262.jar\", # S3\n",
    "    \"/home/jovyan/work/spark-jars/wildfly-openssl-1.0.7.Final.jar\",  # S3\n",
    "    \"/home/jovyan/work/spark-jars/postgresql-42.6.0.jar\",            # PostgreSQL\n",
    "]\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"mustdayker-Spark2\")\n",
    "         .master(\"spark://spark-master:7077\") \n",
    "         .config(\"spark.jars\", \",\".join(drivers))\n",
    "         .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4170a035-4b55-4574-9929-18de614ba2f4",
   "metadata": {},
   "source": [
    "### –ß–∏—Ç–∞–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ac204c9-a6ca-4ff3-8dbd-1ca2c7fb17ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/04 15:42:42 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_2023_01 = spark.read.parquet(\"s3a://silver/nyc-taxi-data/yellow_tripdata_2023-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8768d1-8ded-4b3d-893e-51c26b410166",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2024_01 = spark.read.parquet(\"s3a://bronze/nyc-taxi-data/yellow_tripdata_2024-01.parquet\")\n",
    "df_2025_01 = spark.read.parquet(\"s3a://bronze/nyc-taxi-data/yellow_tripdata_2025-01.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36653660-b6e1-438e-8c4b-5203f601609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = \"silver\"\n",
    "\n",
    "df = (spark.read\n",
    "      .format(\"parquet\")\n",
    "      .option(\"mergeSchema\", \"true\")\n",
    "      .load([\n",
    "          f\"s3a://{layer}/nyc-taxi-data/yellow_tripdata_2023-*\",\n",
    "          f\"s3a://{layer}/nyc-taxi-data/yellow_tripdata_2024-*\",\n",
    "          f\"s3a://{layer}/nyc-taxi-data/yellow_tripdata_2025-*\",\n",
    "      ])\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8762ecf-0e8a-4c46-a43c-75d0e1a0dec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:==================================>                       (6 + 4) / 10]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ö–æ–ª–∏—á–µ—Ç—Å–≤–æ —Å—Ç—Ä–æ–∫: 3066766\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(f\"–ö–æ–ª–∏—á–µ—Ç—Å–≤–æ —Å—Ç—Ä–æ–∫: {df_2023_01.count()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621f0e8c-1cfd-48d4-9d19-c4f820dbf350",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf76c46-f813-4267-8615-40bd52a117ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"–ö–æ–ª–∏—á–µ—Ç—Å–≤–æ —Å—Ç—Ä–æ–∫: {df.count()}\\n\")\n",
    "\n",
    "(df.withColumn(\"year\",  F.year(\"tpep_pickup_datetime\"))\n",
    "   .withColumn(\"month\", F.month(\"tpep_pickup_datetime\"))\n",
    "   .select(\"year\", \n",
    "           \"month\", \n",
    "           \"VendorID\",\n",
    "           \"tpep_pickup_datetime\", \n",
    "           \"passenger_count\", \n",
    "           \"trip_distance\",\n",
    "           \"airport_fee\",\n",
    "          )\n",
    "   .groupBy(\"year\", \"month\")\n",
    "   .agg(\n",
    "       F.count(\"*\").alias(\"total_records\"),\n",
    "       F.sum(\"airport_fee\").alias(\"–û–±–¥–∏—Ä–∞–ª–æ–≤–∫–∞\"),\n",
    "       )\n",
    "   .orderBy(\"year\", \"month\")\n",
    "   .show(50)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3de6e-633c-40e1-b1f9-5084a8f6c509",
   "metadata": {},
   "source": [
    "### –ò—Å—Å–ª–µ–¥—É–µ–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf043c8a-8a10-407b-bcaf-d275ec43f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "def standardize_nyc_taxi_data(input_path, output_path):\n",
    "    \"\"\"\n",
    "    –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ NYC Taxi:\n",
    "    - –ü—Ä–∏–≤–æ–¥–∏—Ç –≤—Å–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "    - –ü—Ä–∏–≤–æ–¥–∏—Ç —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –∫ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–º\n",
    "    - –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø—É—Ç—å\n",
    "    \"\"\"\n",
    "\n",
    "    output_path = output_path.replace('.parquet', '')\n",
    "    \n",
    "    # –ß–∏—Ç–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "    df = spark.read.parquet(input_path)\n",
    "    \n",
    "    # 1. –ü—Ä–∏–≤–æ–¥–∏–º –≤—Å–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "    for col_name in df.columns:\n",
    "        df = df.withColumnRenamed(col_name, col_name.lower())\n",
    "    \n",
    "    # 2. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥ —Ç–∏–ø–æ–≤ –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "    type_mapping = {\n",
    "        # –ß–∏—Å–ª–æ–≤—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã - –≤ Integer\n",
    "        \"vendorid\": IntegerType(),\n",
    "        \"pulocationid\": IntegerType(), \n",
    "        \"dolocationid\": IntegerType(),\n",
    "        \"payment_type\": IntegerType(),\n",
    "        \"ratecodeid\": IntegerType(),\n",
    "        \n",
    "        # –ü–∞—Å—Å–∞–∂–∏—Ä—ã - –≤ Integer (–±–æ–ª–µ–µ –ª–æ–≥–∏—á–Ω–æ)\n",
    "        \"passenger_count\": IntegerType(),\n",
    "        \n",
    "        # –î–µ–Ω–µ–∂–Ω—ã–µ —Å—É–º–º—ã - –≤ Double (–¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏)\n",
    "        \"fare_amount\": DoubleType(),\n",
    "        \"extra\": DoubleType(),\n",
    "        \"mta_tax\": DoubleType(),\n",
    "        \"tip_amount\": DoubleType(),\n",
    "        \"tolls_amount\": DoubleType(),\n",
    "        \"improvement_surcharge\": DoubleType(),\n",
    "        \"total_amount\": DoubleType(),\n",
    "        \"congestion_surcharge\": DoubleType(),\n",
    "        \"airport_fee\": DoubleType(),\n",
    "        \"cbd_congestion_fee\": DoubleType(),\n",
    "        \n",
    "        # –î–∏—Å—Ç–∞–Ω—Ü–∏—è - –≤ Double\n",
    "        \"trip_distance\": DoubleType()\n",
    "    }\n",
    "    \n",
    "    # 3. –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫\n",
    "    for col_name, target_type in type_mapping.items():\n",
    "        if col_name in df.columns:\n",
    "            df = df.withColumn(\n",
    "                col_name, \n",
    "                F.coalesce(\n",
    "                    F.col(col_name).cast(target_type), \n",
    "                    F.lit(0 if target_type == IntegerType() else 0.0)\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    # 4. –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –ø–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è consistency\n",
    "    expected_columns = [\n",
    "        \"vendorid\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\",\n",
    "        \"passenger_count\", \"trip_distance\", \"ratecodeid\", \"store_and_fwd_flag\",\n",
    "        \"pulocationid\", \"dolocationid\", \"payment_type\", \"fare_amount\", \"extra\",\n",
    "        \"mta_tax\", \"tip_amount\", \"tolls_amount\", \"improvement_surcharge\",\n",
    "        \"total_amount\", \"congestion_surcharge\", \"airport_fee\", \"cbd_congestion_fee\"\n",
    "    ]\n",
    "    \n",
    "    # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ\n",
    "    final_columns = [col for col in expected_columns if col in df.columns]\n",
    "    df_standardized = df.select(final_columns)\n",
    "    \n",
    "    # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏\n",
    "    (df_standardized\n",
    "     .coalesce(1)\n",
    "     .write\n",
    "     .mode(\"overwrite\")\n",
    "     .option(\"compression\", \"snappy\")  # —Ö–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç—å/—Å–∂–∞—Ç–∏–µ\n",
    "     .parquet(output_path)\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {input_path} -> {output_path}\")\n",
    "    # print(f\"üìä –°—Ö–µ–º–∞ –ø–æ—Å–ª–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏:\")\n",
    "    # df_standardized.printSchema()\n",
    "    \n",
    "    return df_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd36c7e4-33e0-42e4-8bb3-0c8b620e18c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞\n",
    "input_path = \"s3a://bronze/nyc-taxi-data/yellow_tripdata_2023-01.parquet\"\n",
    "output_path = \"s3a://silver/nyc-taxi-data/yellow_tripdata_2023-01.parquet\"\n",
    "\n",
    "standardized_df = standardize_nyc_taxi_data(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c1f8ed-0c6d-4ff8-a73c-66884d269d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_nyc_taxi_files():\n",
    "    \"\"\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã NYC Taxi –∏–∑ Bronze –≤ Silver\"\"\"\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö parquet —Ñ–∞–π–ª–æ–≤ –≤ bronze\n",
    "    bronze_files = spark.sql(f\"\"\"\n",
    "        SELECT path \n",
    "        FROM (\n",
    "            SELECT input_file_name() as path \n",
    "            FROM parquet.`s3a://bronze/nyc-taxi-data/`\n",
    "        ) \n",
    "        GROUP BY path\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    print(f\"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(bronze_files)}\")\n",
    "    \n",
    "    for i, row in enumerate(bronze_files, 1):\n",
    "        input_path = row['path']\n",
    "        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ –ø—É—Ç–∏\n",
    "        file_name = input_path.split('/')[-1]\n",
    "        output_path = f\"s3a://silver/nyc-taxi-data/{file_name}\"\n",
    "        \n",
    "        print(f\"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é ({i}/{len(bronze_files)}): {file_name}\")\n",
    "        \n",
    "        try:\n",
    "            standardize_nyc_taxi_data(input_path, output_path)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_name}: {e}\")\n",
    "    \n",
    "    print(\"üéâ –í—Å–µ —Ñ–∞–π–ª—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13223e7-6875-46fd-89aa-163df47d1617",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤\n",
    "process_all_nyc_taxi_files()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {execution_time:.4f} —Å–µ–∫—É–Ω–¥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e3feb-5cd6-4690-8a14-ab2c84ececd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4205bece-ef0e-48ba-ac62-1bc23208cdbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6644c8-078c-4cb6-9df8-c0e42abe51e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d794e42-a050-4221-a1f5-21c1d5e9ee88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting test Spark application...\n",
      "‚úÖ Spark session created successfully!\n",
      "üìä Test DataFrame:\n",
      "+---+-------+-----+\n",
      "| id|   name|value|\n",
      "+---+-------+-----+\n",
      "|  1|  Alice|  100|\n",
      "|  2|    Bob|  200|\n",
      "|  3|Charlie|  300|\n",
      "|  4|  David|  400|\n",
      "|  5|    Eve|  500|\n",
      "+---+-------+-----+\n",
      "\n",
      "üí∞ Total value: 1500\n",
      "[Row(summ=1500, average=300.0, max_name='Eve')]\n",
      "Row(summ=1500, average=300.0, max_name='Eve')\n",
      "1500\n",
      "300.0\n",
      "‚úÖ Spark application completed successfully!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def main():\n",
    "    print(\"üöÄ Starting test Spark application...\")\n",
    "\n",
    "    # –ü—Ä–æ—Å—Ç–∞—è —Å–µ—Å—Å–∏—è –±–µ–∑ –ª–∏—à–Ω–∏—Ö –∫–æ–Ω—Ñ–∏–≥–æ–≤\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"airflow-test-app\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º\n",
    "    schema = StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"value\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    data = [\n",
    "        (1, \"Alice\", 100),\n",
    "        (2, \"Bob\", 200),\n",
    "        (3, \"Charlie\", 300),\n",
    "        (4, \"David\", 400),\n",
    "        (5, \"Eve\", 500)\n",
    "    ]\n",
    "\n",
    "    df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "    print(\"‚úÖ Spark session created successfully!\")\n",
    "    print(\"üìä Test DataFrame:\")\n",
    "    df.show()\n",
    "\n",
    "    # –ü—Ä–æ—Å—Ç–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
    "    result = (df\n",
    "              .groupBy()\n",
    "              .agg(\n",
    "                  F.sum(\"value\").alias(\"summ\"),\n",
    "                  F.avg(\"value\").alias(\"average\"),\n",
    "                  F.max(\"name\").alias(\"max_name\"),                 \n",
    "              )\n",
    "              .collect())\n",
    "    total_value = result[0][0]\n",
    "    print(f\"üí∞ Total value: {total_value}\")\n",
    "\n",
    "    print(result)\n",
    "    print(result[0])\n",
    "    print(result[0][0])\n",
    "    print(result[0][1])\n",
    "    \n",
    "    print(\"‚úÖ Spark application completed successfully!\")\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54f48a2-485c-4fd8-8523-18020ddab39e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fe09fc-938f-443a-896c-10adac5c09a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1a1d3f-8eae-4869-b381-4a44672437b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "import re\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "import time\n",
    "\n",
    "# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è MinIO\n",
    "MINIO_ENDPOINT = 'minio:9000' \n",
    "MINIO_ACCESS_KEY = 'minioadmin'\n",
    "MINIO_SECRET_KEY = 'minioadmin'\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"airflow-test-app\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def get_minio_client():\n",
    "    \"\"\"–°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–ª–∏–µ–Ω—Ç MinIO\"\"\"\n",
    "    return Minio(\n",
    "        MINIO_ENDPOINT,\n",
    "        access_key=MINIO_ACCESS_KEY,\n",
    "        secret_key=MINIO_SECRET_KEY,\n",
    "        secure=False  \n",
    "    )\n",
    "\n",
    "def extract_month_from_filename(file_path):\n",
    "    \"\"\"–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Å—è—Ü –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYY-MM\"\"\"\n",
    "    match = re.search(r'(\\d{4}-\\d{2})', file_path)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def get_processed_slices(output_bucket, output_prefix):\n",
    "    \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ä–µ–∑–æ–≤ –∏–∑ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –±–∞–∫–µ—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É—è MinIO\"\"\"\n",
    "    try:\n",
    "        client = get_minio_client()\n",
    "        processed_slices = set()\n",
    "        \n",
    "        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –ø—Ä–µ—Ñ–∏–∫—Å–µ\n",
    "        objects = client.list_objects(output_bucket, prefix=output_prefix, recursive=True)\n",
    "        \n",
    "        for obj in objects:\n",
    "            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Å—è—Ü –∏–∑ –ø—É—Ç–∏\n",
    "            month = extract_month_from_filename(obj.object_name)\n",
    "            if month:\n",
    "                processed_slices.add(month)\n",
    "        \n",
    "        print(f\"üìÅ –ù–∞–π–¥–µ–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ä–µ–∑–æ–≤ –≤ {output_bucket}/{output_prefix}: {len(processed_slices)}\")\n",
    "        return processed_slices\n",
    "        \n",
    "    except S3Error as e:\n",
    "        if e.code == 'NoSuchBucket':\n",
    "            print(f\"‚ö†Ô∏è –ë–∞–∫–µ—Ç {output_bucket} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –ø—É—Å—Ç–æ–π\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {output_bucket} –±–∞–∫–µ—Ç–∞: {e}\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å {output_bucket} –±–∞–∫–µ—Ç: {e}\")\n",
    "        return set()\n",
    "\n",
    "def get_input_files_with_months(input_bucket, input_prefix):\n",
    "    \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤/–ø–∞–ø–æ–∫ –∏–∑ –≤—Ö–æ–¥–Ω–æ–≥–æ –±–∞–∫–µ—Ç–∞ —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ –º–µ—Å—è—Ü–∞–º–∏ –∏—Å–ø–æ–ª—å–∑—É—è MinIO\"\"\"\n",
    "    try:\n",
    "        client = get_minio_client()\n",
    "        input_files = []\n",
    "        \n",
    "        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –ü–ï–†–í–û–ú —É—Ä–æ–≤–Ω–µ –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏ (–Ω–µ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ!)\n",
    "        objects = client.list_objects(input_bucket, prefix=input_prefix, recursive=False)\n",
    "        \n",
    "        for obj in objects:\n",
    "            object_name = obj.object_name\n",
    "            \n",
    "            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ–±—ä–µ–∫—Ç–∞: —Ñ–∞–π–ª .parquet –∏–ª–∏ –ø–∞–ø–∫–∞\n",
    "            is_parquet_file = object_name.endswith('.parquet')\n",
    "            is_folder = not is_parquet_file and object_name.endswith('/')\n",
    "            \n",
    "            if is_parquet_file or is_folder:\n",
    "                month = extract_month_from_filename(object_name)\n",
    "                if month:\n",
    "                    # –§–æ—Ä–º–∏—Ä—É–µ–º S3 –ø—É—Ç—å –¥–ª—è Spark\n",
    "                    s3_path = f\"s3a://{input_bucket}/{object_name}\"\n",
    "                    input_files.append({\n",
    "                        'path': s3_path,\n",
    "                        'month': month,\n",
    "                        'file_name': object_name.split('/')[-1] if is_parquet_file else object_name.split('/')[-2] + '/'\n",
    "                    })\n",
    "        \n",
    "        print(f\"üìÅ –ù–∞–π–¥–µ–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ {input_bucket}/{input_prefix}: {len(input_files)}\")\n",
    "        return input_files\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {input_bucket} –±–∞–∫–µ—Ç–∞: {e}\")\n",
    "        return []\n",
    "\n",
    "def standardize_nyc_taxi_data(input_path, output_path):\n",
    "    \"\"\"\n",
    "    –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ NYC Taxi (–≤–∞—à–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è —Ñ—É–Ω–∫—Ü–∏—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)\n",
    "    \"\"\"\n",
    "    output_path = output_path.replace('.parquet', '')\n",
    "    \n",
    "    # –ß–∏—Ç–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "    df = spark.read.parquet(input_path)\n",
    "    \n",
    "    # 1. –ü—Ä–∏–≤–æ–¥–∏–º –≤—Å–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "    for col_name in df.columns:\n",
    "        df = df.withColumnRenamed(col_name, col_name.lower())\n",
    "    \n",
    "    # 2. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥ —Ç–∏–ø–æ–≤ –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "    type_mapping = {\n",
    "        \"vendorid\": IntegerType(),\n",
    "        \"pulocationid\": IntegerType(), \n",
    "        \"dolocationid\": IntegerType(),\n",
    "        \"payment_type\": IntegerType(),\n",
    "        \"ratecodeid\": IntegerType(),\n",
    "        \"passenger_count\": IntegerType(),\n",
    "        \"fare_amount\": DoubleType(),\n",
    "        \"extra\": DoubleType(),\n",
    "        \"mta_tax\": DoubleType(),\n",
    "        \"tip_amount\": DoubleType(),\n",
    "        \"tolls_amount\": DoubleType(),\n",
    "        \"improvement_surcharge\": DoubleType(),\n",
    "        \"total_amount\": DoubleType(),\n",
    "        \"congestion_surcharge\": DoubleType(),\n",
    "        \"airport_fee\": DoubleType(),\n",
    "        \"cbd_congestion_fee\": DoubleType(),\n",
    "        \"trip_distance\": DoubleType()\n",
    "    }\n",
    "    \n",
    "    # 3. –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫\n",
    "    for col_name, target_type in type_mapping.items():\n",
    "        if col_name in df.columns:\n",
    "            df = df.withColumn(\n",
    "                col_name, \n",
    "                F.coalesce(\n",
    "                    F.col(col_name).cast(target_type), \n",
    "                    F.lit(0 if target_type == IntegerType() else 0.0)\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    # 4. –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –ø–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è consistency\n",
    "    expected_columns = [\n",
    "        \"vendorid\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\",\n",
    "        \"passenger_count\", \"trip_distance\", \"ratecodeid\", \"store_and_fwd_flag\",\n",
    "        \"pulocationid\", \"dolocationid\", \"payment_type\", \"fare_amount\", \"extra\",\n",
    "        \"mta_tax\", \"tip_amount\", \"tolls_amount\", \"improvement_surcharge\",\n",
    "        \"total_amount\", \"congestion_surcharge\", \"airport_fee\", \"cbd_congestion_fee\"\n",
    "    ]\n",
    "    \n",
    "    # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ\n",
    "    final_columns = [col for col in expected_columns if col in df.columns]\n",
    "    df_standardized = df.select(final_columns)\n",
    "    \n",
    "    # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏\n",
    "    (df_standardized\n",
    "     .coalesce(1)\n",
    "     .write\n",
    "     .mode(\"overwrite\")\n",
    "     .option(\"compression\", \"snappy\")\n",
    "     .parquet(output_path)\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {input_path} -> {output_path}\")\n",
    "    return df_standardized\n",
    "\n",
    "def process_incremental_nyc_taxi_files(input_bucket, input_prefix, output_bucket, output_prefix):\n",
    "    \"\"\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã NYC Taxi –∏–∑ –≤—Ö–æ–¥–Ω–æ–≥–æ –±–∞–∫–µ—Ç–∞ –≤ –≤—ã—Ö–æ–¥–Ω–æ–π\"\"\"\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∏ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ —á–µ—Ä–µ–∑ MinIO\n",
    "    processed_slices = get_processed_slices(output_bucket, output_prefix)\n",
    "    input_files = get_input_files_with_months(input_bucket, input_prefix)\n",
    "    \n",
    "    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã\n",
    "    new_files = [f for f in input_files if f['month'] not in processed_slices]\n",
    "    \n",
    "    print(f\"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
    "    print(f\"   - –í—Å–µ–≥–æ –≤–æ –≤—Ö–æ–¥–Ω–æ–º –±–∞–∫–µ—Ç–µ: {len(input_files)}\")\n",
    "    print(f\"   - –£–∂–µ –≤ –≤—ã—Ö–æ–¥–Ω–æ–º –±–∞–∫–µ—Ç–µ: {len(processed_slices)}\") \n",
    "    print(f\"   - –ù–æ–≤—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(new_files)}\")\n",
    "    \n",
    "    if not new_files:\n",
    "        print(\"üéâ –í—Å–µ —Å—Ä–µ–∑—ã —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã! –ù–∏—á–µ–≥–æ –¥–µ–ª–∞—Ç—å –Ω–µ –Ω—É–∂–Ω–æ.\")\n",
    "        return\n",
    "    \n",
    "    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã\n",
    "    for i, file_info in enumerate(new_files, 1):\n",
    "        input_path = file_info['path']\n",
    "        file_name = file_info['file_name']\n",
    "        \n",
    "        # –§–æ—Ä–º–∏—Ä—É–µ–º –≤—ã—Ö–æ–¥–Ω–æ–π –ø—É—Ç—å, —Å–æ—Ö—Ä–∞–Ω—è—è —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–æ—Å–ª–µ –ø—Ä–µ—Ñ–∏–∫—Å–∞\n",
    "        # –ü—Ä–∏–º–µ—Ä: –≤—Ö–æ–¥–Ω–æ–π –ø—É—Ç—å s3a://bronze/nyc-taxi-data/yellow_tripdata_2022-01\n",
    "        # –í—ã—Ö–æ–¥–Ω–æ–π –ø—É—Ç—å: s3a://silver/nyc-taxi-data/yellow_tripdata_2022-01\n",
    "        output_path = f\"s3a://{output_bucket}/{output_prefix}{file_name}\".replace('.parquet', '')\n",
    "        \n",
    "        print(f\"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –Ω–æ–≤—ã–π —Å—Ä–µ–∑ ({i}/{len(new_files)}): {file_info['month']}\")\n",
    "        \n",
    "        try:\n",
    "            standardize_nyc_taxi_data(input_path, output_path)\n",
    "            print(f\"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {file_info['month']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_info['month']}: {e}\")\n",
    "    \n",
    "    print(f\"üéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(new_files)} –Ω–æ–≤—ã—Ö —Å—Ä–µ–∑–æ–≤.\")\n",
    "\n",
    "# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ - –ø—Ä–æ—Å–º–æ—Ç—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –±–∞–∫–µ—Ç–æ–≤\n",
    "def inspect_buckets(bucket_name, prefix):\n",
    "    \"\"\"–§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ - –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –±–∞–∫–µ—Ç–æ–≤\"\"\"\n",
    "    try:\n",
    "        client = get_minio_client()\n",
    "        \n",
    "        print(f\"üîç –ò–Ω—Å–ø–µ–∫—Ü–∏—è –±–∞–∫–µ—Ç–∞ {bucket_name}/{prefix}:\")\n",
    "        objects = client.list_objects(bucket_name, prefix=prefix, recursive=True)\n",
    "        for obj in objects:\n",
    "            print(f\"   {bucket_name}: {obj.object_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω—Å–ø–µ–∫—Ü–∏–∏ –±–∞–∫–µ—Ç–∞ {bucket_name}: {e}\")\n",
    "\n",
    "\n",
    "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ bronze –≤ silver\n",
    "process_incremental_nyc_taxi_files(\n",
    "    input_bucket='bronze', \n",
    "    input_prefix='nyc-taxi-data/',\n",
    "    output_bucket='silver', \n",
    "    output_prefix='nyc-taxi-data-norm/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5c6bbd-9003-4eb1-8f45-403861e94cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f55ee89-d1a5-4e37-a143-68958a4cdbfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f8550d-60cd-42bc-b1e2-58eb52c6e404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
