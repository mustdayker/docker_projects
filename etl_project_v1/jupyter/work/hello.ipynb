{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f52bac4-233c-49f1-b780-33303e38d045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.connectors import get_df_from_postgres, ddl_on_postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99f79942-2d21-45a3-b24f-bb1bd9f2b751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_month</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>pulocationid</th>\n",
       "      <th>trip_count</th>\n",
       "      <th>total_revenue</th>\n",
       "      <th>avg_revenue</th>\n",
       "      <th>avg_duration</th>\n",
       "      <th>avg_distance</th>\n",
       "      <th>avg_speed</th>\n",
       "      <th>avg_tip_ratio</th>\n",
       "      <th>tip_probability</th>\n",
       "      <th>avg_passengers</th>\n",
       "      <th>avg_efficiency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>–î–µ–Ω—å</td>\n",
       "      <td>161</td>\n",
       "      <td>10021</td>\n",
       "      <td>176047.33</td>\n",
       "      <td>17.567841</td>\n",
       "      <td>14.881506</td>\n",
       "      <td>2.181883</td>\n",
       "      <td>12.810100</td>\n",
       "      <td>0.210130</td>\n",
       "      <td>0.763696</td>\n",
       "      <td>1.313941</td>\n",
       "      <td>1.328225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>–í–µ—á–µ—Ä</td>\n",
       "      <td>262</td>\n",
       "      <td>1588</td>\n",
       "      <td>27347.96</td>\n",
       "      <td>17.221637</td>\n",
       "      <td>11.563308</td>\n",
       "      <td>2.312790</td>\n",
       "      <td>18.772555</td>\n",
       "      <td>0.231688</td>\n",
       "      <td>0.812972</td>\n",
       "      <td>1.280227</td>\n",
       "      <td>1.866740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>–£—Ç—Ä–æ</td>\n",
       "      <td>24</td>\n",
       "      <td>732</td>\n",
       "      <td>13281.63</td>\n",
       "      <td>18.144303</td>\n",
       "      <td>15.216872</td>\n",
       "      <td>2.804986</td>\n",
       "      <td>17.931714</td>\n",
       "      <td>0.197625</td>\n",
       "      <td>0.789617</td>\n",
       "      <td>1.274590</td>\n",
       "      <td>1.339882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>–£—Ç—Ä–æ</td>\n",
       "      <td>254</td>\n",
       "      <td>12</td>\n",
       "      <td>567.85</td>\n",
       "      <td>47.320833</td>\n",
       "      <td>39.469444</td>\n",
       "      <td>11.066667</td>\n",
       "      <td>26.557805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.263949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>–í–µ—á–µ—Ä</td>\n",
       "      <td>166</td>\n",
       "      <td>864</td>\n",
       "      <td>15290.50</td>\n",
       "      <td>17.697338</td>\n",
       "      <td>13.579591</td>\n",
       "      <td>2.714213</td>\n",
       "      <td>18.253560</td>\n",
       "      <td>0.207913</td>\n",
       "      <td>0.802083</td>\n",
       "      <td>1.268519</td>\n",
       "      <td>1.449197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  date_month  year  month  day_of_week time_of_day  pulocationid  trip_count  \\\n",
       "0 2022-03-01  2022      3            3        –î–µ–Ω—å           161       10021   \n",
       "1 2022-03-01  2022      3            3       –í–µ—á–µ—Ä           262        1588   \n",
       "2 2022-03-01  2022      3            4        –£—Ç—Ä–æ            24         732   \n",
       "3 2022-03-01  2022      3            3        –£—Ç—Ä–æ           254          12   \n",
       "4 2022-03-01  2022      3            4       –í–µ—á–µ—Ä           166         864   \n",
       "\n",
       "   total_revenue  avg_revenue  avg_duration  avg_distance  avg_speed  \\\n",
       "0      176047.33    17.567841     14.881506      2.181883  12.810100   \n",
       "1       27347.96    17.221637     11.563308      2.312790  18.772555   \n",
       "2       13281.63    18.144303     15.216872      2.804986  17.931714   \n",
       "3         567.85    47.320833     39.469444     11.066667  26.557805   \n",
       "4       15290.50    17.697338     13.579591      2.714213  18.253560   \n",
       "\n",
       "   avg_tip_ratio  tip_probability  avg_passengers  avg_efficiency  \n",
       "0       0.210130         0.763696        1.313941        1.328225  \n",
       "1       0.231688         0.812972        1.280227        1.866740  \n",
       "2       0.197625         0.789617        1.274590        1.339882  \n",
       "3       0.000000         0.000000        1.000000        1.263949  \n",
       "4       0.207913         0.802083        1.268519        1.449197  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = get_df_from_postgres(\"SELECT * FROM nyc_taxi.nyc_taxi_agg_table LIMIT 5\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a56c39-9590-46d0-9bb0-9e891ca5581c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6b9f9b-4a9f-4827-8587-1c80b3498354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97019ab1-d9a0-4b57-8016-cd9175bea2a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b198c4-ea52-4343-af9b-09ee4b773df2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c4fa0b-a17b-4d2d-a367-1bc4d2f51a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ffff3b-f7f8-4a1b-bbd6-b90a10929d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acc8509-d9e7-4367-ba8f-6433c796a79a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b29007-0cc9-4775-b5e3-ec607dca1d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09f5f40-9d6a-464e-bb24-4cdb5c17bfe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaef7489-2677-473e-b897-cbde80bebb76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50277e43-6e66-41c6-adca-e590f3bae47d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d914c3-62d8-48d1-b322-24d056232b28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a31bdd8-2389-4a19-ae59-72007f093dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDL —Å–∫—Ä–∏–ø—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ\n"
     ]
    }
   ],
   "source": [
    "ddl_on_postgres(\"CREATE SCHEMA IF NOT EXISTS testovoe_ololo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78abebc0-8ef3-4d79-83d6-dc1c9a8e4b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a436ff02-c4ef-40aa-85c5-15fedd4565b5",
   "metadata": {},
   "source": [
    "### Spark —Å–µ—Å—Å–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8ada13-0939-4069-899c-608687a95b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "drivers = [\n",
    "    \"/home/jovyan/work/spark-jars/hadoop-aws-3.3.4.jar\",             # S3\n",
    "    \"/home/jovyan/work/spark-jars/aws-java-sdk-bundle-1.12.262.jar\", # S3\n",
    "    \"/home/jovyan/work/spark-jars/wildfly-openssl-1.0.7.Final.jar\",  # S3\n",
    "    \"/home/jovyan/work/spark-jars/postgresql-42.6.0.jar\",            # PostgreSQL\n",
    "]\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"mustdayker-Spark2\")\n",
    "         .master(\"spark://spark-master:7077\") \n",
    "         .config(\"spark.jars\", \",\".join(drivers))\n",
    "         .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4170a035-4b55-4574-9929-18de614ba2f4",
   "metadata": {},
   "source": [
    "### –ß–∏—Ç–∞–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac204c9-a6ca-4ff3-8dbd-1ca2c7fb17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023_01 = spark.read.parquet(\"s3a://silver/nyc-taxi-data/yellow_tripdata_2023-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8768d1-8ded-4b3d-893e-51c26b410166",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2024_01 = spark.read.parquet(\"s3a://bronze/nyc-taxi-data/yellow_tripdata_2024-01.parquet\")\n",
    "df_2025_01 = spark.read.parquet(\"s3a://bronze/nyc-taxi-data/yellow_tripdata_2025-01.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36653660-b6e1-438e-8c4b-5203f601609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = \"silver\"\n",
    "\n",
    "df = (spark.read\n",
    "      .format(\"parquet\")\n",
    "      .option(\"mergeSchema\", \"true\")\n",
    "      .load([\n",
    "          f\"s3a://{layer}/nyc-taxi-data/yellow_tripdata_2023-*\",\n",
    "          f\"s3a://{layer}/nyc-taxi-data/yellow_tripdata_2024-*\",\n",
    "          f\"s3a://{layer}/nyc-taxi-data/yellow_tripdata_2025-*\",\n",
    "      ])\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8762ecf-0e8a-4c46-a43c-75d0e1a0dec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"–ö–æ–ª–∏—á–µ—Ç—Å–≤–æ —Å—Ç—Ä–æ–∫: {df_2023_01.count()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621f0e8c-1cfd-48d4-9d19-c4f820dbf350",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf76c46-f813-4267-8615-40bd52a117ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"–ö–æ–ª–∏—á–µ—Ç—Å–≤–æ —Å—Ç—Ä–æ–∫: {df.count()}\\n\")\n",
    "\n",
    "(df.withColumn(\"year\",  F.year(\"tpep_pickup_datetime\"))\n",
    "   .withColumn(\"month\", F.month(\"tpep_pickup_datetime\"))\n",
    "   .select(\"year\", \n",
    "           \"month\", \n",
    "           \"VendorID\",\n",
    "           \"tpep_pickup_datetime\", \n",
    "           \"passenger_count\", \n",
    "           \"trip_distance\",\n",
    "           \"airport_fee\",\n",
    "          )\n",
    "   .groupBy(\"year\", \"month\")\n",
    "   .agg(\n",
    "       F.count(\"*\").alias(\"total_records\"),\n",
    "       F.sum(\"airport_fee\").alias(\"–û–±–¥–∏—Ä–∞–ª–æ–≤–∫–∞\"),\n",
    "       )\n",
    "   .orderBy(\"year\", \"month\")\n",
    "   .show(50)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3de6e-633c-40e1-b1f9-5084a8f6c509",
   "metadata": {},
   "source": [
    "### –ò—Å—Å–ª–µ–¥—É–µ–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf043c8a-8a10-407b-bcaf-d275ec43f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "def standardize_nyc_taxi_data(input_path, output_path):\n",
    "    \"\"\"\n",
    "    –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ NYC Taxi:\n",
    "    - –ü—Ä–∏–≤–æ–¥–∏—Ç –≤—Å–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "    - –ü—Ä–∏–≤–æ–¥–∏—Ç —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –∫ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–º\n",
    "    - –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø—É—Ç—å\n",
    "    \"\"\"\n",
    "\n",
    "    output_path = output_path.replace('.parquet', '')\n",
    "    \n",
    "    # –ß–∏—Ç–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "    df = spark.read.parquet(input_path)\n",
    "    \n",
    "    # 1. –ü—Ä–∏–≤–æ–¥–∏–º –≤—Å–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "    for col_name in df.columns:\n",
    "        df = df.withColumnRenamed(col_name, col_name.lower())\n",
    "    \n",
    "    # 2. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥ —Ç–∏–ø–æ–≤ –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "    type_mapping = {\n",
    "        # –ß–∏—Å–ª–æ–≤—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã - –≤ Integer\n",
    "        \"vendorid\": IntegerType(),\n",
    "        \"pulocationid\": IntegerType(), \n",
    "        \"dolocationid\": IntegerType(),\n",
    "        \"payment_type\": IntegerType(),\n",
    "        \"ratecodeid\": IntegerType(),\n",
    "        \n",
    "        # –ü–∞—Å—Å–∞–∂–∏—Ä—ã - –≤ Integer (–±–æ–ª–µ–µ –ª–æ–≥–∏—á–Ω–æ)\n",
    "        \"passenger_count\": IntegerType(),\n",
    "        \n",
    "        # –î–µ–Ω–µ–∂–Ω—ã–µ —Å—É–º–º—ã - –≤ Double (–¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏)\n",
    "        \"fare_amount\": DoubleType(),\n",
    "        \"extra\": DoubleType(),\n",
    "        \"mta_tax\": DoubleType(),\n",
    "        \"tip_amount\": DoubleType(),\n",
    "        \"tolls_amount\": DoubleType(),\n",
    "        \"improvement_surcharge\": DoubleType(),\n",
    "        \"total_amount\": DoubleType(),\n",
    "        \"congestion_surcharge\": DoubleType(),\n",
    "        \"airport_fee\": DoubleType(),\n",
    "        \"cbd_congestion_fee\": DoubleType(),\n",
    "        \n",
    "        # –î–∏—Å—Ç–∞–Ω—Ü–∏—è - –≤ Double\n",
    "        \"trip_distance\": DoubleType()\n",
    "    }\n",
    "    \n",
    "    # 3. –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫\n",
    "    for col_name, target_type in type_mapping.items():\n",
    "        if col_name in df.columns:\n",
    "            df = df.withColumn(\n",
    "                col_name, \n",
    "                F.coalesce(\n",
    "                    F.col(col_name).cast(target_type), \n",
    "                    F.lit(0 if target_type == IntegerType() else 0.0)\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    # 4. –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –ø–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è consistency\n",
    "    expected_columns = [\n",
    "        \"vendorid\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\",\n",
    "        \"passenger_count\", \"trip_distance\", \"ratecodeid\", \"store_and_fwd_flag\",\n",
    "        \"pulocationid\", \"dolocationid\", \"payment_type\", \"fare_amount\", \"extra\",\n",
    "        \"mta_tax\", \"tip_amount\", \"tolls_amount\", \"improvement_surcharge\",\n",
    "        \"total_amount\", \"congestion_surcharge\", \"airport_fee\", \"cbd_congestion_fee\"\n",
    "    ]\n",
    "    \n",
    "    # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ\n",
    "    final_columns = [col for col in expected_columns if col in df.columns]\n",
    "    df_standardized = df.select(final_columns)\n",
    "    \n",
    "    # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏\n",
    "    (df_standardized\n",
    "     .coalesce(1)\n",
    "     .write\n",
    "     .mode(\"overwrite\")\n",
    "     .option(\"compression\", \"snappy\")  # —Ö–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç—å/—Å–∂–∞—Ç–∏–µ\n",
    "     .parquet(output_path)\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {input_path} -> {output_path}\")\n",
    "    # print(f\"üìä –°—Ö–µ–º–∞ –ø–æ—Å–ª–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏:\")\n",
    "    # df_standardized.printSchema()\n",
    "    \n",
    "    return df_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd36c7e4-33e0-42e4-8bb3-0c8b620e18c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞\n",
    "input_path = \"s3a://bronze/nyc-taxi-data/yellow_tripdata_2023-01.parquet\"\n",
    "output_path = \"s3a://silver/nyc-taxi-data/yellow_tripdata_2023-01.parquet\"\n",
    "\n",
    "standardized_df = standardize_nyc_taxi_data(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c1f8ed-0c6d-4ff8-a73c-66884d269d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_nyc_taxi_files():\n",
    "    \"\"\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã NYC Taxi –∏–∑ Bronze –≤ Silver\"\"\"\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö parquet —Ñ–∞–π–ª–æ–≤ –≤ bronze\n",
    "    bronze_files = spark.sql(f\"\"\"\n",
    "        SELECT path \n",
    "        FROM (\n",
    "            SELECT input_file_name() as path \n",
    "            FROM parquet.`s3a://bronze/nyc-taxi-data/`\n",
    "        ) \n",
    "        GROUP BY path\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    print(f\"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(bronze_files)}\")\n",
    "    \n",
    "    for i, row in enumerate(bronze_files, 1):\n",
    "        input_path = row['path']\n",
    "        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ –ø—É—Ç–∏\n",
    "        file_name = input_path.split('/')[-1]\n",
    "        output_path = f\"s3a://silver/nyc-taxi-data/{file_name}\"\n",
    "        \n",
    "        print(f\"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é ({i}/{len(bronze_files)}): {file_name}\")\n",
    "        \n",
    "        try:\n",
    "            standardize_nyc_taxi_data(input_path, output_path)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_name}: {e}\")\n",
    "    \n",
    "    print(\"üéâ –í—Å–µ —Ñ–∞–π–ª—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13223e7-6875-46fd-89aa-163df47d1617",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤\n",
    "process_all_nyc_taxi_files()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {execution_time:.4f} —Å–µ–∫—É–Ω–¥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e3feb-5cd6-4690-8a14-ab2c84ececd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4205bece-ef0e-48ba-ac62-1bc23208cdbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6644c8-078c-4cb6-9df8-c0e42abe51e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d794e42-a050-4221-a1f5-21c1d5e9ee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def main():\n",
    "    print(\"üöÄ Starting test Spark application...\")\n",
    "\n",
    "    # –ü—Ä–æ—Å—Ç–∞—è —Å–µ—Å—Å–∏—è –±–µ–∑ –ª–∏—à–Ω–∏—Ö –∫–æ–Ω—Ñ–∏–≥–æ–≤\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"airflow-test-app\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º\n",
    "    schema = StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"value\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    data = [\n",
    "        (1, \"Alice\", 100),\n",
    "        (2, \"Bob\", 200),\n",
    "        (3, \"Charlie\", 300),\n",
    "        (4, \"David\", 400),\n",
    "        (5, \"Eve\", 500)\n",
    "    ]\n",
    "\n",
    "    df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "    print(\"‚úÖ Spark session created successfully!\")\n",
    "    print(\"üìä Test DataFrame:\")\n",
    "    df.show()\n",
    "\n",
    "    # –ü—Ä–æ—Å—Ç–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
    "    result = (df\n",
    "              .groupBy()\n",
    "              .agg(\n",
    "                  F.sum(\"value\").alias(\"summ\"),\n",
    "                  F.avg(\"value\").alias(\"average\"),\n",
    "                  F.max(\"name\").alias(\"max_name\"),                 \n",
    "              )\n",
    "              .collect())\n",
    "    total_value = result[0][0]\n",
    "    print(f\"üí∞ Total value: {total_value}\")\n",
    "\n",
    "    print(result)\n",
    "    print(result[0])\n",
    "    print(result[0][0])\n",
    "    print(result[0][1])\n",
    "    \n",
    "    print(\"‚úÖ Spark application completed successfully!\")\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54f48a2-485c-4fd8-8523-18020ddab39e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fe09fc-938f-443a-896c-10adac5c09a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1a1d3f-8eae-4869-b381-4a44672437b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "import re\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "import time\n",
    "\n",
    "# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è MinIO\n",
    "MINIO_ENDPOINT = 'minio:9000' \n",
    "MINIO_ACCESS_KEY = 'minioadmin'\n",
    "MINIO_SECRET_KEY = 'minioadmin'\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"airflow-test-app\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def get_minio_client():\n",
    "    \"\"\"–°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–ª–∏–µ–Ω—Ç MinIO\"\"\"\n",
    "    return Minio(\n",
    "        MINIO_ENDPOINT,\n",
    "        access_key=MINIO_ACCESS_KEY,\n",
    "        secret_key=MINIO_SECRET_KEY,\n",
    "        secure=False  \n",
    "    )\n",
    "\n",
    "def extract_month_from_filename(file_path):\n",
    "    \"\"\"–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Å—è—Ü –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYY-MM\"\"\"\n",
    "    match = re.search(r'(\\d{4}-\\d{2})', file_path)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def get_processed_slices(output_bucket, output_prefix):\n",
    "    \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ä–µ–∑–æ–≤ –∏–∑ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –±–∞–∫–µ—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É—è MinIO\"\"\"\n",
    "    try:\n",
    "        client = get_minio_client()\n",
    "        processed_slices = set()\n",
    "        \n",
    "        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –ø—Ä–µ—Ñ–∏–∫—Å–µ\n",
    "        objects = client.list_objects(output_bucket, prefix=output_prefix, recursive=True)\n",
    "        \n",
    "        for obj in objects:\n",
    "            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Å—è—Ü –∏–∑ –ø—É—Ç–∏\n",
    "            month = extract_month_from_filename(obj.object_name)\n",
    "            if month:\n",
    "                processed_slices.add(month)\n",
    "        \n",
    "        print(f\"üìÅ –ù–∞–π–¥–µ–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ä–µ–∑–æ–≤ –≤ {output_bucket}/{output_prefix}: {len(processed_slices)}\")\n",
    "        return processed_slices\n",
    "        \n",
    "    except S3Error as e:\n",
    "        if e.code == 'NoSuchBucket':\n",
    "            print(f\"‚ö†Ô∏è –ë–∞–∫–µ—Ç {output_bucket} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –ø—É—Å—Ç–æ–π\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {output_bucket} –±–∞–∫–µ—Ç–∞: {e}\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å {output_bucket} –±–∞–∫–µ—Ç: {e}\")\n",
    "        return set()\n",
    "\n",
    "def get_input_files_with_months(input_bucket, input_prefix):\n",
    "    \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤/–ø–∞–ø–æ–∫ –∏–∑ –≤—Ö–æ–¥–Ω–æ–≥–æ –±–∞–∫–µ—Ç–∞ —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ –º–µ—Å—è—Ü–∞–º–∏ –∏—Å–ø–æ–ª—å–∑—É—è MinIO\"\"\"\n",
    "    try:\n",
    "        client = get_minio_client()\n",
    "        input_files = []\n",
    "        \n",
    "        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –ü–ï–†–í–û–ú —É—Ä–æ–≤–Ω–µ –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏ (–Ω–µ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ!)\n",
    "        objects = client.list_objects(input_bucket, prefix=input_prefix, recursive=False)\n",
    "        \n",
    "        for obj in objects:\n",
    "            object_name = obj.object_name\n",
    "            \n",
    "            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ–±—ä–µ–∫—Ç–∞: —Ñ–∞–π–ª .parquet –∏–ª–∏ –ø–∞–ø–∫–∞\n",
    "            is_parquet_file = object_name.endswith('.parquet')\n",
    "            is_folder = not is_parquet_file and object_name.endswith('/')\n",
    "            \n",
    "            if is_parquet_file or is_folder:\n",
    "                month = extract_month_from_filename(object_name)\n",
    "                if month:\n",
    "                    # –§–æ—Ä–º–∏—Ä—É–µ–º S3 –ø—É—Ç—å –¥–ª—è Spark\n",
    "                    s3_path = f\"s3a://{input_bucket}/{object_name}\"\n",
    "                    input_files.append({\n",
    "                        'path': s3_path,\n",
    "                        'month': month,\n",
    "                        'file_name': object_name.split('/')[-1] if is_parquet_file else object_name.split('/')[-2] + '/'\n",
    "                    })\n",
    "        \n",
    "        print(f\"üìÅ –ù–∞–π–¥–µ–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ {input_bucket}/{input_prefix}: {len(input_files)}\")\n",
    "        return input_files\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {input_bucket} –±–∞–∫–µ—Ç–∞: {e}\")\n",
    "        return []\n",
    "\n",
    "def standardize_nyc_taxi_data(input_path, output_path):\n",
    "    \"\"\"\n",
    "    –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ NYC Taxi (–≤–∞—à–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è —Ñ—É–Ω–∫—Ü–∏—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)\n",
    "    \"\"\"\n",
    "    output_path = output_path.replace('.parquet', '')\n",
    "    \n",
    "    # –ß–∏—Ç–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "    df = spark.read.parquet(input_path)\n",
    "    \n",
    "    # 1. –ü—Ä–∏–≤–æ–¥–∏–º –≤—Å–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "    for col_name in df.columns:\n",
    "        df = df.withColumnRenamed(col_name, col_name.lower())\n",
    "    \n",
    "    # 2. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥ —Ç–∏–ø–æ–≤ –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "    type_mapping = {\n",
    "        \"vendorid\": IntegerType(),\n",
    "        \"pulocationid\": IntegerType(), \n",
    "        \"dolocationid\": IntegerType(),\n",
    "        \"payment_type\": IntegerType(),\n",
    "        \"ratecodeid\": IntegerType(),\n",
    "        \"passenger_count\": IntegerType(),\n",
    "        \"fare_amount\": DoubleType(),\n",
    "        \"extra\": DoubleType(),\n",
    "        \"mta_tax\": DoubleType(),\n",
    "        \"tip_amount\": DoubleType(),\n",
    "        \"tolls_amount\": DoubleType(),\n",
    "        \"improvement_surcharge\": DoubleType(),\n",
    "        \"total_amount\": DoubleType(),\n",
    "        \"congestion_surcharge\": DoubleType(),\n",
    "        \"airport_fee\": DoubleType(),\n",
    "        \"cbd_congestion_fee\": DoubleType(),\n",
    "        \"trip_distance\": DoubleType()\n",
    "    }\n",
    "    \n",
    "    # 3. –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫\n",
    "    for col_name, target_type in type_mapping.items():\n",
    "        if col_name in df.columns:\n",
    "            df = df.withColumn(\n",
    "                col_name, \n",
    "                F.coalesce(\n",
    "                    F.col(col_name).cast(target_type), \n",
    "                    F.lit(0 if target_type == IntegerType() else 0.0)\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    # 4. –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –ø–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è consistency\n",
    "    expected_columns = [\n",
    "        \"vendorid\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\",\n",
    "        \"passenger_count\", \"trip_distance\", \"ratecodeid\", \"store_and_fwd_flag\",\n",
    "        \"pulocationid\", \"dolocationid\", \"payment_type\", \"fare_amount\", \"extra\",\n",
    "        \"mta_tax\", \"tip_amount\", \"tolls_amount\", \"improvement_surcharge\",\n",
    "        \"total_amount\", \"congestion_surcharge\", \"airport_fee\", \"cbd_congestion_fee\"\n",
    "    ]\n",
    "    \n",
    "    # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ\n",
    "    final_columns = [col for col in expected_columns if col in df.columns]\n",
    "    df_standardized = df.select(final_columns)\n",
    "    \n",
    "    # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏\n",
    "    (df_standardized\n",
    "     .coalesce(1)\n",
    "     .write\n",
    "     .mode(\"overwrite\")\n",
    "     .option(\"compression\", \"snappy\")\n",
    "     .parquet(output_path)\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {input_path} -> {output_path}\")\n",
    "    return df_standardized\n",
    "\n",
    "def process_incremental_nyc_taxi_files(input_bucket, input_prefix, output_bucket, output_prefix):\n",
    "    \"\"\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã NYC Taxi –∏–∑ –≤—Ö–æ–¥–Ω–æ–≥–æ –±–∞–∫–µ—Ç–∞ –≤ –≤—ã—Ö–æ–¥–Ω–æ–π\"\"\"\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∏ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ —á–µ—Ä–µ–∑ MinIO\n",
    "    processed_slices = get_processed_slices(output_bucket, output_prefix)\n",
    "    input_files = get_input_files_with_months(input_bucket, input_prefix)\n",
    "    \n",
    "    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã\n",
    "    new_files = [f for f in input_files if f['month'] not in processed_slices]\n",
    "    \n",
    "    print(f\"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
    "    print(f\"   - –í—Å–µ–≥–æ –≤–æ –≤—Ö–æ–¥–Ω–æ–º –±–∞–∫–µ—Ç–µ: {len(input_files)}\")\n",
    "    print(f\"   - –£–∂–µ –≤ –≤—ã—Ö–æ–¥–Ω–æ–º –±–∞–∫–µ—Ç–µ: {len(processed_slices)}\") \n",
    "    print(f\"   - –ù–æ–≤—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(new_files)}\")\n",
    "    \n",
    "    if not new_files:\n",
    "        print(\"üéâ –í—Å–µ —Å—Ä–µ–∑—ã —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã! –ù–∏—á–µ–≥–æ –¥–µ–ª–∞—Ç—å –Ω–µ –Ω—É–∂–Ω–æ.\")\n",
    "        return\n",
    "    \n",
    "    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã\n",
    "    for i, file_info in enumerate(new_files, 1):\n",
    "        input_path = file_info['path']\n",
    "        file_name = file_info['file_name']\n",
    "        \n",
    "        # –§–æ—Ä–º–∏—Ä—É–µ–º –≤—ã—Ö–æ–¥–Ω–æ–π –ø—É—Ç—å, —Å–æ—Ö—Ä–∞–Ω—è—è —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–æ—Å–ª–µ –ø—Ä–µ—Ñ–∏–∫—Å–∞\n",
    "        # –ü—Ä–∏–º–µ—Ä: –≤—Ö–æ–¥–Ω–æ–π –ø—É—Ç—å s3a://bronze/nyc-taxi-data/yellow_tripdata_2022-01\n",
    "        # –í—ã—Ö–æ–¥–Ω–æ–π –ø—É—Ç—å: s3a://silver/nyc-taxi-data/yellow_tripdata_2022-01\n",
    "        output_path = f\"s3a://{output_bucket}/{output_prefix}{file_name}\".replace('.parquet', '')\n",
    "        \n",
    "        print(f\"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –Ω–æ–≤—ã–π —Å—Ä–µ–∑ ({i}/{len(new_files)}): {file_info['month']}\")\n",
    "        \n",
    "        try:\n",
    "            standardize_nyc_taxi_data(input_path, output_path)\n",
    "            print(f\"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {file_info['month']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_info['month']}: {e}\")\n",
    "    \n",
    "    print(f\"üéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(new_files)} –Ω–æ–≤—ã—Ö —Å—Ä–µ–∑–æ–≤.\")\n",
    "\n",
    "# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ - –ø—Ä–æ—Å–º–æ—Ç—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –±–∞–∫–µ—Ç–æ–≤\n",
    "def inspect_buckets(bucket_name, prefix):\n",
    "    \"\"\"–§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ - –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –±–∞–∫–µ—Ç–æ–≤\"\"\"\n",
    "    try:\n",
    "        client = get_minio_client()\n",
    "        \n",
    "        print(f\"üîç –ò–Ω—Å–ø–µ–∫—Ü–∏—è –±–∞–∫–µ—Ç–∞ {bucket_name}/{prefix}:\")\n",
    "        objects = client.list_objects(bucket_name, prefix=prefix, recursive=True)\n",
    "        for obj in objects:\n",
    "            print(f\"   {bucket_name}: {obj.object_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω—Å–ø–µ–∫—Ü–∏–∏ –±–∞–∫–µ—Ç–∞ {bucket_name}: {e}\")\n",
    "\n",
    "\n",
    "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ bronze –≤ silver\n",
    "process_incremental_nyc_taxi_files(\n",
    "    input_bucket='bronze', \n",
    "    input_prefix='nyc-taxi-data/',\n",
    "    output_bucket='silver', \n",
    "    output_prefix='nyc-taxi-data-norm/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5c6bbd-9003-4eb1-8f45-403861e94cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f55ee89-d1a5-4e37-a143-68958a4cdbfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f8550d-60cd-42bc-b1e2-58eb52c6e404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
