{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d12b9c-e270-4b49-b56c-97db8f8956b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbfb275d-d744-472c-ba19-98bcd8780ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/27 20:44:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark —Å–µ—Å—Å–∏—è —Å–æ–∑–¥–∞–Ω–∞\n",
      "\n",
      "üìÅ –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/27 20:44:29 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Å–æ–±—ã—Ç–∏–π: 5 –∑–∞–ø–∏—Å–µ–π\n",
      "+--------+--------------------+\n",
      "|event_id|          event_name|\n",
      "+--------+--------------------+\n",
      "|       5|–ü–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏–µ —Å–µ–∫—Ç–∞–º|\n",
      "|       4|    –î–æ–Ω–∞—Ç–Ω–∞—è –ø–æ–º–æ–π–∫–∞|\n",
      "|       2|    –í–ª–æ–∂–µ–Ω–∏–µ –≤ –∞–∫—Ü–∏–∏|\n",
      "|       1|      –ü–æ–∫—É–ø–∫–∞ –∫—Ä–∏–ø—Ç—ã|\n",
      "|       3|       –û–ø–ª–∞—Ç–∞ —Ç–æ–≤–∞—Ä–∞|\n",
      "+--------+--------------------+\n",
      "\n",
      "\n",
      "–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –≥—Ä—É–ø–ø: 5 –∑–∞–ø–∏—Å–µ–π\n",
      "+--------+-----------+\n",
      "|group_id| group_name|\n",
      "+--------+-----------+\n",
      "|       2|–ò–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã–µ|\n",
      "|       3| –†–µ–ø—Ç–∏–ª–æ–π–¥—ã|\n",
      "|       1|      –ù–æ–≤—ã–µ|\n",
      "|       5|   Platinum|\n",
      "|       4|       Gold|\n",
      "+--------+-----------+\n",
      "\n",
      "\n",
      "üì® –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –ø–æ—Ç–æ–∫—É Kafka...\n",
      "‚úÖ –°—Ç—Ä–∏–º —Å–æ–∑–¥–∞–Ω, —Å—Ö–µ–º–∞:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- event_date: timestamp (nullable = true)\n",
      " |-- event_id: integer (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      " |-- group_id: integer (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n",
      "\n",
      "üìä –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ö–µ–º–∞ –ø–æ—Ç–æ–∫–∞:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- event_date: timestamp (nullable = true)\n",
      " |-- event_id: integer (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      " |-- group_id: integer (nullable = true)\n",
      " |-- group_name: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n",
      "\n",
      "üöÄ –ó–ê–ü–£–°–ö –°–¢–†–ò–ú–ò–ù–ì–ê –í POSTGRESQL\n",
      "==================================================\n",
      "‚úÖ –°—Ç—Ä–∏–º–∏–Ω–≥ –∑–∞–ø—É—â–µ–Ω –∏ –ø–∏—à–µ—Ç –≤ PostgreSQL!\n",
      "üìù –î–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç—É–ø–∞—é—Ç –≤ —Ç–∞–±–ª–∏—Ü—É: kafka_farm.user_events\n",
      "\n",
      "üîç –ö–û–ú–ê–ù–î–´ –£–ü–†–ê–í–õ–ï–ù–ò–Ø:\n",
      "--------------------------------------------------\n",
      "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å: query.isActive\n",
      "–ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å: query.recentProgress\n",
      "–û–°–¢–ê–ù–û–í–ò–¢–¨: query.stop()\n",
      "==================================================\n",
      "\n",
      "–°—Ç–∞—Ç—É—Å: –ê–ö–¢–ò–í–ï–ù\n",
      "ID —Å—Ç—Ä–∏–º–∞: 964ab753-6f74-443d-ac4c-4a9140bf8aa8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/27 20:44:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "26/02/27 20:44:41 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "26/02/27 20:44:41 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "26/02/27 20:44:41 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "26/02/27 20:44:41 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "26/02/27 20:44:41 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 0: –∑–∞–ø–∏—Å–∞–Ω–æ 0 –∑–∞–ø–∏—Å–µ–π\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 1: –∑–∞–ø–∏—Å–∞–Ω–æ 9 –∑–∞–ø–∏—Å–µ–π\n",
      "‚úÖ Batch 2: –∑–∞–ø–∏—Å–∞–Ω–æ 9 –∑–∞–ø–∏—Å–µ–π\n",
      "‚úÖ Batch 3: –∑–∞–ø–∏—Å–∞–Ω–æ 9 –∑–∞–ø–∏—Å–µ–π\n",
      "‚úÖ Batch 4: –∑–∞–ø–∏—Å–∞–Ω–æ 8 –∑–∞–ø–∏—Å–µ–π\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 5: –∑–∞–ø–∏—Å–∞–Ω–æ 9 –∑–∞–ø–∏—Å–µ–π\n",
      "‚úÖ Batch 6: –∑–∞–ø–∏—Å–∞–Ω–æ 9 –∑–∞–ø–∏—Å–µ–π\n",
      "‚úÖ Batch 7: –∑–∞–ø–∏—Å–∞–Ω–æ 9 –∑–∞–ø–∏—Å–µ–π\n",
      "‚úÖ Batch 8: –∑–∞–ø–∏—Å–∞–Ω–æ 8 –∑–∞–ø–∏—Å–µ–π\n",
      "‚úÖ Batch 9: –∑–∞–ø–∏—Å–∞–Ω–æ 9 –∑–∞–ø–∏—Å–µ–π\n",
      "‚úÖ Batch 10: –∑–∞–ø–∏—Å–∞–Ω–æ 9 –∑–∞–ø–∏—Å–µ–π\n",
      "‚úÖ Batch 11: –∑–∞–ø–∏—Å–∞–Ω–æ 9 –∑–∞–ø–∏—Å–µ–π\n",
      "‚úÖ Batch 12: –∑–∞–ø–∏—Å–∞–Ω–æ 9 –∑–∞–ø–∏—Å–µ–π\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, DateType\n",
    "\n",
    "# –°–ø–∏—Å–æ–∫ –¥—Ä–∞–π–≤–µ—Ä–æ–≤ (–∫–∞–∫ —É –≤–∞—Å)\n",
    "drivers = [\n",
    "    \"/opt/spark/external-jars/hadoop-aws-3.3.4.jar\",\n",
    "    \"/opt/spark/external-jars/aws-java-sdk-bundle-1.12.262.jar\",\n",
    "    \"/opt/spark/external-jars/wildfly-openssl-1.0.7.Final.jar\",\n",
    "    \"/opt/spark/external-jars/postgresql-42.6.0.jar\",\n",
    "    \"/opt/spark/external-jars/spark-sql-kafka-0-10_2.12-3.5.0.jar\",\n",
    "    \"/opt/spark/external-jars/kafka-clients-3.2.0.jar\",\n",
    "    \"/opt/spark/external-jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar\",\n",
    "    \"/opt/spark/external-jars/commons-pool2-2.11.1.jar\"\n",
    "]\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º Spark —Å–µ—Å—Å–∏—é (–±–µ–∑ –ª–∏—à–Ω–∏—Ö –∫–æ–Ω—Ñ–∏–≥–æ–≤)\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"KafkaStreamingToPostgres\")\n",
    "         .master(\"spark://spark-master:7077\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", 5)\n",
    "         .config(\"spark.jars\", \",\".join(drivers))\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"‚úÖ Spark —Å–µ—Å—Å–∏—è —Å–æ–∑–¥–∞–Ω–∞\")\n",
    "\n",
    "# 1. –ß–∏—Ç–∞–µ–º —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏ –∏–∑ MinIO (–∫–µ—à–∏—Ä—É–µ–º –∏—Ö –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)\n",
    "print(\"\\nüìÅ –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏...\")\n",
    "events_df = (spark.read\n",
    "             .format(\"parquet\")\n",
    "             .load(\"s3a://library/kafka-enrich/event_list.parquet\"))\n",
    "events_df.cache().count()\n",
    "print(f\"–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Å–æ–±—ã—Ç–∏–π: {events_df.count()} –∑–∞–ø–∏—Å–µ–π\")\n",
    "events_df.show()\n",
    "\n",
    "groups_df = (spark.read\n",
    "             .format(\"parquet\")\n",
    "             .load(\"s3a://library/kafka-enrich/group_list.parquet\"))\n",
    "groups_df.cache().count()\n",
    "print(f\"\\n–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –≥—Ä—É–ø–ø: {groups_df.count()} –∑–∞–ø–∏—Å–µ–π\")\n",
    "groups_df.show()\n",
    "\n",
    "# 2. –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ö–µ–º—É –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"event_date\", StringType(), True),\n",
    "    StructField(\"event_id\", IntegerType(), True),\n",
    "    StructField(\"username\", StringType(), True),\n",
    "    StructField(\"group_id\", IntegerType(), True),\n",
    "    StructField(\"value\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# 3. –°–û–ó–î–ê–ï–ú –°–¢–†–ò–ú–ò–ù–ì –≤–º–µ—Å—Ç–æ –±–∞—Ç—á-—á—Ç–µ–Ω–∏—è\n",
    "print(\"\\nüì® –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –ø–æ—Ç–æ–∫—É Kafka...\")\n",
    "kafka_stream = (spark.readStream\n",
    "                .format(\"kafka\")\n",
    "                .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "                .option(\"subscribe\", \"user-events\")\n",
    "                .option(\"startingOffsets\", \"latest\")  # –¢–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è\n",
    "                .option(\"failOnDataLoss\", \"false\")\n",
    "                .option(\"maxOffsetsPerTrigger\", 10)   # –ö–æ–Ω—Ç—Ä–æ–ª—å —Å–∫–æ—Ä–æ—Å—Ç–∏\n",
    "                .load())\n",
    "\n",
    "# 4. –ü–∞—Ä—Å–∏–º JSON –∫–∞–∫ –≤ –≤–∞—à–µ–º –∫–æ–¥–µ\n",
    "parsed_stream = (kafka_stream\n",
    "                 .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"))\n",
    "                 .select(\"data.*\")\n",
    "                 .withColumn(\"date\", col(\"date\").cast(DateType()))\n",
    "                 .withColumn(\"event_date\", col(\"event_date\").cast(TimestampType())))\n",
    "\n",
    "print(\"‚úÖ –°—Ç—Ä–∏–º —Å–æ–∑–¥–∞–Ω, —Å—Ö–µ–º–∞:\")\n",
    "parsed_stream.printSchema()\n",
    "\n",
    "# 5. –û–±–æ–≥–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ (—Ç–æ—á–Ω–æ –∫–∞–∫ –≤ –≤–∞—à–µ–º –∫–æ–¥–µ)\n",
    "enriched_stream = (parsed_stream\n",
    "                   .join(events_df, on=\"event_id\", how=\"left\")\n",
    "                   .join(groups_df, on=\"group_id\", how=\"left\"))\n",
    "\n",
    "# 6. –ì–æ—Ç–æ–≤–∏–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç (–∫–∞–∫ —É –≤–∞—Å)\n",
    "final_stream = enriched_stream.select(\n",
    "    \"id\", \"date\", \"event_date\", \n",
    "    \"event_id\", \n",
    "    \"event_name\",  \n",
    "    \"username\", \n",
    "    \"group_id\",\n",
    "    \"group_name\",\n",
    "    \"value\"\n",
    ")\n",
    "\n",
    "print(\"\\nüìä –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ö–µ–º–∞ –ø–æ—Ç–æ–∫–∞:\")\n",
    "final_stream.printSchema()\n",
    "\n",
    "# 7. –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø–∏—Å–∏ –∫–∞–∂–¥–æ–≥–æ –º–∏–∫—Ä–æ–±–∞—Ç—á–∞ –≤ PostgreSQL\n",
    "def write_to_postgres(batch_df, batch_id):\n",
    "    \"\"\"–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –∫–∞–∂–¥—ã–π –º–∏–∫—Ä–æ–±–∞—Ç—á –≤ PostgreSQL\"\"\"\n",
    "    try:\n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—á–Ω–æ —Ç–∞–∫–æ–π –∂–µ –∫–æ–¥ –∑–∞–ø–∏—Å–∏, –∫–∞–∫ –≤ –≤–∞—à–µ–º —Ä–∞–±–æ—á–µ–º –ø—Ä–∏–º–µ—Ä–µ\n",
    "        batch_df.write \\\n",
    "            .mode(\"append\") \\\n",
    "            .jdbc(url=\"jdbc:postgresql://postgres-db:5432/learn_base\",\n",
    "                  table=\"kafka_farm.user_events\",\n",
    "                  properties={\n",
    "                      \"user\": \"airflow\",\n",
    "                      \"password\": \"airflow\",\n",
    "                      \"driver\": \"org.postgresql.Driver\"\n",
    "                  })\n",
    "        print(f\"‚úÖ Batch {batch_id}: –∑–∞–ø–∏—Å–∞–Ω–æ {batch_df.count()} –∑–∞–ø–∏—Å–µ–π\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Batch {batch_id}: –æ—à–∏–±–∫–∞ - {e}\")\n",
    "\n",
    "# 8. –ó–ê–ü–£–°–ö–ê–ï–ú –°–¢–†–ò–ú–ò–ù–ì\n",
    "print(\"\\nüöÄ –ó–ê–ü–£–°–ö –°–¢–†–ò–ú–ò–ù–ì–ê –í POSTGRESQL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "query = (final_stream.writeStream\n",
    "         .foreachBatch(write_to_postgres)\n",
    "         .outputMode(\"append\")\n",
    "         .trigger(processingTime=\"10 seconds\")  # –ú–∏–∫—Ä–æ–±–∞—Ç—á–∏ –∫–∞–∂–¥—ã–µ 10 —Å–µ–∫—É–Ω–¥\n",
    "         .option(\"checkpointLocation\", \"/tmp/spark-checkpoints/kafka-to-postgres\")\n",
    "         .start())\n",
    "\n",
    "print(\"‚úÖ –°—Ç—Ä–∏–º–∏–Ω–≥ –∑–∞–ø—É—â–µ–Ω –∏ –ø–∏—à–µ—Ç –≤ PostgreSQL!\")\n",
    "print(\"üìù –î–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç—É–ø–∞—é—Ç –≤ —Ç–∞–±–ª–∏—Ü—É: kafka_farm.user_events\")\n",
    "print(\"\\nüîç –ö–û–ú–ê–ù–î–´ –£–ü–†–ê–í–õ–ï–ù–ò–Ø:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å: query.isActive\")\n",
    "print(\"–ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å: query.recentProgress\")\n",
    "print(\"–û–°–¢–ê–ù–û–í–ò–¢–¨: query.stop()\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# –°—Ä–∞–∑—É –ø–æ–∫–∞–∂–µ–º —Å—Ç–∞—Ç—É—Å\n",
    "print(f\"\\n–°—Ç–∞—Ç—É—Å: {'–ê–ö–¢–ò–í–ï–ù' if query.isActive else '–û–°–¢–ê–ù–û–í–õ–ï–ù'}\")\n",
    "print(f\"ID —Å—Ç—Ä–∏–º–∞: {query.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9d80d8-0657-4b6f-b7e1-1ea52d6900aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°—Ç—Ä–∏–º–∏–Ω–≥ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\n"
     ]
    }
   ],
   "source": [
    "query.stop()\n",
    "print(\"–°—Ç—Ä–∏–º–∏–Ω–≥ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5683d81c-35a1-409c-9c69-cda94cc0a320",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab6ff99-3fed-42f8-a740-84376c7af78a",
   "metadata": {},
   "source": [
    "# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3fe353-2574-4dc2-bc8b-ee92874f471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_status(query):\n",
    "    \"\"\"–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞\"\"\"\n",
    "    if query.recentProgress:\n",
    "        last = query.recentProgress[-1]\n",
    "        total = sum(p.get('numInputRows', 0) for p in query.recentProgress)\n",
    "        print(f\"‚úÖ –°—Ç—Ä–∏–º –∞–∫—Ç–∏–≤–µ–Ω | –ë–∞—Ç—á #{last.get('batchId', 0)}: {last.get('numInputRows', 0)} –∑–∞–ø–∏—Å–µ–π | \"\n",
    "              f\"–í—Å–µ–≥–æ: {total} –∑–∞–ø–∏—Å–µ–π –∑–∞ {len(query.recentProgress)} –±–∞—Ç—á–µ–π | \"\n",
    "              f\"–°–∫–æ—Ä–æ—Å—Ç—å: {last.get('processedRowsPerSecond', 0):.1f} rows/—Å–µ–∫\")\n",
    "    else:\n",
    "        print(\"‚úÖ –°—Ç—Ä–∏–º –∞–∫—Ç–∏–≤–µ–Ω | –û–∂–∏–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ\n",
    "quick_status(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2ead30-1c30-443f-a1b9-a7bb7b666f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stream_status(query):\n",
    "    \"\"\"–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç–∞—Ç—É—Å —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ –∏ –≤—ã–≤–æ–¥–∏—Ç –¥–æ—Å—Ç—É–ø–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìä –ú–û–ù–ò–¢–û–†–ò–ù–ì –°–¢–†–ò–ú–ò–ù–ì–ê\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # –û—Å–Ω–æ–≤–Ω–æ–π —Å—Ç–∞—Ç—É—Å\n",
    "    print(f\"\\n‚úÖ –ê–∫—Ç–∏–≤–µ–Ω: {query.isActive}\")\n",
    "    print(f\"üÜî ID —Å—Ç—Ä–∏–º–∞: {query.id}\")\n",
    "    print(f\"üÜî Run ID: {query.runId}\")\n",
    "    \n",
    "    # –°—Ç–∞—Ç—É—Å —Å–æ–æ–±—â–µ–Ω–∏–µ\n",
    "    status = query.status\n",
    "    print(f\"\\nüìã –°—Ç–∞—Ç—É—Å: {status['message']}\")\n",
    "    \n",
    "    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ—Å–ª–µ–¥–Ω–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–µ\n",
    "    if query.recentProgress:\n",
    "        last = query.recentProgress[-1]\n",
    "        print(f\"\\nüìà –ü–û–°–õ–ï–î–ù–ò–ô –ë–ê–¢–ß (ID: {last.get('batchId', 'N/A')}):\")\n",
    "        print(f\"   ‚Ä¢ –í—Ä–µ–º—è: {last.get('timestamp', 'N/A')}\")\n",
    "        print(f\"   ‚Ä¢ –ó–∞–ø–∏—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {last.get('numInputRows', 0)}\")\n",
    "        print(f\"   ‚Ä¢ –°–∫–æ—Ä–æ—Å—Ç—å –≤—Ö–æ–¥–∞: {last.get('inputRowsPerSecond', 0):.2f} rows/—Å–µ–∫\")\n",
    "        print(f\"   ‚Ä¢ –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏: {last.get('processedRowsPerSecond', 0):.2f} rows/—Å–µ–∫\")\n",
    "        \n",
    "        # –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —ç—Ç–∞–ø–æ–≤ (–∏–∑ durationMs)\n",
    "        if 'durationMs' in last:\n",
    "            duration = last['durationMs']\n",
    "            total_time = duration.get('triggerExecution', 0)\n",
    "            print(f\"\\n   ‚è±Ô∏è  –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —ç—Ç–∞–ø–æ–≤ (–º—Å):\")\n",
    "            print(f\"      ‚Ä¢ –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time} –º—Å\")\n",
    "            print(f\"      ‚Ä¢ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –±–∞—Ç—á–∞: {duration.get('addBatch', 0)} –º—Å\")\n",
    "            print(f\"      ‚Ä¢ –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ñ—Ñ—Å–µ—Ç–æ–≤: {duration.get('latestOffset', 0)} –º—Å\")\n",
    "            print(f\"      ‚Ä¢ –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞: {duration.get('queryPlanning', 0)} –º—Å\")\n",
    "            print(f\"      ‚Ä¢ –ö–æ–º–º–∏—Ç –æ—Ñ—Ñ—Å–µ—Ç–æ–≤: {duration.get('commitOffsets', 0)} –º—Å\")\n",
    "        \n",
    "        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö (–ø–∞—Ä—Ç–∏—Ü–∏–∏ Kafka)\n",
    "        if 'sources' in last and last['sources']:\n",
    "            source = last['sources'][0]\n",
    "            print(f\"\\n   üìç –ò—Å—Ç–æ—á–Ω–∏–∫ Kafka:\")\n",
    "            print(f\"      ‚Ä¢ –¢–æ–ø–∏–∫: {source.get('description', 'N/A')}\")\n",
    "            if 'metrics' in source:\n",
    "                metrics = source['metrics']\n",
    "                print(f\"      ‚Ä¢ –û—Ç—Å—Ç–∞–≤–∞–Ω–∏–µ (max): {metrics.get('maxOffsetsBehindLatest', 0)}\")\n",
    "                print(f\"      ‚Ä¢ –û—Ç—Å—Ç–∞–≤–∞–Ω–∏–µ (avg): {metrics.get('avgOffsetsBehindLatest', 0)}\")\n",
    "        \n",
    "        # –°—É–º–º–∞—Ä–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞ –≤—Å–µ –≤—Ä–µ–º—è\n",
    "        total_rows = sum(p.get('numInputRows', 0) for p in query.recentProgress)\n",
    "        total_batches = len(query.recentProgress)\n",
    "        \n",
    "        print(f\"\\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ó–ê –í–°–ï –í–†–ï–ú–Ø:\")\n",
    "        print(f\"   ‚Ä¢ –í—Å–µ–≥–æ –±–∞—Ç—á–µ–π: {total_batches}\")\n",
    "        print(f\"   ‚Ä¢ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {total_rows}\")\n",
    "        \n",
    "        if total_batches > 0:\n",
    "            avg_rows = total_rows / total_batches\n",
    "            print(f\"   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –∑–∞–ø–∏—Å–µ–π/–±–∞—Ç—á: {avg_rows:.1f}\")\n",
    "    else:\n",
    "        print(\"\\n‚è≥ –ù–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ (–≤–æ–∑–º–æ–∂–Ω–æ, —Å—Ç—Ä–∏–º —Ç–æ–ª—å–∫–æ —á—Ç–æ –∑–∞–ø—É—â–µ–Ω)\")\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–π\n",
    "    if query.exception():\n",
    "        print(f\"\\n‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ: {query.exception()}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é\n",
    "check_stream_status(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547ef6cd-fc3e-441a-a0b4-10eed2338464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca4871f-0bf3-41f9-adf8-f5bdd199d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í Jupyter –≤—ã–ø–æ–ª–Ω–∏—Ç–µ:\n",
    "print(f\"–°—Ç—Ä–∏–º –∞–∫—Ç–∏–≤–µ–Ω: {query.isActive}\")\n",
    "print(f\"ID —Å—Ç—Ä–∏–º–∞: {query.id}\")\n",
    "\n",
    "if query.recentProgress:\n",
    "    print(f\"–ü–æ—Å–ª–µ–¥–Ω–∏–π –±–∞—Ç—á: {query.recentProgress[-1]['batchId']}\")\n",
    "    print(f\"–í—Å–µ–≥–æ –±–∞—Ç—á–µ–π: {len(query.recentProgress)}\")\n",
    "else:\n",
    "    print(\"–ù–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ - —Å—Ç—Ä–∏–º –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c55ed9-bbec-4938-9fcf-96dc11dd6fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2933cb-625a-4977-8cbe-c29b1c4aaa7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a33c3d7-28a5-4260-a8cd-8433997386cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c182498-738b-482b-a8f0-eb64656e3f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed578b5a-16c5-47b2-b242-d5d9a2550c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9ef0fe-d013-4ccb-b30c-7b2f3de83a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
