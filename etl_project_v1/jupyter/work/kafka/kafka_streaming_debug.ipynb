{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ac668b-bb0b-4fe2-979d-dbfd79161f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4d8023-1c5c-45eb-96fd-46a85ce6e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Отладка Kafka Streaming в Jupyter\n",
    "# Этот notebook позволяет поэкспериментировать с чтением из Kafka и обогащением данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76068dd5-44c1-428e-b173-cce0edae7cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2. Подготовка данных для event_list\n",
    "# event_data = [\n",
    "#     (1, \"Покупка крипты\"),\n",
    "#     (2, \"Вложение в акции\"),\n",
    "#     (3, \"Оплата товара\"),\n",
    "#     (4, \"Донатная помойка\"),\n",
    "#     (5, \"Пожертвование сектам\")\n",
    "# ]\n",
    "# event_schema = StructType([\n",
    "#     StructField(\"event_id\", IntegerType(), True),\n",
    "#     StructField(\"event_name\", StringType(), True)\n",
    "# ])\n",
    "# df_events = spark.createDataFrame(event_data, schema=event_schema)\n",
    "\n",
    "# # 3. Подготовка данных для group_list\n",
    "# group_data = [\n",
    "#     (1, \"Новые\"),\n",
    "#     (2, \"Иностранные\"),\n",
    "#     (3, \"Рептилойды\"),\n",
    "#     (4, \"Gold\"),\n",
    "#     (5, \"Platinum\")\n",
    "# ]\n",
    "# group_schema = StructType([\n",
    "#     StructField(\"group_id\", IntegerType(), True),\n",
    "#     StructField(\"group_name\", StringType(), True)\n",
    "# ])\n",
    "# df_groups = spark.createDataFrame(group_data, schema=group_schema)\n",
    "\n",
    "# # 4. Запись данных в MinIO (Bucket: library)\n",
    "# # Примечание: Spark пишет в директорию. Расширение .parquet в пути создаст папку с таким именем.\n",
    "# # Режим 'overwrite' перезапишет данные, если они уже существуют.\n",
    "\n",
    "# bucket_path = \"s3a://library/kafka-enrich\"\n",
    "\n",
    "# print(\"Запись таблицы event_list...\")\n",
    "# df_events.write.mode(\"overwrite\").parquet(f\"{bucket_path}/event_list.parquet\")\n",
    "\n",
    "# print(\"Запись таблицы group_list...\")\n",
    "# df_groups.write.mode(\"overwrite\").parquet(f\"{bucket_path}/group_list.parquet\")\n",
    "\n",
    "# print(\"Готово! Таблицы созданы в MinIO.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d9414c-ab51-45dd-80fb-654fad9c6cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, DateType\n",
    "import pandas as pd\n",
    "\n",
    "drivers = [\n",
    "    \"/opt/spark/external-jars/hadoop-aws-3.3.4.jar\",\n",
    "    \"/opt/spark/external-jars/aws-java-sdk-bundle-1.12.262.jar\",\n",
    "    \"/opt/spark/external-jars/wildfly-openssl-1.0.7.Final.jar\",\n",
    "    \"/opt/spark/external-jars/postgresql-42.6.0.jar\",\n",
    "    \"/opt/spark/external-jars/spark-sql-kafka-0-10_2.12-3.5.0.jar\",\n",
    "    \"/opt/spark/external-jars/kafka-clients-3.2.0.jar\",\n",
    "    \"/opt/spark/external-jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar\",\n",
    "    \"/opt/spark/external-jars/commons-pool2-2.11.1.jar\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"KafkaDebug\")\n",
    "         .master(\"spark://spark-master:7077\")\n",
    "         .config(\"spark.sql.repl.eagerEval.enabled\", True)  # Красивый вывод в notebook\n",
    "         .config(\"spark.sql.shuffle.partitions\", 5)         \n",
    "         .config(\"spark.jars\", \",\".join(drivers))\n",
    "         .getOrCreate()\n",
    "        )\n",
    "\n",
    "# 1. ПРОВЕРКА: Чтение справочников из MinIO\n",
    "print(\"Читаем справочник событий:\")\n",
    "events_df = (spark.read\n",
    "                  .format(\"parquet\")\n",
    "                  .load(\"s3a://library/kafka-enrich/event_list.parquet\"))\n",
    "events_df.show()\n",
    "\n",
    "print(\"\\nЧитаем справочник групп:\")\n",
    "groups_df = (spark.read\n",
    "                  .format(\"parquet\")\n",
    "                  .load(\"s3a://library/kafka-enrich/group_list.parquet\"))\n",
    "groups_df.show()\n",
    "\n",
    "# 2. ПРОВЕРКА: Чтение из Kafka (батч-режим для отладки)\n",
    "# Читаем последние 100 сообщений из Kafka\n",
    "kafka_df = (spark.read\n",
    "            .format(\"kafka\")\n",
    "            .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "            .option(\"subscribe\", \"user-events\")\n",
    "            .option(\"startingOffsets\", \"earliest\")  # С начала\n",
    "            .option(\"endingOffsets\", \"latest\")      # До конца\n",
    "            .load()\n",
    "            .limit(100))  # Ограничим для отладки\n",
    "\n",
    "print(f\"Получено сообщений из Kafka: {kafka_df.count()}\")\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "# Посмотрим схему полученных данных\n",
    "print(\"Схема Kafka DataFrame:\")\n",
    "kafka_df.printSchema()\n",
    "\n",
    "# Посмотрим сами сообщения\n",
    "print(\"\\nПримеры сообщений (первые 10):\")\n",
    "kafka_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"partition\", \"offset\") \\\n",
    "        .show(10, truncate=False)\n",
    "\n",
    "# 3. ПРОВЕРКА: Парсинг JSON\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"event_date\", StringType(), True),\n",
    "    StructField(\"event_id\", IntegerType(), True),\n",
    "    StructField(\"username\", StringType(), True),\n",
    "    StructField(\"group_id\", IntegerType(), True),\n",
    "    StructField(\"value\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Парсим и конвертируем типы\n",
    "parsed_df = (kafka_df\n",
    "             .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"))\n",
    "             .select(\"data.*\")\n",
    "             .withColumn(\"date\", col(\"date\").cast(DateType()))\n",
    "             .withColumn(\"event_date\", col(\"event_date\").cast(TimestampType())))\n",
    "\n",
    "print(\"Распарсенные данные:\")\n",
    "parsed_df.show(10, truncate=False)\n",
    "\n",
    "# 4. ПРОВЕРКА: Обогащение данных\n",
    "events_for_join = events_df.withColumnRenamed(\"id\", \"event_id\").withColumnRenamed(\"event\", \"event_name\")\n",
    "groups_for_join = groups_df.withColumnRenamed(\"id\", \"group_id\").withColumnRenamed(\"group\", \"group_name\")\n",
    "\n",
    "# Делаем JOIN\n",
    "enriched_df = (parsed_df\n",
    "               .join(events_df, on=\"event_id\", how=\"left\")\n",
    "               .join(groups_df, on=\"group_id\", how=\"left\")\n",
    "              )\n",
    "\n",
    "print(\"Обогащенные данные:\")\n",
    "enriched_df.show(10, truncate=False)\n",
    "\n",
    "# 5. ПРОВЕРКА: Подготовка к записи в Postgres\n",
    "final_df = enriched_df.select(\n",
    "    \"id\", \"date\", \"event_date\", \"event_id\", \"event_name\", \"username\", \"group_id\", \"group_name\", \"value\"\n",
    ")\n",
    "\n",
    "print(\"Финальный датасет для записи:\")\n",
    "final_df.show(10, truncate=False)\n",
    "\n",
    "# 6. ЗАПИСЬ В POSTGRES \n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "try:\n",
    "    final_df.write \\\n",
    "        .mode(\"append\") \\\n",
    "        .jdbc(url=\"jdbc:postgresql://postgres-db:5432/learn_base\",\n",
    "              table=\"kafka_farm.user_events\",\n",
    "              properties={\n",
    "                  \"user\": \"airflow\",\n",
    "                  \"password\": \"airflow\",\n",
    "                  \"driver\": \"org.postgresql.Driver\"\n",
    "              })\n",
    "    print(\"Данные успешно записаны в PostgreSQL!\")\n",
    "except AnalysisException as e:\n",
    "    print(f\"Ошибка записи: {e}\")\n",
    "    print(\"Возможно, таблица еще не создана или нет данных\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff85a5ba-43bc-4f3a-81c5-5b3371080dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Закрываем сессию\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dab32b-841c-4268-ad70-330b22f1d54d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
