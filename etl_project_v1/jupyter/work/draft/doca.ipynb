{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc45316-40bc-41d2-aa9b-2444f875881b",
   "metadata": {},
   "source": [
    "# Сессия с полной производительностью\n",
    "\n",
    "```python\n",
    "# Создание SparkSession с настройками для максимальной производительности\n",
    "spark = (SparkSession.builder\n",
    "         # Имя приложения для идентификации в Spark UI и логах\n",
    "         .appName(\"Full Power\")\n",
    "         \n",
    "         # Подключение к Spark кластеру\n",
    "         # spark-master:7077 - главный узел кластера, который управляет распределенными вычислениями\n",
    "         .master(\"spark://spark-master:7077\")\n",
    "         \n",
    "         # КОНФИГУРАЦИЯ ИСПОЛНИТЕЛЕЙ (EXECUTORS)\n",
    "         # Executor - процесс, выполняющий задачи на рабочих узлах кластера\n",
    "         \n",
    "         # Количество исполнителей на рабочем узле\n",
    "         # 2 исполнителя будут запущены на каждом worker node\n",
    "         # Это позволяет лучше изолировать задачи и управлять памятью\n",
    "         .config(\"spark.executor.instances\", \"2\")  \n",
    "         \n",
    "         # Количество CPU ядер на каждый исполнитель\n",
    "         # 10 ядер позволяют выполнять до 10 параллельных задач в каждом executor\n",
    "         # Оптимально для узлов с большим количеством процессорных ядер\n",
    "         .config(\"spark.executor.cores\", \"10\")     \n",
    "         \n",
    "         # Объем оперативной памяти на каждый исполнитель\n",
    "         # 16GB RAM для каждого executor процесса\n",
    "         # Включает память для выполнения задач и хранения данных в памяти\n",
    "         # Структура памяти executor:\n",
    "         # - Execution Memory (шаффлинг, joins, агрегации)\n",
    "         # - Storage Memory (кэшированные DataFrame/RDD)\n",
    "         # - User Memory (пользовательские структуры данных)\n",
    "         # - Reserved Memory (системные нужды Spark ~300MB)\n",
    "         .config(\"spark.executor.memory\", \"16g\")   \n",
    "         \n",
    "         # КОНФИГУРАЦИЯ ДРАЙВЕРА\n",
    "         # Driver - главный процесс, координирующий выполнение приложения\n",
    "         \n",
    "         # Память драйвера - 4GB для:\n",
    "         # - Хранения метаданных приложения\n",
    "         # - Сбора результатов (collect())\n",
    "         # - Broadcast переменных\n",
    "         # - Управления задачами и планирования\n",
    "         .config(\"spark.driver.memory\", \"4g\")      \n",
    "         \n",
    "         # Создание или получение существующей сессии\n",
    "         .getOrCreate()\n",
    "        )\n",
    "```\n",
    "\n",
    "## Детальное объяснение архитектуры:\n",
    "\n",
    "### **Распределение ресурсов:**\n",
    "```\n",
    "Worker Node (физическая/виртуальная машина)\n",
    "├── Executor 1 (16GB RAM, 10 cores)\n",
    "│   ├── Task Slot 1\n",
    "│   ├── Task Slot 2\n",
    "│   └── ... Task Slot 10\n",
    "└── Executor 2 (16GB RAM, 10 cores)\n",
    "    ├── Task Slot 1\n",
    "    ├── Task Slot 2\n",
    "    └── ... Task Slot 10\n",
    "```\n",
    "\n",
    "### **Производительность и масштабирование:**\n",
    "\n",
    "1. **Параллелизм задач:**\n",
    "   - Всего слотов для задач: `2 executors × 10 cores = 20 параллельных задач`\n",
    "   - Это позволяет обрабатывать до 20 партиций данных одновременно\n",
    "\n",
    "2. **Распределение памяти:**\n",
    "   ```python\n",
    "   # Примерная структура памяти executor (16GB):\n",
    "   - Spark Memory: ~60% (9.6GB)\n",
    "     * Storage Memory: кэшированные данные\n",
    "     * Execution Memory: операции шаффлинга\n",
    "   - User Memory: ~25% (4GB) - пользовательские структуры\n",
    "   - Reserved: ~5% (0.8GB) - системные нужды\n",
    "   ```\n",
    "\n",
    "3. **Рекомендации для продакшена:**\n",
    "   ```python\n",
    "   # Дополнительные оптимизации для \"Full Power\":\n",
    "   .config(\"spark.sql.adaptive.enabled\", \"true\")          # Адаптивное выполнение запросов\n",
    "   .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") # Объединение партиций\n",
    "   .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") # Быстрая сериализация\n",
    "   .config(\"spark.sql.shuffle.partitions\", \"200\")         # Партиции для шаффлинга\n",
    "   .config(\"spark.default.parallelism\", \"40\")             # Параллелизм по умолчанию\n",
    "   ```\n",
    "\n",
    "### **Типичные сценарии использования:**\n",
    "- **Обработка больших данных** (сотни GB - TB)\n",
    "- **Сложные ETL-пайплайны** с множеством преобразований\n",
    "- **Машинное обучение на больших датасетах**\n",
    "- **Аналитика в реальном времени** с высокими требованиями к производительности\n",
    "\n",
    "Такая конфигурация подходит для мощных продакшен-кластеров с значительными вычислительными ресурсами."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc209d07-5671-40d0-8c57-d152bf608bf4",
   "metadata": {},
   "source": [
    "Отличная конфигурация! С вашим железом и текущим setup вы можете обрабатывать значительные объемы данных. Давайте разберем детально:\n",
    "\n",
    "## **Текущие ресурсы кластера:**\n",
    "\n",
    "### Вычислительная мощность:\n",
    "```\n",
    "Всего в кластере:\n",
    "- 2 worker × 10 cores = 20 CPU ядер\n",
    "- 2 worker × 18GB RAM = 36GB оперативной памяти\n",
    "```\n",
    "\n",
    "### Память для обработки данных:\n",
    "```python\n",
    "# Доступная память для данных (приблизительно):\n",
    "Общая RAM: 36GB\n",
    "- Системные нужды Spark: ~2GB\n",
    "- Резерв: ~4GB\n",
    "= Доступно для данных: ~30GB\n",
    "```\n",
    "\n",
    "## **Объемы данных для обработки:**\n",
    "\n",
    "### **1. Оптимальные объемы (комфортная работа):**\n",
    "- **В памяти:** 15-25 GB данных\n",
    "- **На диске (с шаффлингом):** 50-100 GB данных\n",
    "- **Ежедневная обработка:** 200-500 GB (с разбивкой на батчи)\n",
    "\n",
    "### **2. Максимальные объемы (с оптимизацией):**\n",
    "- **Единоразово в памяти:** до 30 GB\n",
    "- **С промежуточной записью на диск:** 200-300 GB\n",
    "- **Ежедневно:** 1-2 TB (с правильным партиционированием)\n",
    "\n",
    "## **Рекомендации по оптимизации:**\n",
    "\n",
    "### Конфигурация Spark для вашего железа:\n",
    "```python\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"Optimized for i5-14600K\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.executor.instances\", \"2\")  \n",
    "    .config(\"spark.executor.cores\", \"8\")      # Оставляем 2 ядра на систему\n",
    "    .config(\"spark.executor.memory\", \"14g\")   # Оставляем 4GB на систему\n",
    "    .config(\"spark.driver.memory\", \"8g\")      # Увеличиваем для больших collect()\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"128MB\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\")  # 8 cores × 2 workers × 6 = 96\n",
    "    .config(\"spark.default.parallelism\", \"96\")\n",
    "    .config(\"spark.memory.fraction\", \"0.8\")\n",
    "    .config(\"spark.memory.storageFraction\", \"0.3\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "```\n",
    "\n",
    "## **Типичные сценарии обработки:**\n",
    "\n",
    "### **1. Аналитические запросы:**\n",
    "```python\n",
    "# Мгновенная обработка: 5-10 GB\n",
    "# Запросы до 1 минуты: 10-20 GB  \n",
    "# Длительные ETL: 50-100 GB за запуск\n",
    "```\n",
    "\n",
    "### **2. Машинное обучение:**\n",
    "```python\n",
    "# Обучение моделей:\n",
    "# - До 10M строк × 100 фич: комфортно\n",
    "# - До 50M строк × 50 фич: с оптимизацией\n",
    "# - Feature engineering: 20-30 GB наборов данных\n",
    "```\n",
    "\n",
    "### **3. Стриминг данных:**\n",
    "```python\n",
    "# Потоковая обработка:\n",
    "# - До 50,000 событий/секунду\n",
    "# - Микробатчи по 10-30 секунд\n",
    "# - Окна агрегации: 1-60 минут\n",
    "```\n",
    "\n",
    "## **Мониторинг и ограничения:**\n",
    "\n",
    "### Признаки нехватки ресурсов:\n",
    "```python\n",
    "# Тревожные сигналы:\n",
    "- Frequent GC pauses (> 10% времени)\n",
    "- Spill на диск (Disk spill bytes > 0)\n",
    "- OOM errors\n",
    "- Tasks taking > 5-10 minutes\n",
    "```\n",
    "\n",
    "### Практические примеры объемов:\n",
    "```\n",
    "✅ Легко:     Анализ 10GB CSV (~100M строк)\n",
    "✅ Комфортно: ETL 50GB данных ежедневно  \n",
    "⚠️ Возможно:  Обработка 150GB с оптимизацией\n",
    "❌ Сложно:    Единовременная обработка >200GB\n",
    "```\n",
    "\n",
    "## **Рекомендации для вашего setup:**\n",
    "\n",
    "1. **Используйте партиционирование** для больших datasets\n",
    "2. **Кэшируйте часто используемые DataFrames**\n",
    "3. **Настройте правильные типы данных** (меньше памяти)\n",
    "4. **Используйте формат Parquet/ORC** вместо CSV/JSON\n",
    "5. **Мониторьте Spark UI** на портах 8085-8087\n",
    "\n",
    "Ваша система отлично подходит для серьезных data engineering задач и может обрабатывать гигабайты данных ежедневно!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d5b92-e166-4c9b-b701-4f95c5d58411",
   "metadata": {},
   "source": [
    "# Сессия с MinIO\n",
    "\n",
    "```python\n",
    "# Создание SparkSession - основной точки входа для работы с Spark\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"MinIO Test\")\n",
    "         \n",
    "         # Задаем URL мастер-ноды Spark кластера\n",
    "         # spark://spark-master:7077 - означает, что Spark работает в кластерном режиме\n",
    "         # spark-master - имя хоста мастер-ноды (обычно задается в Docker Compose или Kubernetes)\n",
    "         # 7077 - стандартный порт для подключения драйверов к кластеру\n",
    "         .master(\"spark://spark-master:7077\") \n",
    "         \n",
    "         # Указываем необходимые JAR-пакеты для работы с S3-совместимыми хранилищами\n",
    "         # hadoop-aws:3.3.4 - библиотека Hadoop для работы с AWS S3\n",
    "         # aws-java-sdk-bundle:1.12.262 - AWS Java SDK для клиентских операций\n",
    "         # Spark автоматически скачает эти зависимости при запуске\n",
    "         .config(\"spark.jars.packages\", \n",
    "                 \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262\")\n",
    "         \n",
    "         .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "         .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "         \n",
    "         # Указываем endpoint MinIO сервера\n",
    "         # http://minio:9000 - MinIO работает по HTTP на порту 9000\n",
    "         # 'minio' - имя сервиса в Docker-сети\n",
    "         .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "         \n",
    "         # Включаем path-style доступ к бакетам (требуется для MinIO)\n",
    "         # В отличие от virtual-hosted style, где бакет в домене: bucket.host.com\n",
    "         # Path-style: host.com/bucket/path/to/file\n",
    "         .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \n",
    "         \n",
    "         # Указываем реализацию файловой системы для S3\n",
    "         # S3AFileSystem - Hadoop файловая система для работы с S3-совместимыми хранилищами\n",
    "         .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \n",
    "         \n",
    "         # Отключаем SSL/TLS шифрование соединения\n",
    "         # Так как мы используем HTTP (не HTTPS) для подключения к MinIO\n",
    "         .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "         .getOrCreate()\n",
    "        )\n",
    "```\n",
    "\n",
    "## Ключевые моменты конфигурации:\n",
    "\n",
    "### 1. **Архитектура подключения**\n",
    "- Spark драйвер подключается к кластеру через мастер-ноду\n",
    "- Executor'ы в кластере получат те же конфигурации S3/MinIO\n",
    "\n",
    "### 2. **Работа с MinIO**\n",
    "- MinIO эмулирует S3 API, поэтому используем S3A connector\n",
    "- Path-style access обязателен для MinIO\n",
    "- Отключен SSL т.к. используется HTTP\n",
    "\n",
    "### 3. **Зависимости**\n",
    "- `hadoop-aws` предоставляет S3A файловую систему  \n",
    "- `aws-java-sdk` обеспечивает низкоуровневые S3 операции\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7c9051-f7db-425c-a976-23f7e3f2071c",
   "metadata": {},
   "source": [
    "# Запись CSV\n",
    "\n",
    "Подробно разберем каждую опцию записи CSV в этом формате:\n",
    "\n",
    "### Базовый синтаксис\n",
    "```python\n",
    "(df.write.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"quote\", \"\\\"\")\n",
    "    .option(\"escape\", \"\\\"\")\n",
    "    .option(\"encoding\", \"UTF-8\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"s3a://my-bucket/data/csv-output\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **1. `.format(\"csv\")`**\n",
    "**Назначение**: Указывает формат записи\n",
    "```python\n",
    ".format(\"csv\")  # Запись в CSV формате\n",
    "```\n",
    "**Альтернативы**: `\"parquet\"`, `\"json\"`, `\"orc\"`, `\"avro\"`\n",
    "\n",
    "---\n",
    "\n",
    "### **2. `.option(\"header\", \"true\")`**\n",
    "**Назначение**: Включает запись заголовков столбцов\n",
    "```python\n",
    ".option(\"header\", \"true\")   # ✅ Записать названия колонок\n",
    ".option(\"header\", \"false\")  # ❌ Без заголовков (по умолчанию)\n",
    "```\n",
    "\n",
    "**Пример данных:**\n",
    "```\n",
    "id,name,age\n",
    "1,John,25\n",
    "2,Jane,30\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. `.option(\"delimiter\", \",\")`**\n",
    "**Назначение**: Задает разделитель полей\n",
    "```python\n",
    ".option(\"delimiter\", \",\")    # Стандартный CSV\n",
    ".option(\"delimiter\", \";\")    # Европейский CSV\n",
    ".option(\"delimiter\", \"|\")    # Pipe-separated\n",
    ".option(\"delimiter\", \"\\t\")   # TSV (табуляция)\n",
    "```\n",
    "\n",
    "**Результат с разными разделителями:**\n",
    "```csv\n",
    "# delimiter = \",\"\n",
    "1,John,25\n",
    "\n",
    "# delimiter = \"|\"\n",
    "1|John|25\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. `.option(\"quote\", \"\\\"\")`**\n",
    "**Назначение**: Символ для обрамления полей\n",
    "```python\n",
    ".option(\"quote\", \"\\\"\")    # Двойные кавычки (стандарт)\n",
    ".option(\"quote\", \"'\")     # Одинарные кавычки\n",
    ".option(\"quote\", \"\")      # Без quoting (опасно!)\n",
    "```\n",
    "\n",
    "**Когда используется:**\n",
    "```csv\n",
    "# Без quote\n",
    "1,John Doe,25\"  # ❌ Проблема с кавычкой в данных\n",
    "\n",
    "# С quote\n",
    "1,\"John Doe\",\"25\"\"\",Engineer  # ✅ Экранирование работает\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. `.option(\"escape\", \"\\\"\")`**\n",
    "**Назначение**: Символ экранирования специальных символов\n",
    "```python\n",
    ".option(\"escape\", \"\\\"\")    # Экранирование кавычкой\n",
    ".option(\"escape\", \"\\\\\")    # Экранирование обратным слэшем\n",
    "```\n",
    "\n",
    "**Пример экранирования:**\n",
    "```csv\n",
    "# Данные: He said \"Hello\"\n",
    "\"He said \"\"Hello\"\"\"     # escape = \"\\\"\"\n",
    "\"He said \\\"Hello\\\"\"     # escape = \"\\\\\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **6. `.option(\"encoding\", \"UTF-8\")`**\n",
    "**Назначение**: Кодировка файла\n",
    "```python\n",
    ".option(\"encoding\", \"UTF-8\")      # Unicode (рекомендуется)\n",
    ".option(\"encoding\", \"ISO-8859-1\") # Latin-1\n",
    ".option(\"encoding\", \"CP1251\")     # Windows Cyrillic\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **7. `.mode(\"overwrite\")`**\n",
    "**Назначение**: Режим записи при существующем файле\n",
    "```python\n",
    ".mode(\"overwrite\")   # 📝 Перезаписать существующие данные\n",
    ".mode(\"append\")      # ➕ Добавить к существующим\n",
    ".mode(\"ignore\")      # ⏭️ Пропустить если файл существует\n",
    ".mode(\"error\")       # ❌ Ошибка если файл существует (по умолчанию)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Дополнительные важные опции**\n",
    "\n",
    "### **Управление NULL значениями**\n",
    "```python\n",
    ".option(\"nullValue\", \"NULL\")     # Заменяет null на \"NULL\"\n",
    ".option(\"nullValue\", \"\")         # Заменяет null на пустую строку\n",
    ".option(\"nullValue\", \"\\\\N\")      # Как в MySQL\n",
    "```\n",
    "\n",
    "### **Форматы дат и времени**\n",
    "```python\n",
    ".option(\"dateFormat\", \"yyyy-MM-dd\")          # 2024-01-15\n",
    ".option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\")  # 2024-01-15 14:30:00\n",
    "```\n",
    "\n",
    "### **Сжатие**\n",
    "```python\n",
    ".option(\"compression\", \"gzip\")    # .csv.gz\n",
    ".option(\"compression\", \"none\")    # Без сжатия\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Полный пример с всеми опциями**\n",
    "```python\n",
    "(df.write.format(\"csv\")\n",
    "    .option(\"header\", \"true\")              # Заголовки\n",
    "    .option(\"delimiter\", \",\")              # Разделитель\n",
    "    .option(\"quote\", \"\\\"\")                 # Символ quoting\n",
    "    .option(\"escape\", \"\\\"\")                # Символ экранирования\n",
    "    .option(\"encoding\", \"UTF-8\")           # Кодировка\n",
    "    .option(\"nullValue\", \"NULL\")           # Замена null\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\")    # Формат дат\n",
    "    .option(\"compression\", \"gzip\")         # Сжатие\n",
    "    .mode(\"overwrite\")                     # Режим записи\n",
    "    .save(\"s3a://my-bucket/data/output\"))  # Путь назначения\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Результат записи**\n",
    "Для DataFrame:\n",
    "```python\n",
    "+---+----------+---+\n",
    "| id|      name|age|\n",
    "+---+----------+---+\n",
    "|  1|  John Doe| 25|\n",
    "|  2|Jane Smith| 30|\n",
    "|  3|      NULL| 35|\n",
    "+---+----------+---+\n",
    "```\n",
    "\n",
    "Будет записан CSV:\n",
    "```csv\n",
    "id,name,age\n",
    "1,\"John Doe\",25\n",
    "2,\"Jane Smith\",30\n",
    "3,NULL,35\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7294f3d0-3704-43f6-bfad-ea2b3e1f1e5c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f588c05-8b7b-4ca6-a986-d78494aa1cbb",
   "metadata": {},
   "source": [
    "# Запись Parquet\n",
    "\n",
    "\n",
    "```python\n",
    "(df.write.format(\"parquet\")\n",
    "    .option(\"compression\", \"snappy\")\n",
    "    .option(\"parquet.block.size\", 134217728)\n",
    "    .option(\"parquet.page.size\", 1048576)\n",
    "    .option(\"parquet.dictionary.enabled\", \"true\")\n",
    "    .option(\"parquet.bloom.filter.enabled\", \"true\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"s3a://my-bucket/data/parquet-output\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **1. `.format(\"parquet\")`**\n",
    "**Назначение**: Указывает формат Parquet\n",
    "```python\n",
    ".format(\"parquet\")  # Запись в колоночном формате Parquet\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2. `.option(\"compression\", \"snappy\")`**\n",
    "**Назначение**: Алгоритм сжатия данных\n",
    "```python\n",
    ".option(\"compression\", \"snappy\")    # 🚀 Быстрое сжатие (рекомендуется)\n",
    ".option(\"compression\", \"gzip\")      # 📦 Высокое сжатие (медленнее)\n",
    ".option(\"compression\", \"lzo\")       # ⚡ Очень быстрое (меньше сжатие)\n",
    ".option(\"compression\", \"none\")      # 🔓 Без сжатия\n",
    ".option(\"compression\", \"zstd\")      # 🎯 Баланс скорости/сжатия (новое)\n",
    "```\n",
    "\n",
    "**Сравнение:**\n",
    "- **Snappy**: Скорость + разумное сжатие\n",
    "- **Gzip**: Медленнее, но лучшее сжатие\n",
    "- **None**: Для часто изменяемых данных\n",
    "\n",
    "---\n",
    "\n",
    "## **3. `.option(\"parquet.block.size\", 134217728)`**\n",
    "**Назначение**: Размер блока данных в байтах\n",
    "```python\n",
    ".option(\"parquet.block.size\", 134217728)  # 128 MB (по умолчанию)\n",
    ".option(\"parquet.block.size\", 268435456)  # 256 MB (для больших данных)\n",
    ".option(\"parquet.block.size\", 67108864)   # 64 MB (для маленьких файлов)\n",
    "```\n",
    "\n",
    "**Зачем нужно:**\n",
    "- Блок - минимальная единица чтения\n",
    "- Большие блоки → лучшее сжатие\n",
    "- Меньшие блокы → лучше параллелизм\n",
    "\n",
    "---\n",
    "\n",
    "## **4. `.option(\"parquet.page.size\", 1048576)`**\n",
    "**Назначение**: Размер страницы в байтах\n",
    "```python\n",
    ".option(\"parquet.page.size\", 1048576)     # 1 MB (по умолчанию)\n",
    ".option(\"parquet.page.size\", 524288)      # 512 KB\n",
    ".option(\"parquet.page.size\", 2097152)     # 2 MB\n",
    "```\n",
    "\n",
    "**Зачем нужно:**\n",
    "- Страница - минимальная единица доступа\n",
    "- Меньший размер → точечное чтение\n",
    "- Больший размер → лучшее сжатие\n",
    "\n",
    "---\n",
    "\n",
    "## **5. `.option(\"parquet.dictionary.enabled\", \"true\")`**\n",
    "**Назначение**: Включить словарное кодирование\n",
    "```python\n",
    ".option(\"parquet.dictionary.enabled\", \"true\")   # ✅ Включить (рекомендуется)\n",
    ".option(\"parquet.dictionary.enabled\", \"false\")  # ❌ Выключить\n",
    "```\n",
    "\n",
    "**Преимущества:**\n",
    "- Эффективно для повторяющихся значений\n",
    "- Уменьшает размер данных\n",
    "- Особенно полезно для строковых колонок\n",
    "\n",
    "---\n",
    "\n",
    "## **6. `.option(\"parquet.bloom.filter.enabled\", \"true\")`**\n",
    "**Назначение**: Включить Bloom filter для быстрого поиска\n",
    "```python\n",
    ".option(\"parquet.bloom.filter.enabled\", \"true\")    # ✅ Включить\n",
    ".option(\"parquet.bloom.filter.enabled\", \"false\")   # ❌ Выключить\n",
    "```\n",
    "\n",
    "**Зачем нужно:**\n",
    "- Ускоряет предикатный pushdown\n",
    "- Эффективно для поиска по конкретным значениям\n",
    "- Особенно полезно для колонок с высоким кардиналитетом\n",
    "\n",
    "---\n",
    "\n",
    "## **7. `.mode(\"overwrite\")`**\n",
    "**Назначение**: Режим записи\n",
    "```python\n",
    ".mode(\"overwrite\")   # 📝 Перезаписать существующие данные\n",
    ".mode(\"append\")      # ➕ Добавить к существующим\n",
    ".mode(\"ignore\")      # ⏭️ Пропустить если существует\n",
    ".mode(\"error\")       # ❌ Ошибка если существует\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Дополнительные важные опции**\n",
    "\n",
    "### **Настройка производительности**\n",
    "```python\n",
    ".option(\"parquet.writer.version\", \"v2\")           # Версия формата\n",
    ".option(\"parquet.enable.dictionary\", \"true\")      # Словарное кодирование\n",
    ".option(\"parquet.dictionary.page.size\", 1048576)  # Размер словарной страницы\n",
    "```\n",
    "\n",
    "### **Схема данных**\n",
    "```python\n",
    ".option(\"mergeSchema\", \"true\")                    # Объединять схемы при append\n",
    ".option(\"parquet.strings.signed-min-max.enabled\", \"true\")  # Для строковых статистик\n",
    "```\n",
    "\n",
    "### **Партиционирование**\n",
    "```python\n",
    ".partitionBy(\"year\", \"month\")  # 🗂️ Партиционирование по колонкам\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Полный пример с партиционированием**\n",
    "\n",
    "```python\n",
    "(df.write.format(\"parquet\")\n",
    "    .option(\"compression\", \"snappy\")\n",
    "    .option(\"parquet.block.size\", 134217728)\n",
    "    .option(\"parquet.page.size\", 1048576)\n",
    "    .option(\"parquet.dictionary.enabled\", \"true\")\n",
    "    .option(\"parquet.bloom.filter.enabled\", \"true\")\n",
    "    .option(\"parquet.writer.version\", \"v2\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .partitionBy(\"year\", \"month\")           # Партиционирование\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"s3a://my-bucket/data/partitioned-parquet/\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Структура результата в MinIO**\n",
    "```\n",
    "s3a://my-bucket/data/parquet-output/\n",
    "├── part-00000-xxx.snappy.parquet\n",
    "├── part-00001-xxx.snappy.parquet\n",
    "└── _SUCCESS\n",
    "\n",
    "s3a://my-bucket/data/partitioned-parquet/\n",
    "├── year=2024/\n",
    "│   ├── month=01/\n",
    "│   │   └── part-00000-xxx.snappy.parquet\n",
    "│   └── month=02/\n",
    "│       └── part-00000-xxx.snappy.parquet\n",
    "└── year=2023/\n",
    "    └── month=12/\n",
    "        └── part-00000-xxx.snappy.parquet\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Преимущества Parquet в MinIO**\n",
    "- **✅ Колоночный формат** - эффективные запросы\n",
    "- **✅ Сжатие** - экономия хранилища\n",
    "- **✅ Предикатный pushdown** - фильтрация на чтении\n",
    "- **✅ Schema evolution** - эволюция схемы данных\n",
    "- **✅ Совместимость** - работа с различными инструментами"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1594ee83-8490-43ed-aab2-5cc2827ab8a6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
