{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e17de36-c8e0-485e-bd0c-bb2cb68de93e",
   "metadata": {},
   "source": [
    "# –ß—Ç–µ–Ω–∏–µ Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f65ea4a-e659-47a6-b346-dbe748543ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "drivers = [\n",
    "    \"/home/jovyan/work/spark-jars/hadoop-aws-3.3.4.jar\",             # S3\n",
    "    \"/home/jovyan/work/spark-jars/aws-java-sdk-bundle-1.12.262.jar\", # S3\n",
    "    \"/home/jovyan/work/spark-jars/wildfly-openssl-1.0.7.Final.jar\",  # S3\n",
    "    \"/home/jovyan/work/spark-jars/postgresql-42.6.0.jar\",            # PostgreSQL\n",
    "]\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"mustdayker-Spark\")\n",
    "         .master(\"spark://spark-master:7077\") \n",
    "         .config(\"spark.jars\", \",\".join(drivers))\n",
    "         .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb44935-f911-4ae2-ad5d-796fd9e4581f",
   "metadata": {},
   "source": [
    "## –ß—Ç–µ–Ω–∏–µ —Å –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –¥–∏—Å–∫–∞ `CSV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f20761-78be-41dc-b51a-bbdb60591acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/shared_data/bmw.csv\", \n",
    "                    header=True, \n",
    "                    inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e931d873-2afb-477e-a824-612f2c8f7a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ß—Ç–µ–Ω–∏–µ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ –ø–∞–ø–∫–∏\n",
    "# –í–ê–ñ–ù–û! –ú–æ–∂–Ω–æ —á–∏—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ —Ñ–∞–π–ª—ã\n",
    "\n",
    "df = spark.read.csv(\"/shared_data/*.csv\", \n",
    "                    header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20969f4e-8e40-4d49-aefc-8fcd2c46ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ß—Ç–µ–Ω–∏–µ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
    "df = (spark.read\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"delimiter\", \",\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .option(\"encoding\", \"utf-8\")\n",
    "      .option(\"nullValue\", \"NULL\")\n",
    "      .csv(\"/shared_data/bmw.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f77d0e4-8da7-42a7-8187-1bfba895644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞\n",
    "df = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .load(\"/shared_data/bmw.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca09457-f35f-454a-a994-f43f3a322162",
   "metadata": {},
   "source": [
    "## –ß—Ç–µ–Ω–∏–µ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —Å—Ö–µ–º—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fdd168-4325-4f88-b637-7a35076a6ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ß—Ç–µ–Ω–∏–µ CSV —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —Å—Ö–µ–º—ã\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ö–µ–º—É –¥–ª—è CSV —Ñ–∞–π–ª–∞\n",
    "schema = StructType([\n",
    "    StructField(\"Model\",                StringType(),  True),\n",
    "    StructField(\"Year\",                 IntegerType(), True),\n",
    "    StructField(\"Region\",               StringType(),  True),\n",
    "    StructField(\"Color\",                StringType(),  True),\n",
    "    StructField(\"Fuel_Type\",            StringType(),  True),\n",
    "    StructField(\"Transmission\",         StringType(),  True),\n",
    "    StructField(\"Engine_Size_L\",        DoubleType(),  True),\n",
    "    StructField(\"Mileage_KM\",           IntegerType(), True),\n",
    "    StructField(\"Price_USD\",            IntegerType(), True),\n",
    "    StructField(\"Sales_Volume\",         IntegerType(), True),\n",
    "    StructField(\"Sales_Classification\", StringType(),  True)\n",
    "])\n",
    "            \n",
    "df = spark.read.csv(\"/shared_data/bmw.csv\",\n",
    "                    schema=schema,\n",
    "                    header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df40af-e9e8-4899-9d51-20afc738ce81",
   "metadata": {},
   "source": [
    "## –ó–∞–ø–∏—Å—å –≤ MinIO `CSV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48d51e9-d39d-4597-9235-e61ab5f689b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import json\n",
    "\n",
    "(df.write\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"header\", \"false\")\n",
    "   .csv(\"s3a://learn-bucket/draft/bmw_csv\"))\n",
    "\n",
    "# –°–ø–∞—Ä–∫ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –Ω–µ –ø–∏—à–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤ CSV —Ñ–∞–π–ª—ã\n",
    "# –ü–æ —ç—Ç–æ–º—É –º–æ–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ö–µ–º—É –ø—Ä—è–º–æ –≤ MinIO\n",
    "\n",
    "schema_json = df.schema.json()\n",
    "schema_df = spark.createDataFrame([(schema_json,)], [\"schema\"])\n",
    "\n",
    "schema_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"compression\", \"none\") \\\n",
    "    .text(\"s3a://learn-bucket/draft/bmw_schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f37bd7-bff9-445a-8845-9104bdd3c4a8",
   "metadata": {},
   "source": [
    "## –ß—Ç–µ–Ω–∏–µ –∏–∑ MinIO `CSV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08d117a-5fa0-4d65-82e6-69b8ab946bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ß–∏—Ç–∞–µ–º —Å—Ö–µ–º—É –Ω–∞–ø—Ä—è–º—É—é –∏–∑ MinIO\n",
    "schema_rdd = spark.sparkContext.textFile(\"s3a://learn-bucket/draft/bmw_schema\")\n",
    "schema_json = schema_rdd.collect()[0]  # –±–µ—Ä–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É\n",
    "\n",
    "# –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å—Ö–µ–º—É\n",
    "restored_schema = StructType.fromJson(json.loads(schema_json))\n",
    "\n",
    "# –¢–µ–ø–µ—Ä—å —á–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å —ç—Ç–æ–π —Å—Ö–µ–º–æ–π\n",
    "df = (spark.read\n",
    "           .schema(restored_schema)\n",
    "           .option(\"header\", \"false\")\n",
    "           .csv(\"s3a://learn-bucket/draft/bmw_csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2bdec4-441f-457d-b0aa-b0b54f18d3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f369274c-cb22-45bf-bfa5-d41cd87b4512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47bf5d64-165f-4683-b147-969cde62e98d",
   "metadata": {},
   "source": [
    "# Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0f8559-b4dc-45bb-9294-f9d854e6c029",
   "metadata": {},
   "source": [
    "### –° –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –¥–∏—Å–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9193ca-91f9-4c0e-a902-a63b6004f31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/shared_data/yellow_taxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f56f745-ef78-40bd-8111-6772211fcfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/shared_data/yellow_taxi/data_*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6843620b-a473-4c93-a43a-f77e3e22f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/shared_data/yellow_taxi/single_file.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16900153-e836-49da-9cac-7876ca647291",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫: {df.count()}\\n\")\n",
    "df.show(1)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f332e6b-25e3-4f0f-b726-1117ae7de9f0",
   "metadata": {},
   "source": [
    "### –ò–∑ MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2d7395-84b7-4d9d-8601-d8be3e736ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a30340a-17ee-49ac-813d-b7f167ec05da",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996a1f34-0436-4c64-a2ef-b4160eff76ce",
   "metadata": {},
   "source": [
    "# Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcb18af-2665-4166-a2e8-65ec17b23caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_url = \"jdbc:postgresql://postgres-db:5432/learn_base\"\n",
    "\n",
    "postgres_con = {\n",
    "    \"user\": \"airflow\",\n",
    "    \"password\": \"airflow\", \n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "# –ü—Ä–∏–º–µ—Ä 1: –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "df = (spark.read.jdbc(url=jdbc_url, \n",
    "                      table=\"–∫–∞–∫–æ–π_–∫–∞–π—Ñ\", \n",
    "                      properties=postgres_con))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f727e8f9-6047-4f51-a0c8-e09dd69affeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db1b325-6758-4f86-95f7-0c36b5718615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7705303-1405-4915-a189-74480f195ece",
   "metadata": {},
   "source": [
    "# –í–∞—Ä–∏–∞–Ω—Ç—ã —á—Ç–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ PostgreSQL –≤ Spark DataFrame\n",
    "\n",
    "## 1. –ë–∞–∑–æ–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç\n",
    "```python\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\") \\\n",
    "    .option(\"dbtable\", \"table_name\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "## 2. –ö–æ—Ä–æ—Ç–∫–∏–π –≤–∞—Ä–∏–∞–Ω—Ç —á–µ—Ä–µ–∑ jdbc()\n",
    "```python\n",
    "df = spark.read.jdbc(\n",
    "    url=\"jdbc:postgresql://localhost:5432/mydb\",\n",
    "    table=\"table_name\",\n",
    "    properties={\n",
    "        \"user\": \"username\",\n",
    "        \"password\": \"password\", \n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "## 3. –ß—Ç–µ–Ω–∏–µ —Å SQL –∑–∞–ø—Ä–æ—Å–æ–º\n",
    "```python\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"query\", \"SELECT * FROM users WHERE age > 18 AND city = 'Moscow'\") \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "## 4. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ —Å –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º\n",
    "```python\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\") \\\n",
    "    .option(\"dbtable\", \"large_table\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .option(\"partitionColumn\", \"id\") \\     # –∫–æ–ª–æ–Ω–∫–∞ –¥–ª—è –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "    .option(\"lowerBound\", \"1\") \\           # –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "    .option(\"upperBound\", \"1000000\") \\     # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ  \n",
    "    .option(\"numPartitions\", \"10\") \\       # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä—Ç–∏—Ü–∏–π\n",
    "    .load()\n",
    "```\n",
    "\n",
    "## 5. –ß—Ç–µ–Ω–∏–µ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π —á–µ—Ä–µ–∑ predicates\n",
    "```python\n",
    "predicates = [\n",
    "    \"date >= '2024-01-01' AND date < '2024-02-01'\",  # –ø–∞—Ä—Ç–∏—Ü–∏—è 1\n",
    "    \"date >= '2024-02-01' AND date < '2024-03-01'\",  # –ø–∞—Ä—Ç–∏—Ü–∏—è 2\n",
    "    \"date >= '2024-03-01' AND date < '2024-04-01'\"   # –ø–∞—Ä—Ç–∏—Ü–∏—è 3\n",
    "]\n",
    "\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\") \\\n",
    "    .option(\"dbtable\", \"sales\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .option(\"predicates\", predicates) \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "## 6. –ß—Ç–µ–Ω–∏–µ —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏\n",
    "```python\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\") \\\n",
    "    .option(\"dbtable\", \"table_name\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"fetchsize\", \"10000\") \\          # —Ä–∞–∑–º–µ—Ä fetch –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    .option(\"sessionInitStatement\", \"SET TIME ZONE 'UTC'\") \\  # –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏\n",
    "    .option(\"customSchema\", \"id DECIMAL(38,0), name STRING\") \\  # –∫–∞—Å—Ç–æ–º–Ω—ã–µ —Ç–∏–ø—ã\n",
    "    .load()\n",
    "```\n",
    "\n",
    "## 7. –ß—Ç–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
    "```python\n",
    "# –ß–µ—Ä–µ–∑ SQL –∑–∞–ø—Ä–æ—Å\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .option(\"query\", \"SELECT id, name, email FROM users WHERE active = true\") \\\n",
    "    .load()\n",
    "\n",
    "# –ß–µ—Ä–µ–∑ dbtable —Å –ø–æ–¥–∑–∞–ø—Ä–æ—Å–æ–º\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .option(\"dbtable\", \"(SELECT id, name FROM users) AS users_subset\") \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "## 8. –ü–æ–ª–Ω—ã–π –ø—Ä–∏–º–µ—Ä —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫\n",
    "```python\n",
    "def read_from_postgres(table_name_or_query, **options):\n",
    "    try:\n",
    "        base_options = {\n",
    "            \"url\": \"jdbc:postgresql://localhost:5432/mydb\",\n",
    "            \"user\": \"username\",\n",
    "            \"password\": \"password\",\n",
    "            \"driver\": \"org.postgresql.Driver\"\n",
    "        }\n",
    "        base_options.update(options)\n",
    "        \n",
    "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —ç—Ç–æ —Ç–∞–±–ª–∏—Ü–∞ –∏–ª–∏ –∑–∞–ø—Ä–æ—Å\n",
    "        if \"SELECT\" in table_name_or_query.upper():\n",
    "            base_options[\"query\"] = table_name_or_query\n",
    "        else:\n",
    "            base_options[\"dbtable\"] = table_name_or_query\n",
    "            \n",
    "        df = spark.read.format(\"jdbc\").options(**base_options).load()\n",
    "        \n",
    "        print(f\"‚úÖ –î–∞–Ω–Ω—ã–µ –ø—Ä–æ—á–∏—Ç–∞–Ω—ã: {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫, {df.count()} —Å—Ç—Ä–æ–∫\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ\n",
    "df1 = read_from_postgres(\"users\")\n",
    "df2 = read_from_postgres(\"SELECT * FROM orders WHERE status = 'completed'\")\n",
    "df3 = read_from_postgres(\"large_table\", numPartitions=10, partitionColumn=\"id\")\n",
    "```\n",
    "\n",
    "## 9. –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —á—Ç–µ–Ω–∏—è\n",
    "```python\n",
    "def postgres_to_spark(source, **options):\n",
    "    \"\"\"\n",
    "    source: –∏–º—è —Ç–∞–±–ª–∏—Ü—ã –∏–ª–∏ SQL –∑–∞–ø—Ä–æ—Å\n",
    "    \"\"\"\n",
    "    base_config = {\n",
    "        \"url\": \"jdbc:postgresql://localhost:5432/mydb\",\n",
    "        \"user\": \"username\",\n",
    "        \"password\": \"password\",\n",
    "        \"driver\": \"org.postgresql.Driver\",\n",
    "        \"fetchsize\": 10000\n",
    "    }\n",
    "    base_config.update(options)\n",
    "    \n",
    "    reader = spark.read.format(\"jdbc\")\n",
    "    \n",
    "    if \"SELECT\" in source.upper():\n",
    "        reader.option(\"query\", source)\n",
    "    else:\n",
    "        reader.option(\"dbtable\", source)\n",
    "    \n",
    "    return reader.options(**base_config).load()\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "df_table = postgres_to_spark(\"products\")\n",
    "df_query = postgres_to_spark(\"SELECT * FROM sales WHERE amount > 1000\")\n",
    "df_partitioned = postgres_to_spark(\n",
    "    \"large_table\",\n",
    "    partitionColumn=\"id\",\n",
    "    lowerBound=1,\n",
    "    upperBound=1000000,\n",
    "    numPartitions=8\n",
    ")\n",
    "```\n",
    "\n",
    "–í—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –≥–æ—Ç–æ–≤—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
