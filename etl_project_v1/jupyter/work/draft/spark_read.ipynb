{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e17de36-c8e0-485e-bd0c-bb2cb68de93e",
   "metadata": {},
   "source": [
    "# Чтение Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f65ea4a-e659-47a6-b346-dbe748543ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "drivers = [\n",
    "    \"/home/jovyan/work/spark-jars/hadoop-aws-3.3.4.jar\",             # S3\n",
    "    \"/home/jovyan/work/spark-jars/aws-java-sdk-bundle-1.12.262.jar\", # S3\n",
    "    \"/home/jovyan/work/spark-jars/wildfly-openssl-1.0.7.Final.jar\",  # S3\n",
    "    \"/home/jovyan/work/spark-jars/postgresql-42.6.0.jar\",            # PostgreSQL\n",
    "]\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"mustdayker-Spark\")\n",
    "         .master(\"spark://spark-master:7077\") \n",
    "         .config(\"spark.jars\", \",\".join(drivers))\n",
    "         .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb44935-f911-4ae2-ad5d-796fd9e4581f",
   "metadata": {},
   "source": [
    "## Чтение с локального диска `CSV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f20761-78be-41dc-b51a-bbdb60591acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/shared_data/bmw.csv\", \n",
    "                    header=True, \n",
    "                    inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e931d873-2afb-477e-a824-612f2c8f7a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение всех файлов из папки\n",
    "# ВАЖНО! Можно читать только одинаковые по структуре файлы\n",
    "\n",
    "df = spark.read.csv(\"/shared_data/*.csv\", \n",
    "                    header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20969f4e-8e40-4d49-aefc-8fcd2c46ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение с параметрами\n",
    "df = (spark.read\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"delimiter\", \",\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .option(\"encoding\", \"utf-8\")\n",
    "      .option(\"nullValue\", \"NULL\")\n",
    "      .csv(\"/shared_data/bmw.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f77d0e4-8da7-42a7-8187-1bfba895644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Использование формата\n",
    "df = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .load(\"/shared_data/bmw.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca09457-f35f-454a-a994-f43f3a322162",
   "metadata": {},
   "source": [
    "## Чтение с указанием схемы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fdd168-4325-4f88-b637-7a35076a6ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение CSV с указанием схемы\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Определяем схему для CSV файла\n",
    "schema = StructType([\n",
    "    StructField(\"Model\",                StringType(),  True),\n",
    "    StructField(\"Year\",                 IntegerType(), True),\n",
    "    StructField(\"Region\",               StringType(),  True),\n",
    "    StructField(\"Color\",                StringType(),  True),\n",
    "    StructField(\"Fuel_Type\",            StringType(),  True),\n",
    "    StructField(\"Transmission\",         StringType(),  True),\n",
    "    StructField(\"Engine_Size_L\",        DoubleType(),  True),\n",
    "    StructField(\"Mileage_KM\",           IntegerType(), True),\n",
    "    StructField(\"Price_USD\",            IntegerType(), True),\n",
    "    StructField(\"Sales_Volume\",         IntegerType(), True),\n",
    "    StructField(\"Sales_Classification\", StringType(),  True)\n",
    "])\n",
    "            \n",
    "df = spark.read.csv(\"/shared_data/bmw.csv\",\n",
    "                    schema=schema,\n",
    "                    header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df40af-e9e8-4899-9d51-20afc738ce81",
   "metadata": {},
   "source": [
    "## Запись в MinIO `CSV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48d51e9-d39d-4597-9235-e61ab5f689b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import json\n",
    "\n",
    "(df.write\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"header\", \"false\")\n",
    "   .csv(\"s3a://learn-bucket/draft/bmw_csv\"))\n",
    "\n",
    "# Спарк по умолчанию не пишет заголовки в CSV файлы\n",
    "# По этому можно сохранить схему прямо в MinIO\n",
    "\n",
    "schema_json = df.schema.json()\n",
    "schema_df = spark.createDataFrame([(schema_json,)], [\"schema\"])\n",
    "\n",
    "schema_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"compression\", \"none\") \\\n",
    "    .text(\"s3a://learn-bucket/draft/bmw_schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f37bd7-bff9-445a-8845-9104bdd3c4a8",
   "metadata": {},
   "source": [
    "## Чтение из MinIO `CSV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08d117a-5fa0-4d65-82e6-69b8ab946bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Читаем схему напрямую из MinIO\n",
    "schema_rdd = spark.sparkContext.textFile(\"s3a://learn-bucket/draft/bmw_schema\")\n",
    "schema_json = schema_rdd.collect()[0]  # берем первую строку\n",
    "\n",
    "# Восстанавливаем схему\n",
    "restored_schema = StructType.fromJson(json.loads(schema_json))\n",
    "\n",
    "# Теперь читаем данные с этой схемой\n",
    "df = (spark.read\n",
    "           .schema(restored_schema)\n",
    "           .option(\"header\", \"false\")\n",
    "           .csv(\"s3a://learn-bucket/draft/bmw_csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2bdec4-441f-457d-b0aa-b0b54f18d3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f369274c-cb22-45bf-bfa5-d41cd87b4512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47bf5d64-165f-4683-b147-969cde62e98d",
   "metadata": {},
   "source": [
    "# Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0f8559-b4dc-45bb-9294-f9d854e6c029",
   "metadata": {},
   "source": [
    "### С локального диска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9193ca-91f9-4c0e-a902-a63b6004f31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/shared_data/yellow_taxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f56f745-ef78-40bd-8111-6772211fcfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/shared_data/yellow_taxi/data_*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6843620b-a473-4c93-a43a-f77e3e22f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/shared_data/yellow_taxi/single_file.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16900153-e836-49da-9cac-7876ca647291",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Количество строк: {df.count()}\\n\")\n",
    "df.show(1)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f332e6b-25e3-4f0f-b726-1117ae7de9f0",
   "metadata": {},
   "source": [
    "### Из MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2d7395-84b7-4d9d-8601-d8be3e736ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a30340a-17ee-49ac-813d-b7f167ec05da",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996a1f34-0436-4c64-a2ef-b4160eff76ce",
   "metadata": {},
   "source": [
    "# Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcb18af-2665-4166-a2e8-65ec17b23caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_url = \"jdbc:postgresql://postgres-db:5432/learn_base\"\n",
    "\n",
    "postgres_con = {\n",
    "    \"user\": \"airflow\",\n",
    "    \"password\": \"airflow\", \n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "# Пример 1: Читаем данные\n",
    "df = (spark.read.jdbc(url=jdbc_url, \n",
    "                      table=\"какой_кайф\", \n",
    "                      properties=postgres_con))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f727e8f9-6047-4f51-a0c8-e09dd69affeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db1b325-6758-4f86-95f7-0c36b5718615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7705303-1405-4915-a189-74480f195ece",
   "metadata": {},
   "source": [
    "# Варианты чтения данных из PostgreSQL в Spark DataFrame\n",
    "\n",
    "## 1. Базовый вариант\n",
    "```python\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\") \\\n",
    "    .option(\"dbtable\", \"table_name\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "## 2. Короткий вариант через jdbc()\n",
    "```python\n",
    "df = spark.read.jdbc(\n",
    "    url=\"jdbc:postgresql://localhost:5432/mydb\",\n",
    "    table=\"table_name\",\n",
    "    properties={\n",
    "        \"user\": \"username\",\n",
    "        \"password\": \"password\", \n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "## 3. Чтение с SQL запросом\n",
    "```python\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"query\", \"SELECT * FROM users WHERE age > 18 AND city = 'Moscow'\") \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "## 4. Параллельное чтение с партиционированием\n",
    "```python\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\") \\\n",
    "    .option(\"dbtable\", \"large_table\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .option(\"partitionColumn\", \"id\") \\     # колонка для партиционирования\n",
    "    .option(\"lowerBound\", \"1\") \\           # минимальное значение\n",
    "    .option(\"upperBound\", \"1000000\") \\     # максимальное значение  \n",
    "    .option(\"numPartitions\", \"10\") \\       # количество партиций\n",
    "    .load()\n",
    "```\n",
    "\n",
    "## 5. Чтение с фильтрацией через predicates\n",
    "```python\n",
    "predicates = [\n",
    "    \"date >= '2024-01-01' AND date < '2024-02-01'\",  # партиция 1\n",
    "    \"date >= '2024-02-01' AND date < '2024-03-01'\",  # партиция 2\n",
    "    \"date >= '2024-03-01' AND date < '2024-04-01'\"   # партиция 3\n",
    "]\n",
    "\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\") \\\n",
    "    .option(\"dbtable\", \"sales\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .option(\"predicates\", predicates) \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "## 6. Чтение с кастомными настройками\n",
    "```python\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\") \\\n",
    "    .option(\"dbtable\", \"table_name\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"fetchsize\", \"10000\") \\          # размер fetch для больших данных\n",
    "    .option(\"sessionInitStatement\", \"SET TIME ZONE 'UTC'\") \\  # инициализация сессии\n",
    "    .option(\"customSchema\", \"id DECIMAL(38,0), name STRING\") \\  # кастомные типы\n",
    "    .load()\n",
    "```\n",
    "\n",
    "## 7. Чтение только определенных колонок\n",
    "```python\n",
    "# Через SQL запрос\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .option(\"query\", \"SELECT id, name, email FROM users WHERE active = true\") \\\n",
    "    .load()\n",
    "\n",
    "# Через dbtable с подзапросом\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .option(\"dbtable\", \"(SELECT id, name FROM users) AS users_subset\") \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "## 8. Полный пример с обработкой ошибок\n",
    "```python\n",
    "def read_from_postgres(table_name_or_query, **options):\n",
    "    try:\n",
    "        base_options = {\n",
    "            \"url\": \"jdbc:postgresql://localhost:5432/mydb\",\n",
    "            \"user\": \"username\",\n",
    "            \"password\": \"password\",\n",
    "            \"driver\": \"org.postgresql.Driver\"\n",
    "        }\n",
    "        base_options.update(options)\n",
    "        \n",
    "        # Определяем, это таблица или запрос\n",
    "        if \"SELECT\" in table_name_or_query.upper():\n",
    "            base_options[\"query\"] = table_name_or_query\n",
    "        else:\n",
    "            base_options[\"dbtable\"] = table_name_or_query\n",
    "            \n",
    "        df = spark.read.format(\"jdbc\").options(**base_options).load()\n",
    "        \n",
    "        print(f\"✅ Данные прочитаны: {len(df.columns)} колонок, {df.count()} строк\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Ошибка чтения: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Использование\n",
    "df1 = read_from_postgres(\"users\")\n",
    "df2 = read_from_postgres(\"SELECT * FROM orders WHERE status = 'completed'\")\n",
    "df3 = read_from_postgres(\"large_table\", numPartitions=10, partitionColumn=\"id\")\n",
    "```\n",
    "\n",
    "## 9. Универсальная функция чтения\n",
    "```python\n",
    "def postgres_to_spark(source, **options):\n",
    "    \"\"\"\n",
    "    source: имя таблицы или SQL запрос\n",
    "    \"\"\"\n",
    "    base_config = {\n",
    "        \"url\": \"jdbc:postgresql://localhost:5432/mydb\",\n",
    "        \"user\": \"username\",\n",
    "        \"password\": \"password\",\n",
    "        \"driver\": \"org.postgresql.Driver\",\n",
    "        \"fetchsize\": 10000\n",
    "    }\n",
    "    base_config.update(options)\n",
    "    \n",
    "    reader = spark.read.format(\"jdbc\")\n",
    "    \n",
    "    if \"SELECT\" in source.upper():\n",
    "        reader.option(\"query\", source)\n",
    "    else:\n",
    "        reader.option(\"dbtable\", source)\n",
    "    \n",
    "    return reader.options(**base_config).load()\n",
    "\n",
    "# Примеры использования\n",
    "df_table = postgres_to_spark(\"products\")\n",
    "df_query = postgres_to_spark(\"SELECT * FROM sales WHERE amount > 1000\")\n",
    "df_partitioned = postgres_to_spark(\n",
    "    \"large_table\",\n",
    "    partitionColumn=\"id\",\n",
    "    lowerBound=1,\n",
    "    upperBound=1000000,\n",
    "    numPartitions=8\n",
    ")\n",
    "```\n",
    "\n",
    "Все варианты готовы к использованию! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
