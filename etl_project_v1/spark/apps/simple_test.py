from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType


def main():
    print("üöÄ Starting test Spark application...")

    # –ü—Ä–æ—Å—Ç–∞—è —Å–µ—Å—Å–∏—è –±–µ–∑ –ª–∏—à–Ω–∏—Ö –∫–æ–Ω—Ñ–∏–≥–æ–≤
    spark = SparkSession.builder \
        .appName("airflow-test-app") \
        .getOrCreate()

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —É—Ä–æ–≤–µ–Ω—å –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è Spark
    spark.sparkContext.setLogLevel("WARN")  # –∏–ª–∏ "ERROR"

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —É—Ä–æ–≤–µ–Ω—å –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è Py4J (–±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è —Å–≤—è–∑–∏ Python-Java)
    logger = spark.sparkContext._jvm.org.apache.log4j
    logger.LogManager.getLogger("org").setLevel(logger.Level.WARN)
    logger.LogManager.getLogger("akka").setLevel(logger.Level.WARN)
    logger.LogManager.getLogger("io").setLevel(logger.Level.WARN)


    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
    schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("name", StringType(), True),
        StructField("value", IntegerType(), True)
    ])

    data = [
        (1, "Alice", 100),
        (2, "Bob", 200),
        (3, "Charlie", 300),
        (4, "David", 400),
        (5, "Eve", 500)
    ]

    df = spark.createDataFrame(data, schema=schema)

    print("‚úÖ Spark session created successfully!")
    print("üìä Test DataFrame:")
    df.show()

    # –ü—Ä–æ—Å—Ç–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    result = df.groupBy().sum("value").collect()
    total_value = result[0][0]
    print(f"üí∞ Total value: {total_value}")

    print("‚úÖ Spark application completed successfully!")


if __name__ == "__main__":
    main()