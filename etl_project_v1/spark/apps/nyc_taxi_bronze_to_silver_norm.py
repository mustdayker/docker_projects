from pyspark.sql import functions as F
from pyspark.sql.types import DoubleType, IntegerType
from pyspark.sql import SparkSession
import re
from minio import Minio
from minio.error import S3Error
import time

import ast
import argparse

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è MinIO
MINIO_ENDPOINT = 'minio:9000'
MINIO_ACCESS_KEY = 'minioadmin'
MINIO_SECRET_KEY = 'minioadmin'


def get_minio_client():
    """–°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–ª–∏–µ–Ω—Ç MinIO"""
    return Minio(
        MINIO_ENDPOINT,
        access_key=MINIO_ACCESS_KEY,
        secret_key=MINIO_SECRET_KEY,
        secure=False
    )


def extract_month_from_filename(file_path):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Å—è—Ü –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYY-MM"""
    match = re.search(r'(\d{4}-\d{2})', file_path)
    return match.group(1) if match else None


def get_processed_slices(output_bucket, output_prefix):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ä–µ–∑–æ–≤ –∏–∑ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –±–∞–∫–µ—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É—è MinIO"""
    try:
        client = get_minio_client()
        processed_slices = set()

        objects = client.list_objects(output_bucket, prefix=output_prefix, recursive=True)

        for obj in objects:
            month = extract_month_from_filename(obj.object_name)
            if month:
                processed_slices.add(month)

        print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ä–µ–∑–æ–≤ –≤ {output_bucket}/{output_prefix}: {len(processed_slices)}")
        return processed_slices

    except S3Error as e:
        if e.code == 'NoSuchBucket':
            print(f"‚ö†Ô∏è –ë–∞–∫–µ—Ç {output_bucket} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –ø—É—Å—Ç–æ–π")
        else:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {output_bucket} –±–∞–∫–µ—Ç–∞: {e}")
        return set()
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å {output_bucket} –±–∞–∫–µ—Ç: {e}")
        return set()


def get_input_files_with_months(input_bucket, input_prefix):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤/–ø–∞–ø–æ–∫ –∏–∑ –≤—Ö–æ–¥–Ω–æ–≥–æ –±–∞–∫–µ—Ç–∞ —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ –º–µ—Å—è—Ü–∞–º–∏ –∏—Å–ø–æ–ª—å–∑—É—è MinIO"""
    try:
        client = get_minio_client()
        input_files = []

        objects = client.list_objects(input_bucket, prefix=input_prefix, recursive=False)

        for obj in objects:
            object_name = obj.object_name

            is_parquet_file = object_name.endswith('.parquet')
            is_folder = not is_parquet_file and object_name.endswith('/')

            if is_parquet_file or is_folder:
                month = extract_month_from_filename(object_name)
                if month:
                    s3_path = f"s3a://{input_bucket}/{object_name}"
                    input_files.append({
                        'path': s3_path,
                        'month': month,
                        'file_name': object_name.split('/')[-1] if is_parquet_file else object_name.split('/')[-2] + '/'
                    })

        print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ {input_bucket}/{input_prefix}: {len(input_files)}")
        return input_files

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {input_bucket} –±–∞–∫–µ—Ç–∞: {e}")
        return []


def standardize_nyc_taxi_data(spark, input_path, output_path):
    """–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ NYC Taxi"""
    output_path = output_path.replace('.parquet', '')

    df = spark.read.parquet(input_path)

    for col_name in df.columns:
        df = df.withColumnRenamed(col_name, col_name.lower())

    type_mapping = {
        "vendorid": IntegerType(),
        "pulocationid": IntegerType(),
        "dolocationid": IntegerType(),
        "payment_type": IntegerType(),
        "ratecodeid": IntegerType(),
        "passenger_count": IntegerType(),
        "fare_amount": DoubleType(),
        "extra": DoubleType(),
        "mta_tax": DoubleType(),
        "tip_amount": DoubleType(),
        "tolls_amount": DoubleType(),
        "improvement_surcharge": DoubleType(),
        "total_amount": DoubleType(),
        "congestion_surcharge": DoubleType(),
        "airport_fee": DoubleType(),
        "cbd_congestion_fee": DoubleType(),
        "trip_distance": DoubleType()
    }

    for col_name, target_type in type_mapping.items():
        if col_name in df.columns:
            df = df.withColumn(
                col_name,
                F.coalesce(
                    F.col(col_name).cast(target_type),
                    F.lit(0 if target_type == IntegerType() else 0.0)
                )
            )

    expected_columns = [
        "vendorid", "tpep_pickup_datetime", "tpep_dropoff_datetime",
        "passenger_count", "trip_distance", "ratecodeid", "store_and_fwd_flag",
        "pulocationid", "dolocationid", "payment_type", "fare_amount", "extra",
        "mta_tax", "tip_amount", "tolls_amount", "improvement_surcharge",
        "total_amount", "congestion_surcharge", "airport_fee", "cbd_congestion_fee"
    ]

    final_columns = [col for col in expected_columns if col in df.columns]
    df_standardized = df.select(final_columns)

    (df_standardized
     .coalesce(1)
     .write
     .mode("overwrite")
     .option("compression", "snappy")
     .parquet(output_path)
     )

    print(f"‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {input_path} -> {output_path}")
    return df_standardized


def process_incremental_nyc_taxi_files(spark, input_bucket, input_prefix, output_bucket, output_prefix):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã NYC Taxi –∏–∑ –≤—Ö–æ–¥–Ω–æ–≥–æ –±–∞–∫–µ—Ç–∞ –≤ –≤—ã—Ö–æ–¥–Ω–æ–π"""

    processed_slices = get_processed_slices(output_bucket, output_prefix)
    input_files = get_input_files_with_months(input_bucket, input_prefix)

    new_files = [f for f in input_files if f['month'] not in processed_slices]

    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   - –í—Å–µ–≥–æ –≤–æ –≤—Ö–æ–¥–Ω–æ–º –±–∞–∫–µ—Ç–µ: {len(input_files)}")
    print(f"   - –£–∂–µ –≤ –≤—ã—Ö–æ–¥–Ω–æ–º –±–∞–∫–µ—Ç–µ: {len(processed_slices)}")
    print(f"   - –ù–æ–≤—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(new_files)}")

    if not new_files:
        print("üéâ –í—Å–µ —Å—Ä–µ–∑—ã —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã! –ù–∏—á–µ–≥–æ –¥–µ–ª–∞—Ç—å –Ω–µ –Ω—É–∂–Ω–æ.")
        return

    for i, file_info in enumerate(new_files, 1):
        input_path = file_info['path']
        file_name = file_info['file_name']

        output_path = f"s3a://{output_bucket}/{output_prefix}{file_name}".replace('.parquet', '')

        print(f"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –Ω–æ–≤—ã–π —Å—Ä–µ–∑ ({i}/{len(new_files)}): {file_info['month']}")

        try:
            standardize_nyc_taxi_data(spark, input_path, output_path)
            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {file_info['month']}")
            print()
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_info['month']}: {e}")
            raise

    print(f"üéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(new_files)} –Ω–æ–≤—ã—Ö —Å—Ä–µ–∑–æ–≤.")



def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è Spark –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""

    # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—Å–µ—Ä –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    parser = argparse.ArgumentParser()
    parser.add_argument('--input-data', type=str, required=True)
    parser.add_argument('--execution-date', type=str, required=True)

    # –ü–∞—Ä—Å–∏–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã
    args = parser.parse_args()

    # –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    input_data = args.input_data
    execution_date = args.execution_date

    print("=" * 60)
    print(f"INPUT DATA FROM XCOM: {input_data}")
    print(f"EXECUTION DATE: {execution_date}")
    print("=" * 60)


    print("-------- üìä –°—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏ download_nyc_taxi_data ---------")

    # –ü–∞—Ä—Å–∏–º Python dict —Å—Ç—Ä–æ–∫—É
    try:
        input_dict = ast.literal_eval(input_data)
    except (SyntaxError, ValueError) as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
        print(f"–ü–æ–ª—É—á–µ–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞: {repr(input_data)}")
        raise

    for i in input_dict.items():
        print(i)

    print("----------------------------------------------------------")
    print("\n\n")
    start_time = time.time()

    spark = SparkSession.builder \
        .appName("nyc-taxi-normalisation") \
        .getOrCreate()

    execution_time = time.time() - start_time
    print(f"‚è±Ô∏è  Spark —Å–µ—Å—Å–∏—è —Å—Ç–∞—Ä—Ç–æ–≤–∞–ª–∞ –∑–∞: {execution_time:.2f} —Å–µ–∫—É–Ω–¥ ({execution_time / 60:.2f} –º–∏–Ω—É—Ç)")

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —É—Ä–æ–≤–µ–Ω—å –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è Spark
    spark.sparkContext.setLogLevel("WARN")  # –∏–ª–∏ "ERROR"

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —É—Ä–æ–≤–µ–Ω—å –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è Py4J (–±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è —Å–≤—è–∑–∏ Python-Java)
    logger = spark.sparkContext._jvm.org.apache.log4j
    logger.LogManager.getLogger("org").setLevel(logger.Level.WARN)
    logger.LogManager.getLogger("akka").setLevel(logger.Level.WARN)
    logger.LogManager.getLogger("io").setLevel(logger.Level.WARN)

    start_time = time.time()

    print()
    try:
        process_incremental_nyc_taxi_files(
            spark=spark,
            input_bucket='bronze',
            input_prefix='nyc-taxi-data/',
            output_bucket='silver',
            output_prefix='nyc-taxi-data-norm/'
        )

        execution_time = time.time() - start_time

        print()
        print("üéä –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–∏–ª–æ —Ä–∞–±–æ—Ç—É!")
        print(f"‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è ETL –ø—Ä–æ—Ü–µ—Å—Å–∞: {execution_time:.2f} —Å–µ–∫—É–Ω–¥ ({execution_time / 60:.2f} –º–∏–Ω—É—Ç)")

        print("\n\n")
    except Exception as e:
        print(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏: {e}")
        print("\n\n\n")
        raise


if __name__ == "__main__":
    main()